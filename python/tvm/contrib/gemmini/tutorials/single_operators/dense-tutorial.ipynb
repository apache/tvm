{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense layer tutorial\n",
    "\n",
    "This tutorials shows how a quantized dense (fully connected) layer can be compiled to be executed on the Gemmini accelerator. The generated baremetal C code is then tested on the Spike RISC-V ISA simulator. Before starting this tutorial, you should have downloaded the Chipyard repository and installed the Spike simulator with the Gemmini extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tvm.contrib.gemmini as gemmini\n",
    "from tvm import relay\n",
    "import tvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to export the environment variable CHIPYARD_HOME, in order to be able to run the Spike simulator correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CHIPYARD_HOME\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the parameters of the layer we want to test. In this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 32\n",
    "input_width = 32\n",
    "output_width = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate a prequantized TFLite model, because for now the Gemmini integration only supports models that were quantized with specific flags as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "        self.w = tf.Variable(tf.random.normal([input_width, output_width]), name=\"w\")\n",
    "        self.b = tf.Variable(tf.random.normal([output_width]), name=\"b\")\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(shape=[input_height, input_width], dtype=tf.float32),\n",
    "        ]\n",
    "    )\n",
    "    def matmul(self, x):\n",
    "        return tf.linalg.matmul(x, self.w, transpose_b=False) + self.b\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Convert the concrete functions using TFLiteConverter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "\n",
    "def representative_data_gen():\n",
    "    dataset = [\n",
    "        (\n",
    "            np.array(\n",
    "                np.random.randint(-127, 128, size=(input_height, input_width)), dtype=np.float32\n",
    "            ),\n",
    "            np.array(\n",
    "                np.random.randint(-127, 128, size=(input_width, output_width)), dtype=np.float32\n",
    "            ),\n",
    "        )\n",
    "        for s in range(100)\n",
    "    ]\n",
    "    for input_value in dataset:\n",
    "        # Model has only one input so each data point has one element.\n",
    "        yield [input_value[0]]\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter._experimental_disable_per_channel = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(\"matmul.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the model, we import the model and run it. We store the output, in order to compare it with the output that will be later obtained from the Gemmini accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"rm -rf model.tar dev/ include/ generated-project/\")\n",
    "\n",
    "tflite_file = \"./matmul.tflite\"\n",
    "tflite_model_buf = open(tflite_file, \"rb\").read()\n",
    "input_tensor = \"layer1_input\"\n",
    "input_dtype = \"uint8\"\n",
    "\n",
    "os.system(\"mkdir -p include\")\n",
    "\n",
    "try:\n",
    "    import tflite\n",
    "\n",
    "    tflite_model = tflite.Model.GetRootAsModel(tflite_model_buf, 0)\n",
    "except AttributeError:\n",
    "    import tflite.Model\n",
    "\n",
    "    tflite_model = tflite.Model.Model.GetRootAsModel(tflite_model_buf, 0)\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_file, experimental_preserve_all_tensors=True)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "input1 = np.random.randint(0, 255, (input_height, input_width), dtype=np.uint8)\n",
    "interpreter.set_tensor(input_details[0][\"index\"], input1)\n",
    "\n",
    "interpreter.invoke()\n",
    "expected_output = interpreter.get_tensor(output_details[0][\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create C files and headers with the inputs and expected output, so that we can then execute the same operation on the Gemmini accelerator, and compare the expected output with the actual predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemmini.create_header_file(\"inputs\", \"data\", \"input\", input1, \"./include\")\n",
    "gemmini.create_header_file(\"outputs\", \"data\", \"output\", expected_output, \"./include\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemmini environment class needs to be initialized with the parameters of the Gemmini accelerator where we want to execute our operation. We use here the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemmini.Environment.init_overwrite(dim=16, acc_rows=1024, bank_rows=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFLite model generated in the previous steps is now imported into TVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relay.frontend.from_tflite(\n",
    "    tflite_model,\n",
    "    shape_dict={\n",
    "        \"serving_default_x:0\": (input_height, input_width),\n",
    "    },\n",
    "    dtype_dict={\n",
    "        \"serving_default_x:0\": input_dtype,\n",
    "    },\n",
    ")\n",
    "mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to build a model for the Gemmini accelerator, we need to replace all supported layers by the Gemmini specific operators. This is done using the __gemmini.preprocess__ pass. Notice the changes in the \"main\" function after running the preprocess pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = gemmini.preprocess_pass(mod)\n",
    "mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the Relay Graph. Notice that we are using the CRT runtime, the target is C because we want to generate C code (but the device is Gemmini), and we use the AOT executor and the USMP feature in order to get a complete bare metal C code, without calls to memory allocator APIs.\n",
    "\n",
    "The __gemmini.build_config__ function returns a PassContext object containing the specific parameters needed to correctly build the model for the Gemmini accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNTIME = tvm.relay.backend.Runtime(\"crt\", {\"system-lib\": False})\n",
    "TARGET = tvm.target.target.Target({\"kind\": \"c\", \"device\": \"gemmini\"})\n",
    "EXECUTOR = tvm.relay.backend.Executor(\"aot\", options={\"interface-api\": \"c\", \"unpacked-api\": 1})\n",
    "\n",
    "with gemmini.build_config(usmp_alg=\"hill_climb\",opt_level=3, disabled_pass=[\"AlterOpLayout\"]):\n",
    "    module = relay.build(mod, executor=EXECUTOR, runtime=RUNTIME, target=TARGET, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The builded model is exported to the model library format. This will be used in the next steps to generate the baremetal project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "os.system(\"mkdir dev\")\n",
    "model_library_format_tar_path = pathlib.Path(pathlib.Path.cwd(), \"dev/model.tar\")\n",
    "tvm.micro.export_model_library_format(module, model_library_format_tar_path)\n",
    "\n",
    "import tarfile\n",
    "\n",
    "with tarfile.open(model_library_format_tar_path, \"r:*\") as tar_f:\n",
    "    print(\"\\n\".join(f\" - {m.name}\" for m in tar_f.getmembers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the test project, using the example project provided for this tutorial in the Gemmini microTVM template projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_project_path = pathlib.Path(tvm.micro.get_microtvm_template_projects(\"gemmini\"))\n",
    "project_options = {\n",
    "    \"project_type\": \"dense_example\"\n",
    "}  \n",
    "\n",
    "generated_project_dir = pathlib.Path(pathlib.Path.cwd(), \"generated-project\")\n",
    "generated_project = tvm.micro.generate_project(\n",
    "    template_project_path, module, generated_project_dir, project_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the project. This will generate an executable we can run on the Spike simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_project.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we execute the compiled baremetal project on the Spike simulator.\n",
    "\n",
    "Note: if there are errors, these can be related to rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_project.flash()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tvm': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7de8d84d32cbbe537c50b34cb949251a03cf44fca18853707459ebd33e07d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
