# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1708+gd62e1844d\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-17 09:29+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/api/python/relay/index.rst:19
msgid "tvm.relay"
msgstr ""

#: of tvm.relay:1
msgid "The Relay IR namespace containing the IR definition and compiler."
msgstr ""

#: of tvm.relay:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`setrecursionlimit <tvm.relay.setrecursionlimit>`\\ \\(n\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Set the maximum depth of the Python interpreter stack to n."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`build <tvm.relay.build>`\\ \\(ir\\_mod\\[\\, target\\, "
"target\\_host\\, params\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.build_module.build:1 tvm.relay:1:<autosummary>:1
msgid "Helper function that builds a Relay function to run on TVM graph executor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`create_executor <tvm.relay.create_executor>`\\ \\(\\[kind\\, mod\\,"
" device\\, target\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.build_module.create_executor:1 tvm.relay:1:<autosummary>:1
msgid "Factory function to create an executor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`optimize <tvm.relay.optimize>`\\ \\(mod\\[\\, target\\, params\\]\\)"
msgstr ""

#: of tvm.relay.build_module.optimize:1 tvm.relay:1:<autosummary>:1
msgid "Helper function that optimizes a Relay module."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`build_config <tvm.relay.build_config>`\\ \\(\\[opt\\_level\\, "
"required\\_pass\\, ...\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Configure the build behavior by setting config variables."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`ShapeVar <tvm.relay.ShapeVar>`\\ \\(name\\)"
msgstr ""

#: of tvm.relay.ty.ShapeVar:1 tvm.relay:1:<autosummary>:1
msgid "A helper which constructs a type var of which the shape kind."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`scalar_type <tvm.relay.scalar_type>`\\ \\(dtype\\)"
msgstr ""

#: of tvm.relay.ty.scalar_type:1 tvm.relay:1:<autosummary>:1
msgid "Creates a scalar type."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`var <tvm.relay.var>`\\ \\(name\\_hint\\[\\, type\\_annotation\\, "
"shape\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.var:1 tvm.relay:1:<autosummary>:1
msgid "Create a new tvm.relay.Var."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`const <tvm.relay.const>`\\ \\(value\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.const:1 tvm.relay:1:<autosummary>:1
msgid "Create a constant value."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`bind <tvm.relay.bind>`\\ \\(expr\\, binds\\)"
msgstr ""

#: of tvm.relay.expr.bind:1 tvm.relay:1:<autosummary>:1
msgid "Bind an free variables in expr or function arguments."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`abs <tvm.relay.abs>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.abs:1 tvm.relay.op.tensor.sign:1
#: tvm.relay:1:<autosummary>:1
msgid "Compute element-wise absolute of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`acos <tvm.relay.acos>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.acos:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise acos of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`acosh <tvm.relay.acosh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.acosh:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise acosh of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`add <tvm.relay.add>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.add:1 tvm.relay:1:<autosummary>:1
msgid "Addition with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`adv_index <tvm.relay.adv_index>`\\ \\(inputs\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Numpy style advanced indexing."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`all <tvm.relay.all>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.all:1 tvm.relay:1:<autosummary>:1
msgid "Computes the logical AND of boolean array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`any <tvm.relay.any>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.any:1 tvm.relay:1:<autosummary>:1
msgid "Computes the logical OR of boolean array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`arange <tvm.relay.arange>`\\ \\(start\\[\\, stop\\, step\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.arange:1 tvm.relay:1:<autosummary>:1
msgid "Return evenly spaced values within a given interval."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`argmax <tvm.relay.argmax>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.argmax:1 tvm.relay:1:<autosummary>:1
msgid "Returns the indices of the maximum values along an axis."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`argmin <tvm.relay.argmin>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.argmin:1 tvm.relay:1:<autosummary>:1
msgid "Returns the indices of the minimum values along an axis."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`argsort <tvm.relay.argsort>`\\ \\(data\\[\\, axis\\, is\\_ascend\\,"
" dtype\\]\\)"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:1 tvm.relay:1:<autosummary>:1
msgid ""
"Performs sorting along the given axis and returns an array of indicies "
"having same shape as an input array that index data in sorted order."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`argwhere <tvm.relay.argwhere>`\\ \\(condition\\)"
msgstr ""

#: of tvm.relay.op.transform.argwhere:1 tvm.relay:1:<autosummary>:1
msgid "Find the indices of elements of a tensor that are non-zero."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`asin <tvm.relay.asin>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.asin:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise asin of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`asinh <tvm.relay.asinh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.asinh:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise asinh of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`atan <tvm.relay.atan>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.atan:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise atan of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`atanh <tvm.relay.atanh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.atanh:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise atanh of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`bitwise_and <tvm.relay.bitwise_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.bitwise_and:1 tvm.relay:1:<autosummary>:1
msgid "bitwise AND with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`bitwise_not <tvm.relay.bitwise_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.bitwise_not:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise bitwise not of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`bitwise_or <tvm.relay.bitwise_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.bitwise_or:1 tvm.relay:1:<autosummary>:1
msgid "bitwise OR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`bitwise_xor <tvm.relay.bitwise_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.bitwise_xor:1 tvm.relay:1:<autosummary>:1
msgid "bitwise XOR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`broadcast_to <tvm.relay.broadcast_to>`\\ \\(data\\, shape\\)"
msgstr ""

#: of tvm.relay.op.transform.broadcast_to:1 tvm.relay:1:<autosummary>:1
msgid ""
"Return a scalar value array with the same type, broadcast to the provided"
" shape."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`broadcast_to_like <tvm.relay.broadcast_to_like>`\\ \\(data\\, "
"broadcast\\_type\\)"
msgstr ""

#: of tvm.relay.op.transform.broadcast_to_like:1
#: tvm.relay.op.transform.collapse_sum_like:1
#: tvm.relay.op.transform.full_like:1 tvm.relay:1:<autosummary>:1
msgid ""
"Return a scalar value array with the same shape and type as the input "
"array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`cast <tvm.relay.cast>`\\ \\(data\\, dtype\\)"
msgstr ""

#: of tvm.relay.op.transform.cast:1 tvm.relay:1:<autosummary>:1
msgid "Cast input tensor to data type."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`cast_like <tvm.relay.cast_like>`\\ \\(data\\, dtype\\_like\\)"
msgstr ""

#: of tvm.relay.op.transform.cast_like:1 tvm.relay:1:<autosummary>:1
msgid "Cast input tensor to data type of another tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`ceil <tvm.relay.ceil>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.ceil:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise ceil of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`clip <tvm.relay.clip>`\\ \\(a\\, a\\_min\\, a\\_max\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Clip the elements in `a` between `a_min` and `a_max`."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`collapse_sum_like <tvm.relay.collapse_sum_like>`\\ \\(data\\, "
"collapse\\_type\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`collapse_sum_to <tvm.relay.collapse_sum_to>`\\ \\(data\\, shape\\)"
msgstr ""

#: of tvm.relay.op.transform.collapse_sum_to:1 tvm.relay:1:<autosummary>:1
msgid "Return a summation of data to the specified shape."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`concatenate <tvm.relay.concatenate>`\\ \\(data\\, axis\\)"
msgstr ""

#: of tvm.relay.op.tensor.concatenate:1 tvm.relay:1:<autosummary>:1
msgid "Concatenate the input tensors along the given axis."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`copy <tvm.relay.copy>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.copy:1 tvm.relay:1:<autosummary>:1
msgid "Copy a tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`copy_shape_func <tvm.relay.copy_shape_func>`\\ \\(attrs\\, "
"inputs\\, \\_\\)"
msgstr ""

#: of tvm.relay.op.tensor.copy_shape_func:1 tvm.relay:1:<autosummary>:1
msgid "Shape function for copy op."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`cos <tvm.relay.cos>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.cos:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise cos of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`cosh <tvm.relay.cosh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.cosh:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise cosh of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`cumprod <tvm.relay.cumprod>`\\ \\(data\\[\\, axis\\, dtype\\, "
"exclusive\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Numpy style cumprod op."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`cumsum <tvm.relay.cumsum>`\\ \\(data\\[\\, axis\\, dtype\\, "
"exclusive\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Numpy style cumsum op."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`device_copy <tvm.relay.device_copy>`\\ \\(data\\, src\\_dev\\, "
"dst\\_dev\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Copy data from the source device to the destination device."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`divide <tvm.relay.divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.divide:1 tvm.relay:1:<autosummary>:1
msgid "Division with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`equal <tvm.relay.equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.equal:1 tvm.relay:1:<autosummary>:1
msgid "Broadcasted elementwise test for (lhs == rhs)."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`erf <tvm.relay.erf>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.erf:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise error function of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`exp <tvm.relay.exp>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.exp:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise exp of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`expand_dims <tvm.relay.expand_dims>`\\ \\(data\\, axis\\[\\, "
"num\\_newaxis\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.expand_dims:1 tvm.relay:1:<autosummary>:1
msgid "Insert `num_newaxis` axes at the position given by `axis`."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`fixed_point_multiply <tvm.relay.fixed_point_multiply>`\\ \\(data\\,"
" multiplier\\, shift\\)"
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:1 tvm.relay:1:<autosummary>:1
msgid ""
"Fixed point multiplication between data and a fixed point constant "
"expressed as multiplier * 2^(-shift), where multiplier is a Q-number with"
" 31 fractional bits"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`floor <tvm.relay.floor>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.floor:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise floor of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`floor_divide <tvm.relay.floor_divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.floor_divide:1 tvm.relay:1:<autosummary>:1
msgid "Floor division with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`floor_mod <tvm.relay.floor_mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.floor_mod:1 tvm.relay:1:<autosummary>:1
msgid "Floor mod with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`full <tvm.relay.full>`\\ \\(fill\\_value\\[\\, shape\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.full:1 tvm.relay:1:<autosummary>:1
msgid "Fill array with scalar value."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`full_like <tvm.relay.full_like>`\\ \\(data\\, fill\\_value\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`gather <tvm.relay.gather>`\\ \\(data\\, axis\\, indices\\)"
msgstr ""

#: of tvm.relay.op.transform.gather:1 tvm.relay:1:<autosummary>:1
msgid "Gather values along given axis from given indices."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`gather_nd <tvm.relay.gather_nd>`\\ \\(data\\, indices\\[\\, "
"batch\\_dims\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.gather_nd:1 tvm.relay:1:<autosummary>:1
msgid ""
"Gather elements or slices from data and store to a tensor whose shape is "
"defined by indices."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`greater <tvm.relay.greater>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.greater:1 tvm.relay:1:<autosummary>:1
msgid "Broadcasted elementwise test for (lhs > rhs)."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`greater_equal <tvm.relay.greater_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.greater_equal:1 tvm.relay:1:<autosummary>:1
msgid "Broadcasted elementwise test for (lhs >= rhs)."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`invert_permutation <tvm.relay.invert_permutation>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Computes the inverse permutation of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`isfinite <tvm.relay.isfinite>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.isfinite:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise finiteness of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`isinf <tvm.relay.isinf>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.isinf:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise infiniteness of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`isnan <tvm.relay.isnan>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.isnan:1 tvm.relay:1:<autosummary>:1
msgid "Check nan in input data element-wise."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`layout_transform <tvm.relay.layout_transform>`\\ \\(data\\, "
"src\\_layout\\, dst\\_layout\\)"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:1 tvm.relay:1:<autosummary>:1
msgid "Transform the layout of a tensor"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`left_shift <tvm.relay.left_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.left_shift:1 tvm.relay:1:<autosummary>:1
msgid "Left shift with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`less <tvm.relay.less>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.less:1 tvm.relay:1:<autosummary>:1
msgid "Broadcasted elementwise test for (lhs < rhs)."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`less_equal <tvm.relay.less_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.less_equal:1 tvm.relay:1:<autosummary>:1
msgid "Broadcasted elementwise test for (lhs <= rhs)."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`log <tvm.relay.log>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.log:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise log of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`log10 <tvm.relay.log10>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.log10:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise log to the base 10 of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`log2 <tvm.relay.log2>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.log2:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise log to the base 2 of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`logical_and <tvm.relay.logical_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.logical_and:1 tvm.relay:1:<autosummary>:1
msgid "logical AND with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`logical_not <tvm.relay.logical_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.logical_not:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise logical not of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`logical_or <tvm.relay.logical_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.logical_or:1 tvm.relay:1:<autosummary>:1
msgid "logical OR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`logical_xor <tvm.relay.logical_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.logical_xor:1 tvm.relay:1:<autosummary>:1
msgid "logical XOR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`logsumexp <tvm.relay.logsumexp>`\\ \\(data\\[\\, axis\\, "
"keepdims\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:1 tvm.relay:1:<autosummary>:1
msgid ""
"Compute the log of the sum of exponentials of input elements over given "
"axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`matrix_set_diag <tvm.relay.matrix_set_diag>`\\ \\(data\\, "
"diagonal\\[\\, k\\, align\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:1 tvm.relay:1:<autosummary>:1
msgid ""
"Returns a tensor with the diagonals of input tensor replaced with the "
"provided diagonal values."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`max <tvm.relay.max>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.max:1 tvm.relay:1:<autosummary>:1
msgid "Computes the max of array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`maximum <tvm.relay.maximum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.maximum:1 tvm.relay:1:<autosummary>:1
msgid "Maximum with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`mean <tvm.relay.mean>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.mean:1 tvm.relay:1:<autosummary>:1
msgid "Computes the mean of array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`mean_std <tvm.relay.mean_std>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.mean_std:1 tvm.relay:1:<autosummary>:1
msgid "Computes the mean and standard deviation of data over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`mean_variance <tvm.relay.mean_variance>`\\ \\(data\\[\\, axis\\, "
"keepdims\\, exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.mean_variance:1 tvm.relay:1:<autosummary>:1
msgid "Computes the mean and variance of data over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`meshgrid <tvm.relay.meshgrid>`\\ \\(data\\[\\, indexing\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.meshgrid:1 tvm.relay:1:<autosummary>:1
msgid "Create coordinate matrices from coordinate vectors."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`min <tvm.relay.min>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.min:1 tvm.relay:1:<autosummary>:1
msgid "Computes the min of array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`minimum <tvm.relay.minimum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.minimum:1 tvm.relay:1:<autosummary>:1
msgid "Minimum with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`mod <tvm.relay.mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.mod:1 tvm.relay:1:<autosummary>:1
msgid "Mod with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`multiply <tvm.relay.multiply>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.multiply:1 tvm.relay:1:<autosummary>:1
msgid "Multiplication with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`ndarray_size <tvm.relay.ndarray_size>`\\ \\(data\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.op.tensor.ndarray_size:1 tvm.relay:1:<autosummary>:1
msgid "Get number of elements of input tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`negative <tvm.relay.negative>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.negative:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise negative of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`not_equal <tvm.relay.not_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.not_equal:1 tvm.relay:1:<autosummary>:1
msgid "Broadcasted elementwise test for (lhs != rhs)."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`one_hot <tvm.relay.one_hot>`\\ \\(indices\\, on\\_value\\, "
"off\\_value\\, depth\\, ...\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
"Returns a one-hot tensor where the locations repsented by indices take "
"value on_value, other locations take value off_value."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`ones <tvm.relay.ones>`\\ \\(shape\\, dtype\\)"
msgstr ""

#: of tvm.relay.op.tensor.ones:1 tvm.relay:1:<autosummary>:1
msgid "Fill array with ones."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`ones_like <tvm.relay.ones_like>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.ones_like:1 tvm.relay:1:<autosummary>:1
msgid "Returns an array of ones, with same type and shape as the input."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`power <tvm.relay.power>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.power:1 tvm.relay:1:<autosummary>:1
msgid "Power with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`prod <tvm.relay.prod>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.prod:1 tvm.relay:1:<autosummary>:1
msgid "Computes the products of array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`reinterpret <tvm.relay.reinterpret>`\\ \\(data\\, dtype\\)"
msgstr ""

#: of tvm.relay.op.transform.reinterpret:1 tvm.relay:1:<autosummary>:1
msgid "Reinterpret input tensor to data type."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`repeat <tvm.relay.repeat>`\\ \\(data\\, repeats\\, axis\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Repeats elements of an array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`reshape <tvm.relay.reshape>`\\ \\(data\\, newshape\\)"
msgstr ""

#: of tvm.relay.op.transform.reshape:1 tvm.relay:1:<autosummary>:1
msgid "Reshape the input array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`reshape_like <tvm.relay.reshape_like>`\\ \\(data\\, "
"shape\\_like\\[\\, lhs\\_begin\\, ...\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Reshapes the input tensor by the size of another tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`reverse <tvm.relay.reverse>`\\ \\(data\\, axis\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
"Reverses the order of elements along given axis while preserving array "
"shape."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`reverse_reshape <tvm.relay.reverse_reshape>`\\ \\(data\\, "
"newshape\\)"
msgstr ""

#: of tvm.relay.op.transform.reverse_reshape:1 tvm.relay:1:<autosummary>:1
msgid ""
"Reshapes the input array where the special values are inferred from right"
" to left."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`reverse_sequence <tvm.relay.reverse_sequence>`\\ \\(data\\, "
"seq\\_lengths\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Reverse the tensor for variable length slices."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`right_shift <tvm.relay.right_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.right_shift:1 tvm.relay:1:<autosummary>:1
msgid "Right shift with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`round <tvm.relay.round>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.round:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise round of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`rsqrt <tvm.relay.rsqrt>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.rsqrt:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise rsqrt of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`scatter <tvm.relay.scatter>`\\ \\(data\\, indices\\, updates\\, "
"axis\\)"
msgstr ""

#: of tvm.relay.op.transform.scatter:1 tvm.relay:1:<autosummary>:1
msgid "Update data at positions defined by indices with values in updates"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`scatter_add <tvm.relay.scatter_add>`\\ \\(data\\, indices\\, "
"updates\\, axis\\)"
msgstr ""

#: of tvm.relay.op.transform.scatter_add:1 tvm.relay:1:<autosummary>:1
msgid "Update data by adding values in updates at positions defined by indices"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`scatter_nd <tvm.relay.scatter_nd>`\\ \\(data\\, indices\\, "
"updates\\[\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.scatter_nd:1 tvm.relay:1:<autosummary>:1
msgid "Scatter values from an array and update."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`script <tvm.relay.script>`\\ \\(pyfunc\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1 tvm.te.hybrid.script:1
msgid "Decorate a python function function as hybrid script."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`segment_sum <tvm.relay.segment_sum>`\\ \\(data\\, "
"segment\\_ids\\[\\, num\\_segments\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Computes the sum along segment_ids along axis 0."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`sequence_mask <tvm.relay.sequence_mask>`\\ \\(data\\, "
"valid\\_length\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:1 tvm.relay:1:<autosummary>:1
msgid ""
"Sets all elements outside the expected length of the sequence to a "
"constant value."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`shape_of <tvm.relay.shape_of>`\\ \\(data\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.op.tensor.shape_of:1 tvm.relay:1:<autosummary>:1
msgid "Get shape of a tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`sigmoid <tvm.relay.sigmoid>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.sigmoid:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise sigmoid of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`sign <tvm.relay.sign>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`sin <tvm.relay.sin>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.sin:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise sin of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`sinh <tvm.relay.sinh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.sinh:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise sinh of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`slice_like <tvm.relay.slice_like>`\\ \\(data\\, shape\\_like\\[\\, "
"axes\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.slice_like:1 tvm.relay:1:<autosummary>:1
msgid "Slice the first input with respect to the second input."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`sort <tvm.relay.sort>`\\ \\(data\\[\\, axis\\, is\\_ascend\\]\\)"
msgstr ""

#: of tvm.relay.op.algorithm.sort:1 tvm.relay:1:<autosummary>:1
msgid "Performs sorting along the given axis and returns data in sorted order."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`sparse_fill_empty_rows <tvm.relay.sparse_fill_empty_rows>`\\ "
"\\(sparse\\_indices\\, ...\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Fill rows in a sparse matrix that do no contain any values."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`sparse_reshape <tvm.relay.sparse_reshape>`\\ \\(sparse\\_indices\\,"
" prev\\_shape\\, ...\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Reshape a Sparse Tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`sparse_to_dense <tvm.relay.sparse_to_dense>`\\ "
"\\(sparse\\_indices\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:1 tvm.relay:1:<autosummary>:1
msgid "Converts a sparse representation into a dense tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`split <tvm.relay.split>`\\ \\(data\\, indices\\_or\\_sections\\[\\,"
" axis\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.split:1 tvm.relay:1:<autosummary>:1
msgid "Split input tensor along axis by sections or indices."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`sqrt <tvm.relay.sqrt>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.sqrt:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise sqrt of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`squeeze <tvm.relay.squeeze>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.squeeze:1 tvm.relay:1:<autosummary>:1
msgid "Squeeze axes in the array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`stack <tvm.relay.stack>`\\ \\(data\\, axis\\)"
msgstr ""

#: of tvm.relay.op.tensor.stack:1 tvm.relay:1:<autosummary>:1
msgid "Join a sequence of arrays along a new axis."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`std <tvm.relay.std>`\\ \\(data\\[\\, axis\\, keepdims\\, exclude\\,"
" unbiased\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.std:1 tvm.relay:1:<autosummary>:1
msgid "Computes the standard deviation of data over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`strided_set <tvm.relay.strided_set>`\\ \\(data\\, v\\, begin\\, "
"end\\[\\, strides\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.strided_set:1 tvm.relay:1:<autosummary>:1
msgid "Strided set of an array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`strided_slice <tvm.relay.strided_slice>`\\ \\(data\\, begin\\, "
"end\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.strided_slice:1 tvm.relay:1:<autosummary>:1
msgid "Strided slice of an array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`subtract <tvm.relay.subtract>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.op.tensor.subtract:1 tvm.relay:1:<autosummary>:1
msgid "Subtraction with numpy-style broadcasting."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`sum <tvm.relay.sum>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.sum:1 tvm.relay:1:<autosummary>:1
msgid "Computes the sum of array elements over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`take <tvm.relay.take>`\\ \\(data\\, indices\\[\\, axis\\, "
"batch\\_dims\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.take:1 tvm.relay:1:<autosummary>:1
msgid "Take elements from an array along an axis."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`tan <tvm.relay.tan>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.tan:1 tvm.relay:1:<autosummary>:1
msgid "Compute elementwise tan of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`tanh <tvm.relay.tanh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.tanh:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise tanh of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`tile <tvm.relay.tile>`\\ \\(data\\, reps\\)"
msgstr ""

#: of tvm.relay.op.transform.tile:1 tvm.relay:1:<autosummary>:1
msgid "Repeats the whole array multiple times."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`topk <tvm.relay.topk>`\\ \\(data\\[\\, k\\, axis\\, ret\\_type\\, "
"is\\_ascend\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.op.algorithm.topk:1 tvm.relay:1:<autosummary>:1
msgid "Get the top k elements in an input tensor along the given axis."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`transpose <tvm.relay.transpose>`\\ \\(data\\[\\, axes\\]\\)"
msgstr ""

#: of tvm.relay.op.transform.transpose:1 tvm.relay:1:<autosummary>:1
msgid "Permutes the dimensions of an array."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`trunc <tvm.relay.trunc>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.trunc:1 tvm.relay:1:<autosummary>:1
msgid "Compute element-wise trunc of data."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`unique <tvm.relay.unique>`\\ \\(data\\[\\, is\\_sorted\\, "
"return\\_counts\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid "Find the unique elements of a 1-D tensor."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`unravel_index <tvm.relay.unravel_index>`\\ \\(indices\\, shape\\)"
msgstr ""

#: of tvm.relay.op.transform.unravel_index:1 tvm.relay:1:<autosummary>:1
msgid ""
"Convert a flat index or array of flat indices into a tuple of coordinate "
"arrays."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":obj:`variance <tvm.relay.variance>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.op.reduce.variance:1 tvm.relay:1:<autosummary>:1
msgid "Computes the variance of data over given axes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`where <tvm.relay.where>`\\ \\(condition\\, x\\, y\\)"
msgstr ""

#: of tvm.relay.op.transform.where:1 tvm.relay:1:<autosummary>:1
msgid ""
"Selecting elements from either x or y depending on the value of the "
"condition."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`zeros <tvm.relay.zeros>`\\ \\(shape\\, dtype\\)"
msgstr ""

#: of tvm.relay.op.tensor.zeros:1 tvm.relay:1:<autosummary>:1
msgid "Fill array with zeros."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`zeros_like <tvm.relay.zeros_like>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.op.tensor.zeros_like:1 tvm.relay:1:<autosummary>:1
msgid "Returns an array of zeros, with same type and shape as the input."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`save_param_dict <tvm.relay.save_param_dict>`\\ \\(params\\)"
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:1 tvm.relay:1:<autosummary>:1
msgid "Save parameter dictionary to binary bytes."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":obj:`load_param_dict <tvm.relay.load_param_dict>`\\ \\(param\\_bytes\\)"
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:1 tvm.relay:1:<autosummary>:1
msgid "Load parameter dictionary to binary bytes."
msgstr ""

#: of tvm.relay:1
msgid "**Classes:**"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`RefType <tvm.relay.RefType>`\\"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid "alias of :class:`tvm.ir.type.RelayRefType`"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Expr <tvm.relay.Expr>`\\"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid "alias of :class:`tvm.ir.expr.RelayExpr`"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Constant <tvm.relay.Constant>`\\ \\(data\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.Constant:1
msgid "A constant expression in Relay."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Tuple <tvm.relay.Tuple>`\\ \\(fields\\[\\, span\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.Tuple:1
msgid "Tuple expression that groups several fields together."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ""
":obj:`Function <tvm.relay.Function>`\\ \\(params\\, body\\[\\, "
"ret\\_type\\, ...\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.function.Function:1
msgid "A function declaration expression."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ""
":obj:`Call <tvm.relay.Call>`\\ \\(op\\, args\\[\\, attrs\\, "
"type\\_args\\, span\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.Call:1
msgid "Function call node in Relay."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Let <tvm.relay.Let>`\\ \\(variable\\, value\\, body\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.Let:1
msgid "Let variable binding expression."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`If <tvm.relay.If>`\\ \\(cond\\, true\\_branch\\, false\\_branch\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.If:1
msgid "A conditional expression in Relay."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ""
":obj:`TupleGetItem <tvm.relay.TupleGetItem>`\\ \\(tuple\\_value\\, "
"index\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.TupleGetItem:1
msgid "Get index-th item from a tuple."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`RefCreate <tvm.relay.RefCreate>`\\ \\(value\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid "Create a new reference from initial value."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`RefRead <tvm.relay.RefRead>`\\ \\(ref\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid "Get the value inside the reference."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`RefWrite <tvm.relay.RefWrite>`\\ \\(ref\\, value\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid "Update the value inside the reference."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Pattern <tvm.relay.Pattern>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.adt.Pattern:1
msgid "Base type for pattern matching constructs."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`PatternWildcard <tvm.relay.PatternWildcard>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.adt.PatternWildcard:1
msgid "Wildcard pattern in Relay: Matches any ADT and binds nothing."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`PatternVar <tvm.relay.PatternVar>`\\ \\(var\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.adt.PatternVar:1
msgid "Variable pattern in Relay: Matches anything and binds it to the variable."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ""
":obj:`PatternConstructor <tvm.relay.PatternConstructor>`\\ "
"\\(constructor\\[\\, patterns\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.adt.PatternConstructor:1
msgid ""
"Constructor pattern in Relay: Matches an ADT of the given constructor, "
"binds recursively."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`PatternTuple <tvm.relay.PatternTuple>`\\ \\(\\[patterns\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.adt.PatternTuple:1
msgid "Constructor pattern in Relay: Matches a tuple, binds recursively."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ""
":obj:`TypeData <tvm.relay.TypeData>`\\ \\(header\\, type\\_vars\\, "
"constructors\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.ir.adt.TypeData:1
msgid "Stores the definition for an Algebraic Data Type (ADT) in Relay."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Clause <tvm.relay.Clause>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.adt.Clause:1
msgid "Clause for pattern matching in Relay."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Match <tvm.relay.Match>`\\ \\(data\\, clauses\\[\\, complete\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.adt.Match:1
msgid "Pattern matching expression in Relay."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`TypeFunctor <tvm.relay.TypeFunctor>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.type_functor.TypeFunctor:1
msgid "An abstract visitor defined over Type."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`TypeVisitor <tvm.relay.TypeVisitor>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.type_functor.TypeVisitor:1
msgid "A visitor over Type."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`TypeMutator <tvm.relay.TypeMutator>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.type_functor.TypeMutator:1
msgid "A functional visitor over Type."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`ExprFunctor <tvm.relay.ExprFunctor>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.expr_functor.ExprFunctor:1
msgid "An abstract visitor defined over Expr."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`ExprVisitor <tvm.relay.ExprVisitor>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.expr_functor.ExprVisitor:1
msgid "A visitor over Expr."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`ExprMutator <tvm.relay.ExprMutator>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.expr_functor.ExprMutator:1
msgid "A functional visitor over Expr."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`Prelude <tvm.relay.Prelude>`\\ \\(\\[mod\\]\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.prelude.Prelude:1
msgid "Contains standard definitions."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`TupleWrapper <tvm.relay.TupleWrapper>`\\ \\(tuple\\_value\\, size\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1 tvm.relay.expr.TupleWrapper:1
msgid "TupleWrapper."
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
msgid ":obj:`ScopeBuilder <tvm.relay.ScopeBuilder>`\\ \\(\\)"
msgstr ""

#: of sys.setrecursionlimit:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder:1
msgid "Scope builder class."
msgstr ""

#: of sys.setrecursionlimit:1
msgid ""
"Set the maximum depth of the Python interpreter stack to n.  This limit "
"prevents infinite recursion from causing an overflow of the C stack and "
"crashing Python.  The highest possible limit is platform- dependent."
msgstr ""

#: of tvm.ir.adt.TypeData tvm.relay.build_module.build
#: tvm.relay.build_module.create_executor tvm.relay.build_module.optimize
#: tvm.relay.expr.Call tvm.relay.expr.Constant tvm.relay.expr.If
#: tvm.relay.expr.Let tvm.relay.expr.Tuple tvm.relay.expr.Tuple.astype
#: tvm.relay.expr.TupleGetItem tvm.relay.expr.TupleWrapper tvm.relay.expr.bind
#: tvm.relay.expr.const tvm.relay.expr.var tvm.relay.function.Function
#: tvm.relay.op.algorithm.argsort tvm.relay.op.algorithm.sort
#: tvm.relay.op.algorithm.topk tvm.relay.op.reduce.all tvm.relay.op.reduce.any
#: tvm.relay.op.reduce.argmax tvm.relay.op.reduce.argmin
#: tvm.relay.op.reduce.logsumexp tvm.relay.op.reduce.max
#: tvm.relay.op.reduce.mean tvm.relay.op.reduce.mean_std
#: tvm.relay.op.reduce.mean_variance tvm.relay.op.reduce.min
#: tvm.relay.op.reduce.prod tvm.relay.op.reduce.std tvm.relay.op.reduce.sum
#: tvm.relay.op.reduce.variance tvm.relay.op.tensor.abs
#: tvm.relay.op.tensor.acos tvm.relay.op.tensor.acosh tvm.relay.op.tensor.add
#: tvm.relay.op.tensor.asin tvm.relay.op.tensor.asinh tvm.relay.op.tensor.atan
#: tvm.relay.op.tensor.atanh tvm.relay.op.tensor.bitwise_and
#: tvm.relay.op.tensor.bitwise_not tvm.relay.op.tensor.bitwise_or
#: tvm.relay.op.tensor.bitwise_xor tvm.relay.op.tensor.ceil
#: tvm.relay.op.tensor.clip tvm.relay.op.tensor.concatenate
#: tvm.relay.op.tensor.copy tvm.relay.op.tensor.cos tvm.relay.op.tensor.cosh
#: tvm.relay.op.tensor.device_copy tvm.relay.op.tensor.divide
#: tvm.relay.op.tensor.equal tvm.relay.op.tensor.erf tvm.relay.op.tensor.exp
#: tvm.relay.op.tensor.fixed_point_multiply tvm.relay.op.tensor.floor
#: tvm.relay.op.tensor.floor_divide tvm.relay.op.tensor.floor_mod
#: tvm.relay.op.tensor.greater tvm.relay.op.tensor.greater_equal
#: tvm.relay.op.tensor.isfinite tvm.relay.op.tensor.isinf
#: tvm.relay.op.tensor.isnan tvm.relay.op.tensor.left_shift
#: tvm.relay.op.tensor.less tvm.relay.op.tensor.less_equal
#: tvm.relay.op.tensor.log tvm.relay.op.tensor.log10 tvm.relay.op.tensor.log2
#: tvm.relay.op.tensor.logical_and tvm.relay.op.tensor.logical_not
#: tvm.relay.op.tensor.logical_or tvm.relay.op.tensor.logical_xor
#: tvm.relay.op.tensor.maximum tvm.relay.op.tensor.minimum
#: tvm.relay.op.tensor.mod tvm.relay.op.tensor.multiply
#: tvm.relay.op.tensor.ndarray_size tvm.relay.op.tensor.negative
#: tvm.relay.op.tensor.not_equal tvm.relay.op.tensor.ones
#: tvm.relay.op.tensor.ones_like tvm.relay.op.tensor.power
#: tvm.relay.op.tensor.right_shift tvm.relay.op.tensor.round
#: tvm.relay.op.tensor.rsqrt tvm.relay.op.tensor.shape_of
#: tvm.relay.op.tensor.sigmoid tvm.relay.op.tensor.sign tvm.relay.op.tensor.sin
#: tvm.relay.op.tensor.sinh tvm.relay.op.tensor.sqrt tvm.relay.op.tensor.stack
#: tvm.relay.op.tensor.subtract tvm.relay.op.tensor.tan
#: tvm.relay.op.tensor.tanh tvm.relay.op.tensor.trunc tvm.relay.op.tensor.zeros
#: tvm.relay.op.tensor.zeros_like tvm.relay.op.transform.adv_index
#: tvm.relay.op.transform.arange tvm.relay.op.transform.argwhere
#: tvm.relay.op.transform.broadcast_to tvm.relay.op.transform.broadcast_to_like
#: tvm.relay.op.transform.cast tvm.relay.op.transform.cast_like
#: tvm.relay.op.transform.collapse_sum_like
#: tvm.relay.op.transform.collapse_sum_to tvm.relay.op.transform.cumprod
#: tvm.relay.op.transform.cumsum tvm.relay.op.transform.expand_dims
#: tvm.relay.op.transform.full tvm.relay.op.transform.full_like
#: tvm.relay.op.transform.gather tvm.relay.op.transform.gather_nd
#: tvm.relay.op.transform.invert_permutation
#: tvm.relay.op.transform.layout_transform
#: tvm.relay.op.transform.matrix_set_diag tvm.relay.op.transform.meshgrid
#: tvm.relay.op.transform.one_hot tvm.relay.op.transform.reinterpret
#: tvm.relay.op.transform.reshape tvm.relay.op.transform.reshape_like
#: tvm.relay.op.transform.reverse tvm.relay.op.transform.reverse_reshape
#: tvm.relay.op.transform.reverse_sequence tvm.relay.op.transform.scatter
#: tvm.relay.op.transform.scatter_add tvm.relay.op.transform.scatter_nd
#: tvm.relay.op.transform.segment_sum tvm.relay.op.transform.sequence_mask
#: tvm.relay.op.transform.slice_like
#: tvm.relay.op.transform.sparse_fill_empty_rows
#: tvm.relay.op.transform.sparse_reshape tvm.relay.op.transform.sparse_to_dense
#: tvm.relay.op.transform.split tvm.relay.op.transform.squeeze
#: tvm.relay.op.transform.strided_set tvm.relay.op.transform.strided_slice
#: tvm.relay.op.transform.take tvm.relay.op.transform.tile
#: tvm.relay.op.transform.transpose tvm.relay.op.transform.unique
#: tvm.relay.op.transform.unravel_index tvm.relay.op.transform.where
#: tvm.relay.param_dict.load_param_dict tvm.relay.param_dict.save_param_dict
#: tvm.relay.scope_builder.ScopeBuilder.if_scope
#: tvm.relay.scope_builder.ScopeBuilder.let
#: tvm.relay.scope_builder.ScopeBuilder.ret
#: tvm.relay.scope_builder.ScopeBuilder.type_of
#: tvm.relay.transform.transform.build_config tvm.relay.ty.ShapeVar
#: tvm.relay.ty.scalar_type
msgid "Parameters"
msgstr ""

#: of tvm.relay.build_module.build:3
msgid "The IR module to build. Using relay.Function is deprecated."
msgstr ""

#: of tvm.relay.build_module.build:5 tvm.relay.build_module.optimize:7
#: tvm.relay.build_module.optimize:9
msgid ""
"For heterogeneous compilation, it is a dictionary indicating context to "
"target mapping. For homogeneous compilation, it is a build target."
msgstr ""

#: of tvm.relay.build_module.build:8
msgid ""
"Host compilation target, if target is device. When TVM compiles device "
"specific program such as CUDA, we also need host(CPU) side code to "
"interact with the driver setup the dimensions and parameters correctly. "
"target_host is used to specify the host side codegen target. By default, "
"llvm is used if it is enabled, otherwise a stackvm intepreter is used."
msgstr ""

#: of tvm.relay.build_module.build:16 tvm.relay.build_module.optimize:11
msgid ""
"Input parameters to the graph that do not change during inference time. "
"Used for constant folding."
msgstr ""

#: of tvm.relay.build_module.build:19
msgid "The module name we will build"
msgstr ""

#: of tvm.relay.build_module.build tvm.relay.build_module.create_executor
#: tvm.relay.build_module.optimize tvm.relay.expr.Tuple.astype
#: tvm.relay.expr.TupleWrapper.astext tvm.relay.expr.bind
#: tvm.relay.op.algorithm.argsort tvm.relay.op.algorithm.sort
#: tvm.relay.op.algorithm.topk tvm.relay.op.reduce.all tvm.relay.op.reduce.any
#: tvm.relay.op.reduce.argmax tvm.relay.op.reduce.argmin
#: tvm.relay.op.reduce.logsumexp tvm.relay.op.reduce.max
#: tvm.relay.op.reduce.mean tvm.relay.op.reduce.mean_std
#: tvm.relay.op.reduce.mean_variance tvm.relay.op.reduce.min
#: tvm.relay.op.reduce.prod tvm.relay.op.reduce.std tvm.relay.op.reduce.sum
#: tvm.relay.op.reduce.variance tvm.relay.op.tensor.abs
#: tvm.relay.op.tensor.acos tvm.relay.op.tensor.acosh tvm.relay.op.tensor.add
#: tvm.relay.op.tensor.asin tvm.relay.op.tensor.asinh tvm.relay.op.tensor.atan
#: tvm.relay.op.tensor.atanh tvm.relay.op.tensor.bitwise_and
#: tvm.relay.op.tensor.bitwise_not tvm.relay.op.tensor.bitwise_or
#: tvm.relay.op.tensor.bitwise_xor tvm.relay.op.tensor.ceil
#: tvm.relay.op.tensor.clip tvm.relay.op.tensor.concatenate
#: tvm.relay.op.tensor.copy tvm.relay.op.tensor.cos tvm.relay.op.tensor.cosh
#: tvm.relay.op.tensor.device_copy tvm.relay.op.tensor.divide
#: tvm.relay.op.tensor.equal tvm.relay.op.tensor.erf tvm.relay.op.tensor.exp
#: tvm.relay.op.tensor.fixed_point_multiply tvm.relay.op.tensor.floor
#: tvm.relay.op.tensor.floor_divide tvm.relay.op.tensor.floor_mod
#: tvm.relay.op.tensor.greater tvm.relay.op.tensor.greater_equal
#: tvm.relay.op.tensor.isfinite tvm.relay.op.tensor.isinf
#: tvm.relay.op.tensor.isnan tvm.relay.op.tensor.left_shift
#: tvm.relay.op.tensor.less tvm.relay.op.tensor.less_equal
#: tvm.relay.op.tensor.log tvm.relay.op.tensor.log10 tvm.relay.op.tensor.log2
#: tvm.relay.op.tensor.logical_and tvm.relay.op.tensor.logical_not
#: tvm.relay.op.tensor.logical_or tvm.relay.op.tensor.logical_xor
#: tvm.relay.op.tensor.maximum tvm.relay.op.tensor.minimum
#: tvm.relay.op.tensor.mod tvm.relay.op.tensor.multiply
#: tvm.relay.op.tensor.ndarray_size tvm.relay.op.tensor.negative
#: tvm.relay.op.tensor.not_equal tvm.relay.op.tensor.ones
#: tvm.relay.op.tensor.ones_like tvm.relay.op.tensor.power
#: tvm.relay.op.tensor.right_shift tvm.relay.op.tensor.round
#: tvm.relay.op.tensor.rsqrt tvm.relay.op.tensor.shape_of
#: tvm.relay.op.tensor.sigmoid tvm.relay.op.tensor.sign tvm.relay.op.tensor.sin
#: tvm.relay.op.tensor.sinh tvm.relay.op.tensor.sqrt tvm.relay.op.tensor.stack
#: tvm.relay.op.tensor.subtract tvm.relay.op.tensor.tan
#: tvm.relay.op.tensor.tanh tvm.relay.op.tensor.trunc tvm.relay.op.tensor.zeros
#: tvm.relay.op.tensor.zeros_like tvm.relay.op.transform.adv_index
#: tvm.relay.op.transform.arange tvm.relay.op.transform.argwhere
#: tvm.relay.op.transform.broadcast_to tvm.relay.op.transform.broadcast_to_like
#: tvm.relay.op.transform.cast tvm.relay.op.transform.cast_like
#: tvm.relay.op.transform.collapse_sum_like
#: tvm.relay.op.transform.collapse_sum_to tvm.relay.op.transform.cumprod
#: tvm.relay.op.transform.cumsum tvm.relay.op.transform.expand_dims
#: tvm.relay.op.transform.full tvm.relay.op.transform.full_like
#: tvm.relay.op.transform.gather_nd tvm.relay.op.transform.invert_permutation
#: tvm.relay.op.transform.layout_transform
#: tvm.relay.op.transform.matrix_set_diag tvm.relay.op.transform.meshgrid
#: tvm.relay.op.transform.one_hot tvm.relay.op.transform.reinterpret
#: tvm.relay.op.transform.repeat tvm.relay.op.transform.reshape
#: tvm.relay.op.transform.reshape_like tvm.relay.op.transform.reverse
#: tvm.relay.op.transform.reverse_reshape
#: tvm.relay.op.transform.reverse_sequence tvm.relay.op.transform.scatter
#: tvm.relay.op.transform.scatter_add tvm.relay.op.transform.scatter_nd
#: tvm.relay.op.transform.segment_sum tvm.relay.op.transform.sequence_mask
#: tvm.relay.op.transform.slice_like
#: tvm.relay.op.transform.sparse_fill_empty_rows
#: tvm.relay.op.transform.sparse_reshape tvm.relay.op.transform.sparse_to_dense
#: tvm.relay.op.transform.split tvm.relay.op.transform.squeeze
#: tvm.relay.op.transform.strided_set tvm.relay.op.transform.strided_slice
#: tvm.relay.op.transform.take tvm.relay.op.transform.tile
#: tvm.relay.op.transform.transpose tvm.relay.op.transform.unique
#: tvm.relay.op.transform.unravel_index tvm.relay.op.transform.where
#: tvm.relay.param_dict.load_param_dict tvm.relay.param_dict.save_param_dict
#: tvm.relay.scope_builder.ScopeBuilder.else_scope
#: tvm.relay.scope_builder.ScopeBuilder.get
#: tvm.relay.scope_builder.ScopeBuilder.if_scope
#: tvm.relay.transform.transform.build_config tvm.relay.ty.ShapeVar
#: tvm.relay.ty.scalar_type tvm.te.hybrid.script
msgid "Returns"
msgstr ""

#: of tvm.relay.build_module.build:22
msgid "**factory_module** -- The runtime factory for the TVM graph executor."
msgstr ""

#: of tvm.relay.build_module.build tvm.relay.build_module.create_executor
#: tvm.relay.expr.Tuple.astype tvm.relay.expr.TupleWrapper.astext
#: tvm.relay.expr.bind tvm.relay.op.algorithm.argsort
#: tvm.relay.op.algorithm.sort tvm.relay.op.algorithm.topk
#: tvm.relay.op.reduce.all tvm.relay.op.reduce.any tvm.relay.op.reduce.argmax
#: tvm.relay.op.reduce.argmin tvm.relay.op.reduce.logsumexp
#: tvm.relay.op.reduce.max tvm.relay.op.reduce.mean
#: tvm.relay.op.reduce.mean_std tvm.relay.op.reduce.mean_variance
#: tvm.relay.op.reduce.min tvm.relay.op.reduce.prod tvm.relay.op.reduce.std
#: tvm.relay.op.reduce.sum tvm.relay.op.reduce.variance tvm.relay.op.tensor.abs
#: tvm.relay.op.tensor.acos tvm.relay.op.tensor.acosh tvm.relay.op.tensor.add
#: tvm.relay.op.tensor.asin tvm.relay.op.tensor.asinh tvm.relay.op.tensor.atan
#: tvm.relay.op.tensor.atanh tvm.relay.op.tensor.bitwise_and
#: tvm.relay.op.tensor.bitwise_not tvm.relay.op.tensor.bitwise_or
#: tvm.relay.op.tensor.bitwise_xor tvm.relay.op.tensor.ceil
#: tvm.relay.op.tensor.clip tvm.relay.op.tensor.concatenate
#: tvm.relay.op.tensor.copy tvm.relay.op.tensor.cos tvm.relay.op.tensor.cosh
#: tvm.relay.op.tensor.device_copy tvm.relay.op.tensor.divide
#: tvm.relay.op.tensor.equal tvm.relay.op.tensor.erf tvm.relay.op.tensor.exp
#: tvm.relay.op.tensor.fixed_point_multiply tvm.relay.op.tensor.floor
#: tvm.relay.op.tensor.floor_divide tvm.relay.op.tensor.floor_mod
#: tvm.relay.op.tensor.greater tvm.relay.op.tensor.greater_equal
#: tvm.relay.op.tensor.isfinite tvm.relay.op.tensor.isinf
#: tvm.relay.op.tensor.isnan tvm.relay.op.tensor.left_shift
#: tvm.relay.op.tensor.less tvm.relay.op.tensor.less_equal
#: tvm.relay.op.tensor.log tvm.relay.op.tensor.log10 tvm.relay.op.tensor.log2
#: tvm.relay.op.tensor.logical_and tvm.relay.op.tensor.logical_not
#: tvm.relay.op.tensor.logical_or tvm.relay.op.tensor.logical_xor
#: tvm.relay.op.tensor.maximum tvm.relay.op.tensor.minimum
#: tvm.relay.op.tensor.mod tvm.relay.op.tensor.multiply
#: tvm.relay.op.tensor.ndarray_size tvm.relay.op.tensor.negative
#: tvm.relay.op.tensor.not_equal tvm.relay.op.tensor.ones
#: tvm.relay.op.tensor.ones_like tvm.relay.op.tensor.power
#: tvm.relay.op.tensor.right_shift tvm.relay.op.tensor.round
#: tvm.relay.op.tensor.rsqrt tvm.relay.op.tensor.shape_of
#: tvm.relay.op.tensor.sigmoid tvm.relay.op.tensor.sign tvm.relay.op.tensor.sin
#: tvm.relay.op.tensor.sinh tvm.relay.op.tensor.sqrt tvm.relay.op.tensor.stack
#: tvm.relay.op.tensor.subtract tvm.relay.op.tensor.tan
#: tvm.relay.op.tensor.tanh tvm.relay.op.tensor.trunc tvm.relay.op.tensor.zeros
#: tvm.relay.op.tensor.zeros_like tvm.relay.op.transform.adv_index
#: tvm.relay.op.transform.arange tvm.relay.op.transform.argwhere
#: tvm.relay.op.transform.broadcast_to tvm.relay.op.transform.broadcast_to_like
#: tvm.relay.op.transform.cast tvm.relay.op.transform.cast_like
#: tvm.relay.op.transform.collapse_sum_like
#: tvm.relay.op.transform.collapse_sum_to tvm.relay.op.transform.cumprod
#: tvm.relay.op.transform.cumsum tvm.relay.op.transform.expand_dims
#: tvm.relay.op.transform.full tvm.relay.op.transform.full_like
#: tvm.relay.op.transform.gather_nd tvm.relay.op.transform.invert_permutation
#: tvm.relay.op.transform.layout_transform
#: tvm.relay.op.transform.matrix_set_diag tvm.relay.op.transform.meshgrid
#: tvm.relay.op.transform.one_hot tvm.relay.op.transform.reinterpret
#: tvm.relay.op.transform.repeat tvm.relay.op.transform.reshape
#: tvm.relay.op.transform.reshape_like tvm.relay.op.transform.reverse
#: tvm.relay.op.transform.reverse_reshape
#: tvm.relay.op.transform.reverse_sequence tvm.relay.op.transform.scatter
#: tvm.relay.op.transform.scatter_add tvm.relay.op.transform.scatter_nd
#: tvm.relay.op.transform.segment_sum tvm.relay.op.transform.sequence_mask
#: tvm.relay.op.transform.slice_like tvm.relay.op.transform.sparse_reshape
#: tvm.relay.op.transform.sparse_to_dense tvm.relay.op.transform.split
#: tvm.relay.op.transform.squeeze tvm.relay.op.transform.strided_set
#: tvm.relay.op.transform.strided_slice tvm.relay.op.transform.take
#: tvm.relay.op.transform.tile tvm.relay.op.transform.transpose
#: tvm.relay.op.transform.unravel_index tvm.relay.op.transform.where
#: tvm.relay.param_dict.load_param_dict tvm.relay.param_dict.save_param_dict
#: tvm.relay.scope_builder.ScopeBuilder.else_scope
#: tvm.relay.scope_builder.ScopeBuilder.get
#: tvm.relay.scope_builder.ScopeBuilder.if_scope
#: tvm.relay.transform.transform.build_config tvm.relay.ty.ShapeVar
#: tvm.relay.ty.scalar_type tvm.te.hybrid.script
msgid "Return type"
msgstr ""

#: of tvm.relay.build_module.create_executor:4
msgid "Example"
msgstr ""

#: of tvm.relay.build_module.create_executor:17
msgid ""
"The type of executor. Avaliable options are `debug` for the interpreter, "
"`graph` for the graph executor, and `vm` for the virtual machine."
msgstr ""

#: of tvm.relay.build_module.create_executor:21
msgid "The Relay module containing collection of functions"
msgstr ""

#: of tvm.relay.build_module.create_executor:23
msgid "The device to execute the code."
msgstr ""

#: of tvm.relay.build_module.create_executor:25
msgid "The corresponding context"
msgstr ""

#: of tvm.relay.build_module.create_executor:27
msgid "Input parameters to the graph that do not change during inference time."
msgstr ""

#: of tvm.relay.build_module.create_executor:31
msgid "**executor**"
msgstr ""

#: of tvm.relay.build_module.create_executor:32
msgid ":py:class:`~tvm.relay.backend.interpreter.Executor`"
msgstr ""

#: of tvm.relay.build_module.optimize:3
msgid "The module to build. Using relay.Function is deprecated."
msgstr ""

#: of tvm.relay.build_module.optimize:15
msgid ""
"* **mod** (:py:class:`~tvm.IRModule`) -- The optimized relay module. * "
"**params** (*dict*) -- The parameters of the final graph."
msgstr ""

#: of tvm.relay.build_module.optimize:15
msgid "**mod** (:py:class:`~tvm.IRModule`) -- The optimized relay module."
msgstr ""

#: of tvm.relay.build_module.optimize:16
msgid "**params** (*dict*) -- The parameters of the final graph."
msgstr ""

#: of tvm.relay.transform.transform.build_config:1
msgid ""
"Configure the build behavior by setting config variables. This function "
"will be deprecated in TVM v0.7. Instead, we should directly use "
"tvm.transform.PassContext."
msgstr ""

#: of tvm.relay.transform.transform.build_config:5
msgid ""
"Optimization level. The optimization pass name and level are as the "
"following:  .. code-block:: python      OPT_PASS_LEVEL = {         "
"\"SimplifyInference\": 0,         \"OpFusion\": 1,         "
"\"FoldConstant\": 2,         \"FoldScaleAxis\": 3,         "
"\"AlterOpLayout\": 3,         \"CanonicalizeOps\": 3,         "
"\"CanonicalizeCast\": 3,         \"EliminateCommonSubexpr\": 3,         "
"\"CombineParallelConv2D\": 4,         \"CombineParallelDense\": 4,"
"         \"CombineParallelBatchMatmul\": 4,         \"FastMath\": 4     }"
msgstr ""

#: of tvm.relay.transform.transform.build_config:5
msgid ""
"Optimization level. The optimization pass name and level are as the "
"following:"
msgstr ""

#: of tvm.relay.transform.transform.build_config:25
msgid "Optimization passes that are required regardless of optimization level."
msgstr ""

#: of tvm.relay.transform.transform.build_config:27
msgid "Optimization passes to be disabled during optimization."
msgstr ""

#: of tvm.relay.transform.transform.build_config:29
msgid "A tracing function for debugging or introspection."
msgstr ""

#: of tvm.relay.transform.transform.build_config:32
msgid "**pass_context** -- The pass context for optimizations."
msgstr ""

#: of tvm.relay.ty.ShapeVar:6
msgid "**type_var** -- The shape variable."
msgstr ""

#: of tvm.relay.ty.scalar_type:3
msgid "This function returns TensorType((), dtype)"
msgstr ""

#: of tvm.relay.ty.scalar_type:5
msgid "The content data type."
msgstr ""

#: of tvm.relay.ty.scalar_type:8
msgid "**s_type** -- The result type."
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":obj:`checked_type <tvm.relay.Expr.checked_type>`\\"
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the checked type of tvm.relay.Expr."
msgstr ""

#: of tvm.relay.expr.Constant:3
msgid "The data content of the constant expression."
msgstr ""

#: of tvm.relay.expr.Tuple:3
msgid "The fields in the tuple."
msgstr ""

#: of tvm.relay.expr.Call:15 tvm.relay.expr.Tuple:5
msgid "Span that points to original source code"
msgstr ""

#: of tvm.relay.expr.Tuple:1 tvm.relay.expr.TupleWrapper:1
#: tvm.relay.expr_functor.ExprFunctor:1 tvm.relay.prelude.Prelude:1
#: tvm.relay.scope_builder.ScopeBuilder:1 tvm.relay.type_functor.TypeFunctor:1
msgid "**Methods:**"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:1:<autosummary>:1
msgid ":obj:`astype <tvm.relay.Tuple.astype>`\\ \\(\\_\\)"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:1
#: tvm.relay.expr.Tuple.astype:1:<autosummary>:1
msgid "Cast the content type of the current data to dtype."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:3 tvm.relay.op.tensor.ndarray_size:5
#: tvm.relay.op.tensor.shape_of:5 tvm.relay.op.transform.arange:17
msgid "The target data type."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:6
msgid "This function only works for TensorType Exprs."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:8
msgid "**result** -- The result expression."
msgstr ""

#: of tvm.relay.function.Function:3
msgid "List of input parameters to the function."
msgstr ""

#: of tvm.relay.function.Function:5
msgid "The body of the function."
msgstr ""

#: of tvm.relay.function.Function:7
msgid "The return type annotation of the function."
msgstr ""

#: of tvm.relay.function.Function:9
msgid ""
"The additional type parameters, this is only used in advanced usecase of "
"template functions."
msgstr ""

#: of tvm.relay.expr.Call:3
msgid ""
"Call node corresponds the operator application node in computational "
"graph terminology."
msgstr ""

#: of tvm.relay.expr.Call:6
msgid "The operation to be called."
msgstr ""

#: of tvm.relay.expr.Call:8
msgid "The arguments to the call."
msgstr ""

#: of tvm.relay.expr.Call:10
msgid "Attributes to the call, can be None"
msgstr ""

#: of tvm.relay.expr.Call:12
msgid ""
"The additional type arguments, this is only used in advanced usecase of "
"template functions."
msgstr ""

#: of tvm.relay.expr.Let:3
msgid "The local variable to be bound."
msgstr ""

#: of tvm.relay.expr.Let:5
msgid "The value to be bound."
msgstr ""

#: of tvm.relay.expr.Let:7
msgid "The body of the let binding."
msgstr ""

#: of tvm.relay.expr.If:3
msgid "The condition."
msgstr ""

#: of tvm.relay.expr.If:5
msgid "The expression evaluated when condition is true."
msgstr ""

#: of tvm.relay.expr.If:7
msgid "The expression evaluated when condition is false."
msgstr ""

#: of tvm.relay.expr.TupleGetItem:3
msgid "The input tuple expression."
msgstr ""

#: of tvm.relay.expr.TupleGetItem:5
msgid "The index."
msgstr ""

#: of tvm.relay.expr.RefCreate:1
msgid ""
"Create a new reference from initial value. :param value: The initial "
"value. :type value: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.RefRead:1
msgid ""
"Get the value inside the reference. :param ref: The reference. :type ref:"
" tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.RefWrite:1
msgid ""
"Update the value inside the reference. The whole expression will evaluate"
" to an empty tuple. :param ref: The reference. :type ref: tvm.relay.Expr "
":param value: The new value. :type value: tvm.relay.Expr"
msgstr ""

#: of tvm.ir.adt.TypeData:3
msgid ""
"Note that ADT definitions are treated as type-level functions because the"
" type parameters need to be given for an instance of the ADT. Thus, any "
"global type var that is an ADT header needs to be wrapped in a type call "
"that passes in the type params."
msgstr ""

#: of tvm.ir.adt.TypeData:8
msgid ""
"The name of the ADT. ADTs with the same constructors but different names "
"are treated as different types."
msgstr ""

#: of tvm.ir.adt.TypeData:12
msgid "Type variables that appear in constructors."
msgstr ""

#: of tvm.ir.adt.TypeData:14
msgid "The constructors for the ADT."
msgstr ""

#: of tvm.ir.adt.TypeData:17
msgid "Alias of :py:func:`tvm.ir.TypeData`"
msgstr ""

#: of tvm.relay.expr.var:3
msgid ""
"This is a simple wrapper function that allows specify shape and dtype "
"directly."
msgstr ""

#: of tvm.relay.expr.var:6
msgid ""
"The name of the variable. This name only acts as a hint, and is not used "
"for equality."
msgstr ""

#: of tvm.relay.expr.var:10
msgid ""
"The type annotation on the variable. When type_annotation is a str, we "
"will create a scalar variable."
msgstr ""

#: of tvm.relay.expr.var:13
msgid "The shape of the tensor type."
msgstr ""

#: of tvm.relay.expr.var:15
msgid "The data type of the tensor."
msgstr ""

#: of tvm.relay.expr.var:19 tvm.relay.op.reduce.all:21
#: tvm.relay.op.reduce.any:21 tvm.relay.op.tensor.add:12
#: tvm.relay.op.tensor.clip:15 tvm.relay.op.transform.arange:24
#: tvm.relay.op.transform.argwhere:11 tvm.relay.op.transform.cumprod:23
#: tvm.relay.op.transform.cumsum:23 tvm.relay.op.transform.gather:22
#: tvm.relay.op.transform.gather_nd:18
#: tvm.relay.op.transform.invert_permutation:16
#: tvm.relay.op.transform.matrix_set_diag:25 tvm.relay.op.transform.meshgrid:15
#: tvm.relay.op.transform.one_hot:22 tvm.relay.op.transform.repeat:16
#: tvm.relay.op.transform.reshape_like:30 tvm.relay.op.transform.reverse:13
#: tvm.relay.op.transform.reverse_sequence:20
#: tvm.relay.op.transform.segment_sum:26
#: tvm.relay.op.transform.sequence_mask:19
#: tvm.relay.op.transform.sparse_fill_empty_rows:31
#: tvm.relay.op.transform.sparse_reshape:15 tvm.relay.op.transform.tile:12
#: tvm.relay.op.transform.unique:19 tvm.relay.op.transform.where:21
#: tvm.relay.param_dict.save_param_dict:16
#: tvm.relay.scope_builder.ScopeBuilder:7
msgid "Examples"
msgstr ""

#: of tvm.relay.expr.const:3
msgid "The constant value."
msgstr ""

#: of tvm.relay.expr.const:5
msgid "The data type of the resulting constant."
msgstr ""

#: of tvm.relay.expr.const:10
msgid "When dtype is None, we use the following rule:"
msgstr ""

#: of tvm.relay.expr.const:12
msgid "int maps to \"int32\""
msgstr ""

#: of tvm.relay.expr.const:13
msgid "float maps to \"float32\""
msgstr ""

#: of tvm.relay.expr.const:14
msgid "bool maps to \"bool\""
msgstr ""

#: of tvm.relay.expr.const:15
msgid "other using the same default rule as numpy."
msgstr ""

#: of tvm.relay.expr.bind:3
msgid "We can bind parameters expr if it is a function."
msgstr ""

#: of tvm.relay.expr.bind:5
msgid "The input expression."
msgstr ""

#: of tvm.relay.expr.bind:7
msgid "The specific bindings."
msgstr ""

#: of tvm.relay.expr.bind:10
msgid "**result** -- The expression or function after binding."
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor:3
msgid "Defines the default dispatch over types."
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor.visit:1:<autosummary>:1
msgid ":obj:`visit <tvm.relay.TypeFunctor.visit>`\\ \\(typ\\)"
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor.visit:1
#: tvm.relay.type_functor.TypeFunctor.visit:1:<autosummary>:1
msgid "Apply the visitor to a type."
msgstr ""

#: of tvm.relay.expr_functor.ExprVisitor:3 tvm.relay.type_functor.TypeVisitor:3
msgid "The default behavior recursively traverses the AST."
msgstr ""

#: of tvm.relay.expr_functor.ExprMutator:3 tvm.relay.type_functor.TypeMutator:3
msgid ""
"The default behavior recursively traverses the AST and reconstructs the "
"AST."
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor:3
msgid "Defines the default dispatch over expressions, and implements memoization."
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor.visit:1:<autosummary>:1
msgid ":obj:`visit <tvm.relay.ExprFunctor.visit>`\\ \\(expr\\)"
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor.visit:1
#: tvm.relay.expr_functor.ExprFunctor.visit:1:<autosummary>:1
msgid "Apply the visitor to an expression."
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ":obj:`get_name <tvm.relay.Prelude.get_name>`\\ \\(canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1
#: tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_name_static:1
msgid "Get name corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_global_var <tvm.relay.Prelude.get_global_var>`\\ "
"\\(canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_global_var:1
#: tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid "Get global var corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ":obj:`get_type <tvm.relay.Prelude.get_type>`\\ \\(canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_type:1
#: tvm.relay.prelude.Prelude.get_type_static:1
msgid "Get type corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_ctor <tvm.relay.Prelude.get_ctor>`\\ \\(ty\\_name\\, "
"canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1
#: tvm.relay.prelude.Prelude.get_ctor_static:1
#: tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_tensor_ctor_static:1
msgid "Get constructor corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_name_static <tvm.relay.Prelude.get_name_static>`\\ "
"\\(canonical\\, dtype\\, shape\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_global_var_static <tvm.relay.Prelude.get_global_var_static>`\\ "
"\\(canonical\\, dtype\\, shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_global_var_static:1
#: tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid "Get var corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_type_static <tvm.relay.Prelude.get_type_static>`\\ "
"\\(canonical\\, dtype\\, shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_ctor_static <tvm.relay.Prelude.get_ctor_static>`\\ "
"\\(ty\\_name\\, name\\, dtype\\, shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ""
":obj:`get_tensor_ctor_static "
"<tvm.relay.Prelude.get_tensor_ctor_static>`\\ \\(name\\, dtype\\, "
"shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
msgid ":obj:`load_prelude <tvm.relay.Prelude.load_prelude>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_name:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.load_prelude:1
msgid "Parses the Prelude from Relay's text format into a module."
msgstr ""

#: of tvm.relay.expr.TupleWrapper:3
msgid ""
"This class is a Python wrapper for a Relay tuple of known size. It allows"
" for accessing the fields of the Relay tuple as though it were a Python "
"tuple."
msgstr ""

#: of tvm.relay.expr.TupleWrapper:7
msgid "The input tuple"
msgstr ""

#: of tvm.relay.expr.TupleWrapper:9
msgid "The size of the tuple."
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astuple:1:<autosummary>:1
msgid ":obj:`astuple <tvm.relay.TupleWrapper.astuple>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astuple:1
#: tvm.relay.expr.TupleWrapper.astuple:1:<autosummary>:1
msgid ""
"Returns the underlying Relay tuple if this wrapper is passed as an "
"argument to an FFI function."
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astuple:1:<autosummary>:1
msgid ":obj:`astext <tvm.relay.TupleWrapper.astext>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:1
#: tvm.relay.expr.TupleWrapper.astuple:1:<autosummary>:1
msgid "Get the text format of the tuple expression."
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:3
msgid "**text** -- The text format of the tuple expression."
msgstr ""

#: of tvm.relay.op.reduce.argmax:3 tvm.relay.op.reduce.argmin:3
#: tvm.relay.op.reduce.logsumexp:7 tvm.relay.op.reduce.max:3
#: tvm.relay.op.reduce.mean:3 tvm.relay.op.reduce.mean_std:3
#: tvm.relay.op.reduce.mean_variance:3 tvm.relay.op.reduce.min:3
#: tvm.relay.op.reduce.prod:3 tvm.relay.op.reduce.std:3
#: tvm.relay.op.reduce.sum:3 tvm.relay.op.reduce.variance:3
#: tvm.relay.op.tensor.abs:3 tvm.relay.op.tensor.acos:3
#: tvm.relay.op.tensor.acosh:3 tvm.relay.op.tensor.asin:3
#: tvm.relay.op.tensor.asinh:3 tvm.relay.op.tensor.atan:3
#: tvm.relay.op.tensor.atanh:3 tvm.relay.op.tensor.bitwise_not:3
#: tvm.relay.op.tensor.ceil:3 tvm.relay.op.tensor.cos:3
#: tvm.relay.op.tensor.cosh:3 tvm.relay.op.tensor.erf:3
#: tvm.relay.op.tensor.exp:3 tvm.relay.op.tensor.floor:3
#: tvm.relay.op.tensor.isfinite:3 tvm.relay.op.tensor.isinf:3
#: tvm.relay.op.tensor.isnan:3 tvm.relay.op.tensor.log:3
#: tvm.relay.op.tensor.log10:3 tvm.relay.op.tensor.log2:3
#: tvm.relay.op.tensor.logical_not:3 tvm.relay.op.tensor.negative:3
#: tvm.relay.op.tensor.ones_like:3 tvm.relay.op.tensor.round:3
#: tvm.relay.op.tensor.rsqrt:7 tvm.relay.op.tensor.sigmoid:3
#: tvm.relay.op.tensor.sign:3 tvm.relay.op.tensor.sin:3
#: tvm.relay.op.tensor.sinh:3 tvm.relay.op.tensor.sqrt:3
#: tvm.relay.op.tensor.tan:3 tvm.relay.op.tensor.tanh:3
#: tvm.relay.op.tensor.trunc:3 tvm.relay.op.tensor.zeros_like:3
msgid "The input data"
msgstr ""

#: of tvm.relay.op.reduce.all:17 tvm.relay.op.reduce.any:17
#: tvm.relay.op.reduce.argmax:20 tvm.relay.op.reduce.argmin:20
#: tvm.relay.op.reduce.logsumexp:17 tvm.relay.op.reduce.max:17
#: tvm.relay.op.reduce.mean:17 tvm.relay.op.reduce.mean_std:17
#: tvm.relay.op.reduce.mean_variance:17 tvm.relay.op.reduce.min:18
#: tvm.relay.op.reduce.prod:17 tvm.relay.op.reduce.std:19
#: tvm.relay.op.reduce.sum:17 tvm.relay.op.reduce.variance:19
#: tvm.relay.op.tensor.abs:6 tvm.relay.op.tensor.acos:6
#: tvm.relay.op.tensor.acosh:6 tvm.relay.op.tensor.add:8
#: tvm.relay.op.tensor.asin:6 tvm.relay.op.tensor.asinh:6
#: tvm.relay.op.tensor.atan:6 tvm.relay.op.tensor.atanh:6
#: tvm.relay.op.tensor.bitwise_and:8 tvm.relay.op.tensor.bitwise_not:6
#: tvm.relay.op.tensor.bitwise_or:8 tvm.relay.op.tensor.bitwise_xor:8
#: tvm.relay.op.tensor.ceil:6 tvm.relay.op.tensor.cos:6
#: tvm.relay.op.tensor.cosh:6 tvm.relay.op.tensor.divide:8
#: tvm.relay.op.tensor.equal:8 tvm.relay.op.tensor.erf:6
#: tvm.relay.op.tensor.exp:6 tvm.relay.op.tensor.floor:6
#: tvm.relay.op.tensor.floor_divide:8 tvm.relay.op.tensor.floor_mod:8
#: tvm.relay.op.tensor.greater:8 tvm.relay.op.tensor.greater_equal:8
#: tvm.relay.op.tensor.isfinite:6 tvm.relay.op.tensor.isinf:6
#: tvm.relay.op.tensor.isnan:6 tvm.relay.op.tensor.left_shift:8
#: tvm.relay.op.tensor.less:8 tvm.relay.op.tensor.less_equal:8
#: tvm.relay.op.tensor.log:6 tvm.relay.op.tensor.log10:6
#: tvm.relay.op.tensor.log2:6 tvm.relay.op.tensor.logical_and:8
#: tvm.relay.op.tensor.logical_not:6 tvm.relay.op.tensor.logical_or:8
#: tvm.relay.op.tensor.logical_xor:8 tvm.relay.op.tensor.maximum:8
#: tvm.relay.op.tensor.minimum:8 tvm.relay.op.tensor.mod:8
#: tvm.relay.op.tensor.multiply:8 tvm.relay.op.tensor.negative:6
#: tvm.relay.op.tensor.not_equal:8 tvm.relay.op.tensor.ones_like:6
#: tvm.relay.op.tensor.power:8 tvm.relay.op.tensor.right_shift:8
#: tvm.relay.op.tensor.round:6 tvm.relay.op.tensor.rsqrt:10
#: tvm.relay.op.tensor.sigmoid:6 tvm.relay.op.tensor.sign:6
#: tvm.relay.op.tensor.sin:6 tvm.relay.op.tensor.sinh:6
#: tvm.relay.op.tensor.sqrt:6 tvm.relay.op.tensor.subtract:8
#: tvm.relay.op.tensor.tan:6 tvm.relay.op.tensor.tanh:6
#: tvm.relay.op.tensor.trunc:6 tvm.relay.op.tensor.zeros_like:6
#: tvm.relay.op.transform.slice_like:14
msgid "**result** -- The computed result."
msgstr ""

#: of tvm.relay.op.tensor.add:3 tvm.relay.op.tensor.bitwise_and:3
#: tvm.relay.op.tensor.bitwise_or:3 tvm.relay.op.tensor.bitwise_xor:3
#: tvm.relay.op.tensor.divide:3 tvm.relay.op.tensor.equal:3
#: tvm.relay.op.tensor.floor_divide:3 tvm.relay.op.tensor.floor_mod:3
#: tvm.relay.op.tensor.greater:3 tvm.relay.op.tensor.greater_equal:3
#: tvm.relay.op.tensor.left_shift:3 tvm.relay.op.tensor.less:3
#: tvm.relay.op.tensor.less_equal:3 tvm.relay.op.tensor.logical_and:3
#: tvm.relay.op.tensor.logical_or:3 tvm.relay.op.tensor.logical_xor:3
#: tvm.relay.op.tensor.maximum:3 tvm.relay.op.tensor.minimum:3
#: tvm.relay.op.tensor.mod:3 tvm.relay.op.tensor.multiply:3
#: tvm.relay.op.tensor.not_equal:3 tvm.relay.op.tensor.power:3
#: tvm.relay.op.tensor.right_shift:3 tvm.relay.op.tensor.subtract:3
msgid "The left hand side input data"
msgstr ""

#: of tvm.relay.op.tensor.add:5 tvm.relay.op.tensor.bitwise_and:5
#: tvm.relay.op.tensor.bitwise_or:5 tvm.relay.op.tensor.bitwise_xor:5
#: tvm.relay.op.tensor.divide:5 tvm.relay.op.tensor.equal:5
#: tvm.relay.op.tensor.floor_divide:5 tvm.relay.op.tensor.floor_mod:5
#: tvm.relay.op.tensor.greater:5 tvm.relay.op.tensor.greater_equal:5
#: tvm.relay.op.tensor.left_shift:5 tvm.relay.op.tensor.less:5
#: tvm.relay.op.tensor.less_equal:5 tvm.relay.op.tensor.logical_and:5
#: tvm.relay.op.tensor.logical_or:5 tvm.relay.op.tensor.logical_xor:5
#: tvm.relay.op.tensor.maximum:5 tvm.relay.op.tensor.minimum:5
#: tvm.relay.op.tensor.mod:5 tvm.relay.op.tensor.multiply:5
#: tvm.relay.op.tensor.not_equal:5 tvm.relay.op.tensor.power:5
#: tvm.relay.op.tensor.right_shift:5 tvm.relay.op.tensor.subtract:5
msgid "The right hand side input data"
msgstr ""

#: of tvm.relay.op.transform.adv_index:1
msgid "Numpy style advanced indexing. Index with a list of tensors."
msgstr ""

#: of tvm.relay.op.transform.adv_index:3
msgid ""
"Input tensor and indices. The first tensor is input data and rests are "
"indices."
msgstr ""

#: of tvm.relay.op.transform.adv_index:7 tvm.relay.op.transform.segment_sum:22
#: tvm.relay.op.transform.sparse_reshape:11
msgid "**result** -- Output tensor."
msgstr ""

#: of tvm.relay.op.reduce.all:3 tvm.relay.op.reduce.any:3
msgid "The input boolean tensor"
msgstr ""

#: of tvm.relay.op.reduce.all:5 tvm.relay.op.reduce.any:5
#: tvm.relay.op.reduce.sum:5
msgid ""
"Axis or axes along which a sum is performed. The default, axis=None, will"
" sum all of the elements of the input array. If axis is negative it "
"counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.all:9 tvm.relay.op.reduce.any:9
#: tvm.relay.op.reduce.argmax:9 tvm.relay.op.reduce.argmin:9
#: tvm.relay.op.reduce.max:9 tvm.relay.op.reduce.mean:9
#: tvm.relay.op.reduce.mean_std:9 tvm.relay.op.reduce.mean_variance:9
#: tvm.relay.op.reduce.min:10 tvm.relay.op.reduce.prod:9
#: tvm.relay.op.reduce.std:9 tvm.relay.op.reduce.sum:9
#: tvm.relay.op.reduce.variance:9
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one. With this option, the result will broadcast"
" correctly against the input array."
msgstr ""

#: of tvm.relay.op.reduce.all:13 tvm.relay.op.reduce.any:13
#: tvm.relay.op.reduce.argmax:13 tvm.relay.op.reduce.argmin:13
#: tvm.relay.op.reduce.max:13 tvm.relay.op.reduce.mean:13
#: tvm.relay.op.reduce.mean_std:13 tvm.relay.op.reduce.mean_variance:13
#: tvm.relay.op.reduce.min:14 tvm.relay.op.reduce.prod:13
#: tvm.relay.op.reduce.std:13 tvm.relay.op.reduce.sum:13
#: tvm.relay.op.reduce.variance:13
msgid ""
"If `exclude` is true, reduction will be performed on the axes that are "
"NOT in axis instead."
msgstr ""

#: of tvm.relay.op.transform.arange:4
msgid ""
"Similar to ``numpy.arange``, when only one argument is given, it is used "
"as `stop` instead of `start` while `start` takes default value 0."
msgstr ""

#: of tvm.relay.op.transform.arange:7
msgid ""
"Warning: Undefined behavior when dtype is incompatible with "
"start/stop/step. It could lead to different results compared to numpy, "
"MXNet, pytorch, etc."
msgstr ""

#: of tvm.relay.op.transform.arange:10
msgid ""
"Start of interval. The interval includes this value. The default start "
"value is 0."
msgstr ""

#: of tvm.relay.op.transform.arange:13
msgid "Stop of interval. The interval does not include this value."
msgstr ""

#: of tvm.relay.op.transform.arange:15
msgid "Spacing between values. The default step size is 1."
msgstr ""

#: of tvm.relay.op.tensor.ones:8 tvm.relay.op.tensor.zeros:8
#: tvm.relay.op.transform.arange:20 tvm.relay.op.transform.broadcast_to:9
#: tvm.relay.op.transform.broadcast_to_like:8
#: tvm.relay.op.transform.collapse_sum_like:8
#: tvm.relay.op.transform.collapse_sum_to:8 tvm.relay.op.transform.full:10
#: tvm.relay.op.transform.full_like:8
msgid "**result** -- The resulting tensor."
msgstr ""

#: of tvm.relay.op.reduce.argmax:5
msgid ""
"Axis or axes along which a argmax operation is performed. The default, "
"axis=None, will find the indices of the maximum element of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.reduce.argmax:16
msgid ""
"Whether to select the last index or the first index if the max element "
"appears in multiple indices, default is False (first index)."
msgstr ""

#: of tvm.relay.op.reduce.argmin:5
msgid ""
"Axis or axes along which a argmin operation is performed. The default, "
"axis=None, will find the indices of minimum element all of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.reduce.argmin:16
msgid ""
"Whether to select the last index or the first index if the min element "
"appears in multiple indices, default is False (first index)."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:4 tvm.relay.op.algorithm.sort:3
#: tvm.relay.op.algorithm.topk:5
msgid "The input data tensor."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:6
msgid "The number of valid elements to be sorted."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:8 tvm.relay.op.algorithm.sort:5
#: tvm.relay.op.algorithm.topk:9
msgid "Axis long which to sort the input tensor."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:10 tvm.relay.op.algorithm.sort:7
#: tvm.relay.op.algorithm.topk:16
msgid "Whether to sort in ascending or descending order."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:12
msgid "The data type of the output indices."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:15 tvm.relay.op.algorithm.sort:10
msgid "**out** -- Tensor with same shape as data."
msgstr ""

#: of tvm.relay.op.transform.argwhere:4
msgid "The input condition tensor."
msgstr ""

#: of tvm.relay.op.transform.argwhere:7
msgid "**out** -- Tensor with the indices of elements that are non-zero."
msgstr ""

#: of tvm.relay.op.tensor.clip:4 tvm.relay.op.tensor.fixed_point_multiply:5
#: tvm.relay.op.tensor.ndarray_size:3 tvm.relay.op.tensor.shape_of:3
#: tvm.relay.op.transform.broadcast_to:4
#: tvm.relay.op.transform.broadcast_to_like:3
#: tvm.relay.op.transform.collapse_sum_like:3
#: tvm.relay.op.transform.collapse_sum_to:3 tvm.relay.op.transform.full_like:3
msgid "The input tensor."
msgstr ""

#: of tvm.relay.op.transform.broadcast_to:6
msgid "Provide the shape to broadcast to."
msgstr ""

#: of tvm.relay.op.transform.broadcast_to_like:5
msgid "Provide the type to broadcast to."
msgstr ""

#: of tvm.relay.op.transform.cast:3 tvm.relay.op.transform.cast_like:3
#: tvm.relay.op.transform.cumprod:4 tvm.relay.op.transform.cumsum:4
#: tvm.relay.op.transform.expand_dims:3 tvm.relay.op.transform.gather:14
#: tvm.relay.op.transform.gather_nd:4 tvm.relay.op.transform.reinterpret:3
#: tvm.relay.op.transform.reshape:50 tvm.relay.op.transform.reshape_like:10
#: tvm.relay.op.transform.reverse:4 tvm.relay.op.transform.reverse_reshape:13
#: tvm.relay.op.transform.scatter:3 tvm.relay.op.transform.scatter_add:3
#: tvm.relay.op.transform.scatter_nd:5 tvm.relay.op.transform.squeeze:3
#: tvm.relay.op.transform.tile:3 tvm.relay.op.transform.transpose:3
msgid "The input data to the operator."
msgstr ""

#: of tvm.relay.op.transform.cast:5 tvm.relay.op.transform.reinterpret:5
msgid "The target data type"
msgstr ""

#: of tvm.relay.op.transform.cast:8 tvm.relay.op.transform.cast_like:8
msgid "**result** -- The casted result."
msgstr ""

#: of tvm.relay.op.transform.cast_like:5
msgid "The tensor to cast to."
msgstr ""

#: of tvm.relay.op.tensor.clip:1
msgid ""
"Clip the elements in `a` between `a_min` and `a_max`. `a_min` and `a_max`"
" are cast to `a`'s dtype."
msgstr ""

#: of tvm.relay.op.tensor.clip:6
msgid "The clip minimum."
msgstr ""

#: of tvm.relay.op.tensor.clip:8
msgid "The clip maximum."
msgstr ""

#: of tvm.relay.op.tensor.clip:11
msgid "**result** -- `a` with elements clipped between `a_min` and `a_max`."
msgstr ""

#: of tvm.relay.op.transform.collapse_sum_like:5
msgid "Provide the type to collapse to."
msgstr ""

#: of tvm.relay.op.transform.collapse_sum_to:5
msgid "Shape to collapse to."
msgstr ""

#: of tvm.relay.op.tensor.concatenate:3
msgid "A list of tensors."
msgstr ""

#: of tvm.relay.op.tensor.concatenate:5
msgid "The axis along which the tensors are concatenated."
msgstr ""

#: of tvm.relay.op.tensor.concatenate:8
msgid "**result** -- The concatenated tensor."
msgstr ""

#: of tvm.relay.op.tensor.copy:3 tvm.relay.op.tensor.device_copy:5
msgid "The tensor to be copied."
msgstr ""

#: of tvm.relay.op.tensor.copy:6 tvm.relay.op.tensor.device_copy:12
msgid "**result** -- The copied result."
msgstr ""

#: of tvm.relay.op.transform.cumprod:1
msgid ""
"Numpy style cumprod op. Return the cumulative inclusive product of the "
"elements along a given axis."
msgstr ""

#: of tvm.relay.op.transform.cumprod:6
msgid ""
"Axis along which the cumulative product is computed. The default (None) "
"is to compute the cumprod over the flattened array."
msgstr ""

#: of tvm.relay.op.transform.cumprod:9
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are multiplied. If dtype is not specified, it defaults to the dtype of "
"data."
msgstr ""

#: of tvm.relay.op.transform.cumprod:12
msgid ""
"If true will return exclusive product in which the first element is not "
"included. In other terms, if true, the j-th output element would be the "
"product of the first (j-1) elements. Otherwise, it would be the product "
"of the first j elements. The product of zero elements will be 1."
msgstr ""

#: of tvm.relay.op.transform.cumprod:18 tvm.relay.op.transform.cumsum:18
msgid ""
"**result** -- The result has the same size as data, and the same shape as"
" data if axis is not None. If axis is None, the result is a 1-d array."
msgstr ""

#: of tvm.relay.op.transform.cumsum:1
msgid ""
"Numpy style cumsum op. Return the cumulative inclusive sum of the "
"elements along a given axis."
msgstr ""

#: of tvm.relay.op.transform.cumsum:6
msgid ""
"Axis along which the cumulative sum is computed. The default (None) is to"
" compute the cumsum over the flattened array."
msgstr ""

#: of tvm.relay.op.transform.cumsum:9
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are summed. If dtype is not specified, it defaults to the dtype of data."
msgstr ""

#: of tvm.relay.op.transform.cumsum:12
msgid ""
"If true will return exclusive sum in which the first element is not "
"included. In other terms, if true, the j-th output element would be the "
"sum of the first (j-1) elements. Otherwise, it would be the sum of the "
"first j elements."
msgstr ""

#: of tvm.relay.op.tensor.device_copy:1
msgid ""
"Copy data from the source device to the destination device. This operator"
" helps data transferring between difference devices for heterogeneous "
"execution."
msgstr ""

#: of tvm.relay.op.tensor.device_copy:7
msgid "The source device where the data is copied from."
msgstr ""

#: of tvm.relay.op.tensor.device_copy:9
msgid "The destination device where the data is copied to."
msgstr ""

#: of tvm.relay.op.transform.expand_dims:5
msgid ""
"The axis at which the input array is expanded. Should lie in range "
"`[-data.ndim - 1, data.ndim]`. If `axis < 0`, it is the first axis "
"inserted; If `axis >= 0`, it is the last axis inserted in Python's "
"negative indexing."
msgstr ""

#: of tvm.relay.op.transform.expand_dims:10
msgid "Number of axes to be inserted. Should be >= 0."
msgstr ""

#: of tvm.relay.op.transform.expand_dims:13 tvm.relay.op.transform.reshape:55
#: tvm.relay.op.transform.reverse_reshape:18
msgid "**result** -- The reshaped result."
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:7
msgid "The integer multiplier of the fixed point constant."
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:9
msgid "The integer shift of the fixed point constant."
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:12
msgid "**result** -- The output of the fixed point multiplication"
msgstr ""

#: of tvm.relay.op.transform.full:3
msgid "The value to fill. Must be a scalar."
msgstr ""

#: of tvm.relay.op.tensor.ones:3 tvm.relay.op.tensor.zeros:3
#: tvm.relay.op.transform.full:5
msgid "The shape of the target."
msgstr ""

#: of tvm.relay.op.tensor.ones:5 tvm.relay.op.tensor.zeros:5
#: tvm.relay.op.transform.full:7
msgid "The data type of the target."
msgstr ""

#: of tvm.relay.op.transform.full_like:5
msgid "The scalar value to fill."
msgstr ""

#: of tvm.relay.op.transform.gather:3
msgid "E.g. for a 3D tensor, output is computed as:"
msgstr ""

#: of tvm.relay.op.transform.gather:11
msgid ""
"``indices`` must have same shape as ``data``, except at dimension "
"``axis`` which must just be not null. Output will have same shape as "
"``indices``."
msgstr ""

#: of tvm.relay.op.transform.gather:16
msgid "The axis along which to index. negative axis is supported."
msgstr ""

#: of tvm.relay.op.transform.gather:18
msgid "The indices of values to gather."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:6
msgid "The shape of output tensor."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:8
msgid "The number of batch dimensions."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:10
msgid ""
"The size of an indexing tuple, which is a fixed value and the same as "
"indices.shape[0] Only needed when other dimensions of indices are "
"dynamic."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:14 tvm.relay.op.transform.meshgrid:11
#: tvm.relay.op.transform.repeat:12 tvm.relay.op.transform.reshape_like:26
#: tvm.relay.op.transform.reverse:9 tvm.relay.op.transform.scatter:12
#: tvm.relay.op.transform.scatter_add:12 tvm.relay.op.transform.scatter_nd:14
#: tvm.relay.op.transform.sequence_mask:15 tvm.relay.op.transform.split:16
#: tvm.relay.op.transform.strided_set:15
#: tvm.relay.op.transform.strided_slice:24 tvm.relay.op.transform.take:18
#: tvm.relay.op.transform.tile:8
msgid "**ret** -- The computed result."
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:1
msgid ""
"Computes the inverse permutation of data. This operation computes the "
"inverse of an index permutation. It takes a 1-D integer tensor x, which "
"represents the indices of a zero-based array and swaps each value with "
"its index position."
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:6
msgid ""
"For an output tensor y and an input tensor x, this operation computes the"
" following: y[x[i]] = i for i in [0, 1, ..., len(x) - 1]"
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:9
msgid "The source data to be invert permuated."
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:12
msgid "**ret** -- Invert permuated data. Has the same type as data."
msgstr ""

#: of tvm.relay.op.transform.layout_transform:3
msgid "The source tensor to be transformed"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:5
msgid "The source layout.  (e.g NCHW)"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:7
msgid "The destination layout.  (e.g. NCHW16c)"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:10
msgid "**ret** -- The transformed tensor."
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:3
msgid ""
"This function is more numerically stable than log(sum(exp(input))). It "
"avoids overflows caused by taking the exp of large inputs and underflows "
"caused by taking the log of small inputs."
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:9
msgid ""
"Axis or axes along which a standard deviation operation is performed. The"
" default, axis=None, will compute the log of the sum of exponentials of "
"all elements in the input array. If axis is negative it counts from the "
"last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:13
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:3
msgid "Input Tensor."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:5
msgid "Values to be filled in the diagonal."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:7
msgid ""
"Diagonal Offset(s). The diagonal or range of diagonals to set. (0 by "
"default) Positive value means superdiagonal, 0 refers to the main "
"diagonal, and negative value means subdiagonals. k can be a single "
"integer (for a single diagonal) or a pair of integers specifying the low "
"and high ends of a matrix band. k[0] must not be larger than k[1]."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:13
msgid ""
"Some diagonals are shorter than max_diag_len and need to be padded. align"
" is a string specifying how superdiagonals and subdiagonals should be "
"aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\""
" (default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". "
"\"RIGHT_LEFT\" aligns superdiagonals to the right (left-pads the row) and"
" subdiagonals to the left (right-pads the row). It is the packing format "
"LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is the opposite "
"alignment."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:21
msgid "**result** -- New tensor with given diagonal values."
msgstr ""

#: of tvm.relay.op.reduce.max:5
msgid ""
"Axis or axes along which the max operation is performed. The default, "
"axis=None, will find the max element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.relay.op.reduce.mean:5
msgid ""
"Axis or axes along which a mean operation is performed. The default, "
"axis=None, will compute the mean of all elements in the input array. If "
"axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.mean_std:5
msgid ""
"Axis or axes along which a mean and standard deviation operation is "
"performed. The default, axis=None, will compute the mean and standard "
"deviation of all elements in the input array. If axis is negative it "
"counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.mean_variance:5
msgid ""
"Axis or axes along which a mean and variance operation is performed. The "
"default, axis=None, will compute the mean and variance of all elements in"
" the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:4
msgid "Similar to ``numpy.meshgrid``."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:6
msgid "A list of tensors, which must be either scalars or 1-D vectors."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:8
msgid ""
"Indexing mode, either \"ij\" for matrix indexing or \"xy\" for Cartesian "
"indexing."
msgstr ""

#: of tvm.relay.op.reduce.min:5
msgid ""
"Axis or axes along which a minimum operation is performed. The default, "
"axis=None, will find the minimum element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.relay.op.tensor.ndarray_size:8
msgid "**result** -- The number of elements of input tensor."
msgstr ""

#: of tvm.relay.op.transform.one_hot:1
msgid ""
"Returns a one-hot tensor where the locations repsented by indices take "
"value on_value, other locations take value off_value. Final dimension is "
"<indices outer dimensions> x depth x <indices inner dimensions>."
msgstr ""

#: of tvm.relay.op.transform.one_hot:5
msgid "Locations to set to on_value."
msgstr ""

#: of tvm.relay.op.transform.one_hot:7
msgid "Value to fill at indices."
msgstr ""

#: of tvm.relay.op.transform.one_hot:9
msgid "Value to fill at all other positions besides indices."
msgstr ""

#: of tvm.relay.op.transform.one_hot:11
msgid "Depth of the one-hot dimension."
msgstr ""

#: of tvm.relay.op.transform.one_hot:13
msgid "Axis to fill."
msgstr ""

#: of tvm.relay.op.transform.one_hot:15
msgid "Data type of the output tensor."
msgstr ""

#: of tvm.relay.op.transform.one_hot:18
msgid "**ret** -- The one-hot tensor."
msgstr ""

#: of tvm.relay.op.reduce.prod:5
msgid ""
"Axis or axes along which a product is performed. The default, axis=None, "
"will find the indices of minimum element all of the elements of the input"
" array. If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.transform.reinterpret:8
msgid "**result** -- The reinterpreted result."
msgstr ""

#: of tvm.relay.op.transform.repeat:1
msgid ""
"Repeats elements of an array. By default, repeat flattens the input array"
" into 1-D and then repeats the elements."
msgstr ""

#: of tvm.relay.op.transform.repeat:5
msgid "repeats"
msgstr ""

#: of
msgid "int"
msgstr ""

#: of tvm.relay.op.transform.repeat:5
msgid "The number of repetitions for each element."
msgstr ""

#: of tvm.relay.op.transform.repeat:10
msgid "axis: int"
msgstr ""

#: of tvm.relay.op.transform.repeat:8
msgid ""
"The axis along which to repeat values. The negative numbers are "
"interpreted counting from the backward. By default, use the flattened "
"input array, and return a flat output array."
msgstr ""

#: of tvm.relay.op.transform.reshape:3
msgid ""
"To give user more convenience in without doing manual shape inference, "
"some dimensions of the shape can take special values from the set {0, -1,"
" -2, -3, -4}. The significance of each is explained below:"
msgstr ""

#: of tvm.relay.op.transform.reshape:7
msgid "``0`` copy this dimension from the input to the output shape."
msgstr ""

#: of tvm.relay.op.transform.reshape:14
msgid ""
"``-1`` infers the dimension of the output shape by using the remainder of"
" the input dimensions keeping the size of the new array same as that of "
"the input array. At most one dimension of shape can be -1."
msgstr ""

#: of tvm.relay.op.transform.reshape:24
msgid "``-2`` copy all/remainder of the input dimensions to the output shape."
msgstr ""

#: of tvm.relay.op.transform.reshape:32
msgid ""
"``-3`` use the product of two consecutive dimensions of the input shape "
"as the output dimension."
msgstr ""

#: of tvm.relay.op.transform.reshape:42
msgid ""
"``-4`` split one dimension of the input into two dimensions passed "
"subsequent to -4 in shape (can contain -1)."
msgstr ""

#: of tvm.relay.op.transform.reshape:52
#: tvm.relay.op.transform.reverse_reshape:15
msgid "The new shape. Should be compatible with the original shape."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:1
msgid ""
"Reshapes the input tensor by the size of another tensor. For an input "
"tensor with shape ``(d0, d1, ..., d(k-1))``, `reshape_like` operation "
"reshapes the input tensor into an output tensor with the same shape as "
"the second input tensor, in particular reshaping the dimensions of `data`"
" in `[lhs_begin, lhs_end)` using the dimensions from `shape_like` in "
"`[rhs_begin, rhs_end)`."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:8
msgid "Sizes for `data` and the output tensor should be compatible."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:12
msgid ""
"The tensor to reshape data like. Should be compatible with the original "
"shape on the reshaped dimensions."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:15
msgid "The axis of data to begin reshaping. Default is 0."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:17
msgid ""
"The axis of data where reshaping should stop, exclusive. Default is None "
"which reshapes to the end."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:20
msgid "The axis of shape_like where the target shape begins. Default is 0."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:22
msgid ""
"The axis of shape_like where the target shape ends, exclusive. Default is"
" None which extends to the end."
msgstr ""

#: of tvm.relay.op.transform.reverse:1
msgid ""
"Reverses the order of elements along given axis while preserving array "
"shape. By default, repeat flattens the input array into 1-D and then "
"repeats the elements."
msgstr ""

#: of tvm.relay.op.transform.reverse:6
msgid "The axis along which to reverse elements."
msgstr ""

#: of tvm.relay.op.transform.reverse_reshape:4
msgid ""
"The special values have the same semantics as "
":py:class:`tvm.relay.reshape`. The difference is that special values are "
"inferred from right to left. It can be explained in the example below."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:1
msgid ""
"Reverse the tensor for variable length slices. Input is first sliced "
"along batch axis and then elements are reversed along seq axis."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:4
msgid "The tensor to be reversed."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:6
msgid ""
"A 1D Tensor with length a.dims[batch_axis] Must be one of the following "
"types: int32, int64 if seq_lengths[i] > a.dims[seq_axis], it is rounded "
"to a.dims[seq_axis] if seq_lengths[i] < 1, it is rounded to 1"
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:11
msgid "The axis along which the elements will be reversed. Default is 1."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:13
msgid "The axis along which the tensor will be sliced. Default is 0."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:16
msgid "**ret** -- The computed result of same shape and type as of input."
msgstr ""

#: of tvm.relay.op.tensor.rsqrt:3
msgid "1/sqrt(x)"
msgstr ""

#: of tvm.relay.op.transform.scatter:5 tvm.relay.op.transform.scatter_add:5
#: tvm.relay.op.transform.scatter_nd:7
msgid "The index locations to update."
msgstr ""

#: of tvm.relay.op.transform.scatter:7 tvm.relay.op.transform.scatter_nd:9
msgid "The values to update."
msgstr ""

#: of tvm.relay.op.transform.scatter:9
msgid "The axis to scatter on"
msgstr ""

#: of tvm.relay.op.transform.scatter_add:7
msgid "The values to add."
msgstr ""

#: of tvm.relay.op.transform.scatter_add:9
msgid "The axis to scatter_add on"
msgstr ""

#: of tvm.relay.op.transform.scatter_nd:3
msgid "See :py:func:`tvm.topi.scatter` for how data is scattered."
msgstr ""

#: of tvm.relay.op.transform.scatter_nd:11
msgid "The accumulation mode for scatter. \"update\" or \"add\""
msgstr ""

#: of tvm.te.hybrid.script:3
msgid ""
"The hybrid function support emulation mode and parsing to the internal "
"language IR."
msgstr ""

#: of tvm.te.hybrid.script:6
msgid "**hybrid_func** -- A decorated hybrid script function."
msgstr ""

#: of tvm.relay.op.transform.segment_sum:1
msgid ""
"Computes the sum along segment_ids along axis 0. If multiple segment_ids "
"reference the same location their contributions add up. result[index, j, "
"k, ...] = i... data[i, j, k,..] where index = segment_ids[i] This op is "
"much better understood with visualization articulated in the following "
"links and examples at the end of this docstring."
msgstr ""

#: of tvm.relay.op.transform.segment_sum:7
msgid ""
"https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum "
"https://caffe2.ai/docs/sparse-operations.html#null__unsorted-segment-"
"reduction-ops"
msgstr ""

#: of tvm.relay.op.transform.segment_sum:10
msgid "Input Tensor. It can be of any type and multi-dimensional"
msgstr ""

#: of tvm.relay.op.transform.segment_sum:12
msgid ""
"A 1-D int32/int64 tensor containing the segment_ids of the rows to "
"calculate the output sum upon. It defines a mapping from the zeroth "
"dimension of data onto segment_ids. The segment_ids tensor should be the "
"size of the first dimension, d0, with consecutive IDs in the range 0 to "
"k, where k<d0. In particular, a segmentation of a matrix tensor is a "
"mapping of rows to segments. This tensor doesn't need to be sorted"
msgstr ""

#: of tvm.relay.op.transform.segment_sum:18
msgid ""
"An integer describing the shape of the zeroth dimension. If unspecified, "
"its calculated equivalent to the number of unique segment_ids"
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:3
msgid ""
"This function takes an n-dimensional input array of the form [MAX_LENGTH,"
" batch_size, ...] or [batch_size, MAX_LENGTH, ...] and returns an array "
"of the same shape."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:6
msgid "The input data."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:8
msgid "The expected (valid) length of each sequence in the tensor."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:10
msgid "The masking value."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:12
msgid "The axis of the length dimension."
msgstr ""

#: of tvm.relay.op.tensor.shape_of:8
msgid "**result** -- The shape tensor."
msgstr ""

#: of tvm.relay.op.transform.slice_like:3
msgid ""
"For an input array with shape ``(d1, d2, ..., dk)``, `slice_like` "
"operation slices the the input array corresponding size of second array. "
"By default will slice on all axes."
msgstr ""

#: of tvm.relay.op.transform.slice_like:6 tvm.relay.op.transform.split:9
#: tvm.relay.op.transform.take:3
msgid "The source array."
msgstr ""

#: of tvm.relay.op.transform.slice_like:8
msgid "The new shape."
msgstr ""

#: of tvm.relay.op.transform.slice_like:10
msgid ""
"List of axes on which input data will be sliced according to the "
"corresponding size of the second input. By default will slice on all "
"axes. Negative axes mean counting in reverse."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:1
msgid ""
"Fill rows in a sparse matrix that do no contain any values. Values are "
"placed in the first column of empty rows. The sparse array is in COO "
"format. It returns a TupleWrapper with 3 outputs"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:5
msgid ""
"A 2-D tensor[N, ndims] of integers containing location of sparse values, "
"where N is the number of sparse values and n_dim is the number of "
"dimensions of the dense_shape. The first column of this relay parameter "
"must be sorted in ascending order."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:9
msgid "A 1-D tensor[N] containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:11
msgid "A 1-D tensor[ndims] which contains shape of the dense output tensor."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:13
msgid "A 1-D tensor[1] containing the default value for the remaining locations."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:16
msgid ""
"* **new_sparse_indices** (*relay.Expr*) -- A 2-D tensor[?, ndims] of "
"integers containing location of new sparse   indices. The first column "
"outputs must be sorted in ascending order. * **new_sparse_values** "
"(*relay.Expr*) -- A 1-D tensor[?] containing the sparse values for the "
"sparse indices. * **empty_row_indicator** (*relay.Expr*) -- A 1-D "
"tensor[dense_shape[0]] filled with zeros and ones   indicating whether "
"the particular row is empty or full respectively"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:16
msgid ""
"**new_sparse_indices** (*relay.Expr*) -- A 2-D tensor[?, ndims] of "
"integers containing location of new sparse indices. The first column "
"outputs must be sorted in ascending order."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:18
msgid ""
"**new_sparse_values** (*relay.Expr*) -- A 1-D tensor[?] containing the "
"sparse values for the sparse indices."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:19
msgid ""
"**empty_row_indicator** (*relay.Expr*) -- A 1-D tensor[dense_shape[0]] "
"filled with zeros and ones indicating whether the particular row is empty"
" or full respectively"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:24
msgid ""
"This op exactly follows the documentation here: "
"https://www.tensorflow.org/api_docs/python/tf/sparse/fill_empty_rows "
"There are two exceptions: 1. Input Sparse Indices are expected to be in "
"row-major order. 2. Empty Row Indicator has int64 output type with 1(for "
"True) and 0(for False)."
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:1
msgid "Reshape a Sparse Tensor. The sparse array is in COO format."
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:3
msgid ""
"A 2-D tensor[N, n_dim] of integers containing location of sparse values, "
"where N is the number of sparse values and n_dim is the number of "
"dimensions of the dense_shape"
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:6
msgid "A 1-D tensor containing the previous shape of the dense tensor"
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:8
msgid "A 1-D tensor containing the new shape of the dense tensor"
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:3
msgid ""
"Example:: -   sparse_to_dense([[0, 0], [1, 1]], [2, 2], [3, 3], 0) = [[3,"
" 0], [0, 3]]"
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:6
msgid ""
"A 0-D, 1-D, or 2-D tensor of integers containing location of sparse "
"values."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:8
msgid "A list of integers. Shape of the dense output tensor."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:10
msgid "A 0-D or 1-D tensor containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:12
msgid ""
"A 0-D tensor containing the default value for the remaining locations. "
"Defaults to 0."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:16
msgid ""
"**result** -- Dense tensor of shape output_shape. Has the same type as "
"sparse_values."
msgstr ""

#: of tvm.relay.op.transform.split:3
msgid ""
"If indices_or_sections is an integer, the input will be divided equally "
"along given axis. If such a split is not possible, an error is raised."
msgstr ""

#: of tvm.relay.op.transform.split:6
msgid ""
"If indices_or_sections is a tuple of sorted integers, the entries "
"indicate where along axis the array is split."
msgstr ""

#: of tvm.relay.op.transform.split:11
msgid "Indices or sections to split into. Accepts an int or a tuple"
msgstr ""

#: of tvm.relay.op.transform.split:13
msgid "The axis over which to split."
msgstr ""

#: of tvm.relay.op.transform.squeeze:5
msgid ""
"The set of axes to remove. If axis = None, remove all axis of dimensions "
"1. If any specified axis has dimension that does not equal 1, it is an "
"error."
msgstr ""

#: of tvm.relay.op.transform.squeeze:10
msgid "**result** -- The squeezed result."
msgstr ""

#: of tvm.relay.op.tensor.stack:3
msgid ""
"A list of tensors or a Relay expression that evaluates to a tuple of "
"tensors."
msgstr ""

#: of tvm.relay.op.tensor.stack:5
msgid "The axis in the result array along which the input arrays are stacked."
msgstr ""

#: of tvm.relay.op.tensor.stack:8
msgid "**ret** -- The stacked tensor."
msgstr ""

#: of tvm.relay.op.reduce.std:5
msgid ""
"Axis or axes along which a standard deviation operation is performed. The"
" default, axis=None, will compute the standard deviation of all elements "
"in the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.reduce.std:16 tvm.relay.op.reduce.variance:16
msgid "If this is set to True, the unbiased estimation will be used."
msgstr ""

#: of tvm.relay.op.transform.strided_set:3
#: tvm.relay.op.transform.strided_slice:3
msgid "The source array to be sliced."
msgstr ""

#: of tvm.relay.op.transform.strided_set:5
msgid "The data to be set."
msgstr ""

#: of tvm.relay.op.transform.strided_set:7
#: tvm.relay.op.transform.strided_slice:5
msgid "The indices to begin with in the slicing."
msgstr ""

#: of tvm.relay.op.transform.strided_set:9
#: tvm.relay.op.transform.strided_slice:7
msgid "Indices indicating end of the slice."
msgstr ""

#: of tvm.relay.op.transform.strided_set:11
#: tvm.relay.op.transform.strided_slice:9
msgid ""
"Specifies the stride values, it can be negative in that case, the input "
"tensor will be reversed in that particular axis."
msgstr ""

#: of tvm.relay.op.transform.strided_slice:12
msgid ""
"Axes along which slicing is applied. When it is specified, the length of "
"begin, end, strides, and axes must be equal. Moreover, begin, end, "
"strides, and axes must be static (cannot be relay.Expr). Axes argument "
"for dynamic parameter slicing is not supported yet."
msgstr ""

#: of tvm.relay.op.transform.strided_slice:17
msgid ""
"The slice mode [end, size]. end: The ending indices for the slice "
"[default]. size: The input strides will be ignored, input end in this "
"mode indicates the size of a slice starting at the location specified by "
"begin. If end[i] is -1, all remaining elements in that dimension are "
"included in the slice."
msgstr ""

#: of tvm.relay.op.transform.take:5
msgid "The indices of the values to extract."
msgstr ""

#: of tvm.relay.op.transform.take:7
msgid ""
"The axis over which to select values. By default, the flattened input "
"array is used."
msgstr ""

#: of tvm.relay.op.transform.take:10
msgid "The number of batch dimensions. By default is 0."
msgstr ""

#: of tvm.relay.op.transform.take:12
msgid ""
"Specifies how out-of-bound indices will behave [clip, wrap, fast]. clip: "
"clip to the range (default). wrap: wrap around the indices. fast: no clip"
" or wrap around (user must make sure indices are in-bound)."
msgstr ""

#: of tvm.relay.op.transform.tile:5
msgid "The number of times repeating the tensor data."
msgstr ""

#: of tvm.relay.op.transform.tile:25
msgid "Notes"
msgstr ""

#: of tvm.relay.op.transform.tile:26
msgid ""
"Each dim size of reps must be a positive integer. If reps has length d, "
"the result will have dimension of max(d, data.ndim); If data.ndim < d, "
"data is promoted to be d-dimensional by prepending new axes. If data.ndim"
" >=  d, reps is promoted to a.ndim by pre-pending 1's to it."
msgstr ""

#: of tvm.relay.op.algorithm.topk:3
msgid ""
"ret_type specifies the return type, can be one of (\"both\", \"values\", "
"\"indices\")."
msgstr ""

#: of tvm.relay.op.algorithm.topk:7
msgid "Number of top elements to select. Return all elements if k < 1."
msgstr ""

#: of tvm.relay.op.algorithm.topk:11
msgid ""
"The return type [both, values, indices]. \"both\": return both top k data"
" and indices. \"values\": return top k data only. \"indices\": return top"
" k indices only."
msgstr ""

#: of tvm.relay.op.algorithm.topk:18
msgid "The data type of the indices output."
msgstr ""

#: of tvm.relay.op.algorithm.topk:21
msgid "**out** -- The computed result."
msgstr ""

#: of tvm.relay.op.transform.transpose:5
msgid "The target axes order, reverse order if not specified."
msgstr ""

#: of tvm.relay.op.transform.transpose:8
msgid "**result** -- The transposed result."
msgstr ""

#: of tvm.relay.op.transform.unique:1
msgid ""
"Find the unique elements of a 1-D tensor. Please note `output` and "
"`counts` are all padded to have the same length of `data` and element "
"with index >= num_unique[0] has undefined value."
msgstr ""

#: of tvm.relay.op.transform.unique:4
msgid "A 1-D tensor of integers."
msgstr ""

#: of tvm.relay.op.transform.unique:6
msgid ""
"Whether to sort the unique elements in ascending order before returning "
"as output."
msgstr ""

#: of tvm.relay.op.transform.unique:8
msgid "Whether to return the count of each unique element."
msgstr ""

#: of tvm.relay.op.transform.unique:11
msgid ""
"* **unique** (*relay.Expr*) -- A 1-D tensor containing the unique "
"elements of the input data tensor. * **indices** (*relay.Expr*) -- A 1-D "
"tensor containing the index of each data element in the output tensor. * "
"**inverse_indices** (*relay.Expr*) -- A 1-D tensor. For each entry in "
"data, it contains the index of that data element in the   unique array. *"
" **num_unique** (*relay.Expr*) -- A 1-D tensor with size=1 containing the"
" number of unique elements in the input data tensor. * **counts "
"(optional)** (*relay.Expr*) -- A 1-D tensor containing the count of each "
"unique element in the output."
msgstr ""

#: of tvm.relay.op.transform.unique:11
msgid ""
"**unique** (*relay.Expr*) -- A 1-D tensor containing the unique elements "
"of the input data tensor."
msgstr ""

#: of tvm.relay.op.transform.unique:12
msgid ""
"**indices** (*relay.Expr*) -- A 1-D tensor containing the index of each "
"data element in the output tensor."
msgstr ""

#: of tvm.relay.op.transform.unique:13
msgid ""
"**inverse_indices** (*relay.Expr*) -- A 1-D tensor. For each entry in "
"data, it contains the index of that data element in the unique array."
msgstr ""

#: of tvm.relay.op.transform.unique:15
msgid ""
"**num_unique** (*relay.Expr*) -- A 1-D tensor with size=1 containing the "
"number of unique elements in the input data tensor."
msgstr ""

#: of tvm.relay.op.transform.unique:16
msgid ""
"**counts (optional)** (*relay.Expr*) -- A 1-D tensor containing the count"
" of each unique element in the output."
msgstr ""

#: of tvm.relay.op.transform.unravel_index:3
msgid "Example:: -   unravel_index([22, 41, 37], [7, 6]) = [[3, 6, 6],[4, 5, 1]]"
msgstr ""

#: of tvm.relay.op.transform.unravel_index:6
msgid "An integer array containing indices."
msgstr ""

#: of tvm.relay.op.transform.unravel_index:8
msgid "The shape of the array."
msgstr ""

#: of tvm.relay.op.transform.unravel_index:11
msgid "**result** -- The tuple of coordinate arrays."
msgstr ""

#: of tvm.relay.op.reduce.variance:5
msgid ""
"Axis or axes along which a variance operation is performed. The default, "
"axis=None, will compute the variance of all elements in the input array. "
"If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.transform.where:5
msgid ""
"Shapes of condition, x, and y must be broadcastable to a common shape. "
"Semantics follow numpy where function "
"https://numpy.org/doc/stable/reference/generated/numpy.where.html"
msgstr ""

#: of tvm.relay.op.transform.where:9
msgid "Where True, yield x, otherwise yield y"
msgstr ""

#: of tvm.relay.op.transform.where:11
msgid "The first array or scalar to be selected."
msgstr ""

#: of tvm.relay.op.transform.where:13
msgid "The second array or scalar to be selected."
msgstr ""

#: of tvm.relay.op.transform.where:16
msgid ""
"**result** -- The selected array. The output shape is the broadcasted "
"shape from condition, x, and y."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder:3
msgid "Enables users to build up a nested scope(let, if) expression easily."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid ":obj:`let <tvm.relay.ScopeBuilder.let>`\\ \\(var\\, value\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1
#: tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid "Create a new let binding."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid ":obj:`if_scope <tvm.relay.ScopeBuilder.if_scope>`\\ \\(cond\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.if_scope:1
#: tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid "Create a new if scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid ":obj:`else_scope <tvm.relay.ScopeBuilder.else_scope>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1
#: tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid "Create a new else scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid ":obj:`type_of <tvm.relay.ScopeBuilder.type_of>`\\ \\(expr\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.type_of:1
msgid "Compute the type of an expression."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid ":obj:`ret <tvm.relay.ScopeBuilder.ret>`\\ \\(value\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.ret:1
msgid "Set the return value of this scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid ":obj:`get <tvm.relay.ScopeBuilder.get>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.get:1
#: tvm.relay.scope_builder.ScopeBuilder.let:1:<autosummary>:1
msgid "Get the generated result."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:3
msgid "The variable or name of variable."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:5
msgid "The value to be bound"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.if_scope:3
msgid "The condition"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:3
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:6
msgid "**scope** -- The if scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.if_scope:9
msgid "The user must follows with an else scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.type_of:3
msgid "The expression to compute the type of."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.ret:3
msgid "The return value."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.get:3
msgid "**value** -- The final result of the expression."
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:3
msgid ""
"The result binary bytes can be loaded by the GraphModule with API "
"\"load_params\"."
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:6
msgid "Use :py:func:`tvm.runtime.save_param_dict` instead."
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:9
msgid "The parameter dictionary."
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:12
msgid "**param_bytes** -- Serialized parameters."
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:3
msgid "Use :py:func:`tvm.runtime.load_param_dict` instead."
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:6
msgid "Serialized parameters."
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:9
msgid "**params** -- The parameter dictionary."
msgstr ""

