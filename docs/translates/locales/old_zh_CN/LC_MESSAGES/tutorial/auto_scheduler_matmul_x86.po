# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# HLearning, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:40+0000\n"
"Last-Translator: HLearning, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:4
msgid ""
"Click :ref:`here <sphx_glr_download_tutorial_auto_scheduler_matmul_x86.py>` "
"to download the full example code"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:11
msgid "Optimizing Operators with Auto-scheduling"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:12
msgid ""
"**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             "
"`Chengfan Jia <https://github.com/jcf94/>`_"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:14
msgid ""
"In this tutorial, we will show how TVM's Auto Scheduling feature can find "
"optimal schedules without the need for writing a custom template."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:17
msgid ""
"Different from the template-based :doc:`AutoTVM <autotvm_matmul_x86>` which "
"relies on manual templates to define the search space, the auto-scheduler "
"does not require any templates.  Users only need to write the computation "
"declaration without any schedule commands or templates.  The auto-scheduler "
"can automatically generate a large search space and find a good schedule in "
"the space."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:24
msgid "We use matrix multiplication as an example in this tutorial."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:27
msgid ""
"Note that this tutorial will not run on Windows or recent versions of macOS."
" To get it to run, you will need to wrap the body of this tutorial in a "
":code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:48
msgid "Defining the Matrix Multiplication"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:49
msgid ""
"To start, we define a matrix multiplication with a bias addition.  Note that"
" this uses standard operations available in TVMs Tensor Expression language."
" The major difference is the use of the `auto_sceduler` decorator at the top"
" of the function definition.  The function should return a list of "
"input/output tensors.  From these tensors, the auto-scheduler can get the "
"whole computational graph."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:86
msgid "Create the search task"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:87
msgid ""
"With the function defined, we can now create the task for the auto_scheduler"
" to search against. We specify the particular parameters for this matrix "
"multiplication, in this case a multiplication of to square matricies of size"
" 1024x1024. We then create a search task with N=L=M=1024 and "
"dtype=\"float32\""
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:92
msgid ""
"Improve performance with custom targets In order for TVM to take full "
"advantage of specific hardware platforms, you will want to manuall specify "
"your CPU capabilities. For example: - replace \"llvm\" below with \"llvm "
"-mcpu=core-avx2\" to enable AVX2 - replace \"llvm\" below with \"llvm -mcpu"
"=skylake-avx512\" to enable AVX-512"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:116
#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:181
#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:209
#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:319
#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:350
#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:409
msgid "Out:"
msgstr "输出:"

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:131
msgid "Set Parameters for Auto-Scheduler"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:132
msgid "Next, we set parameters for the auto-scheduler."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:134
msgid ""
":code:`num_measure_trials` is the number of measurement trials we can use "
"during the search.  We only make 10 trials in this tutorial for a fast "
"demonstration. In practice, 1000 is a good value for the search to converge."
" You can do more trials according to your time budget."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:138
msgid ""
"In addition, we use :code:`RecordToFile` to log measurement records into a "
"file `matmul.json`.  The measurement records can be used to query the "
"history best, resume the search, and do more analyses later."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:141
msgid "see :any:`auto_scheduler.TuningOptions` for more parameters"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:161
msgid "Run the search"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:162
msgid ""
"Now we get all inputs ready. Pretty simple, isn't it?  We can kick off the "
"search and let the auto-scheduler do its magic.  After some measurement "
"trials, we can load the best schedule from the log file and apply it."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:190
msgid "Inspecting the Optimized Schedule"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:191
msgid ""
"We can lower the schedule to see the IR after auto-scheduling.  The auto-"
"scheduler correctly performs optimizations including multi-level tiling, "
"layout transformation, parallelization, vectorization, unrolling, and "
"operator fusion."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:282
msgid "Check correctness and evaluate performance"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:283
msgid "We build the binary and check its correctness and performance."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:328
msgid "Using the record file"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:329
msgid ""
"During the search, all measurement records are logged into the record file "
"\"matmul.json\". The measurement records can be used to re-apply search "
"results, resume the search, and perform other analyses."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:333
msgid ""
"Here is an example where we load the best schedule from a file, and print "
"the equivalent python schedule API. This can be used for debugging and "
"learning the behavior of the auto-scheduler."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:378
msgid ""
"A more complicated example is to resume the search.  In this case, we need "
"to create the search policy and cost model by ourselves and resume the "
"status of search policy and cost model with the log file.  In the example "
"below we resume the status and do more 5 trials."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:421
msgid "Final Notes and Summary"
msgstr "最终说明总结"

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:422
msgid ""
"In this tutorial, we have shown how to use the TVM Auto-Scheduler to "
"automatically optimize a matrix multiplication, without the need to specify "
"a search template.  It ends a series of examples that starts from the Tensor"
" Expression (TE) language that demonstrates how TVM can optimize "
"computational operations."
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:441
msgid ""
":download:`Download Python source code: auto_scheduler_matmul_x86.py "
"<auto_scheduler_matmul_x86.py>`"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:447
msgid ""
":download:`Download Jupyter notebook: auto_scheduler_matmul_x86.ipynb "
"<auto_scheduler_matmul_x86.ipynb>`"
msgstr ""

#: ../../_staging/tutorial/auto_scheduler_matmul_x86.rst:454
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
