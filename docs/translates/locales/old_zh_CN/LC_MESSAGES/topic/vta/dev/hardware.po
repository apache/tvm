# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:39+0000\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/topic/vta/dev/hardware.rst:19
msgid "VTA Hardware Guide"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:21
msgid ""
"We present a top-down overview of the VTA hardware design. This hardware "
"design guide covers VTA hardware at two levels:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:24
msgid ""
"An architectural overview of the VTA design and its ISA hardware-software "
"interface."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:26
msgid ""
"A micro-architectural overview of the VTA hardware modules, and the micro-"
"code specification for the compute core."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:30
msgid "VTA Overview"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:32
msgid ""
"VTA is a generic deep learning accelerator built for fast and efficient "
"dense linear algebra. VTA incorporates a simple RISC-like processor that can"
" perform dense linear algebra operations on rank 1 or 2 tensor registers. In"
" addition the design adopts decoupled access-execute to hide memory access "
"latency."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:37
msgid ""
"To a broader extent, VTA can serve as a template deep learning accelerator "
"design for full stack optimization, exposing a generic tensor computation "
"interface to the compiler stack."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:43
msgid ""
"The figure above gives a high-level overview of the VTA hardware "
"organization. VTA is composed of four modules that communicate among each "
"other via FIFO queues and local memory blocks (SRAM), to enable task-level "
"pipeline parallelism:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:46
msgid ""
"The fetch module takes care of loading an instruction stream from DRAM. It "
"also decodes those instructions to route them into one of three command "
"queues."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:47
msgid ""
"The load module takes care of loading input and weight tensors from DRAM "
"into data-specialized on-chip memories."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:48
msgid ""
"The compute module performs both dense linear algebra computation with its "
"GEMM core, and general computation with its tensor ALU. It also takes care "
"of loading data from DRAM into the register file, and loading micro-op "
"kernels into the micro-op cache."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:49
msgid ""
"The store module stores results produced by the compute core back to DRAM."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:52
msgid "HLS Hardware Source Organization"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:54
msgid ""
"The VTA design is currently specified in Vivado HLS C++, which is only "
"supported by Xilinx toolchains. The VTA hardware sources are contained under"
" ``3rdparty/vta-hw/hardware/xilinx/sources``:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:58
msgid ""
"``vta.cc`` contains the definitions for each VTA module, as well as a top "
"level behavioral model for the top-level VTA design."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:60
msgid ""
"``vta.h`` contains type definitions using Xilinx ``ap_int`` types, and "
"function prototypes declarations."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:63
msgid ""
"In addition preprocessor macros are defined under ``3rdparty/vta-"
"hw/include/vta/hw_spec.h``. Much of these macro definitions are derived from"
" the parameters listed in the ``3rdparty/vta-hw/config/vta_config.json`` "
"file. The json file is processed by ``3rdparty/vta-hw/config/vta_config.py``"
" to produce a string of compile flags that define the preprocessor macros. "
"That string is used by the makefile in order to set those high-level "
"parameters in both the HLS hardware synthesis compiler, and the C++ compiler"
" that builds the VTA runtime."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:73
msgid "HLS Module Example"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:75
msgid "We show a definition of one of the VTA modules defined in C++:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:155
msgid "A few observations on HLS coding:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:112
msgid ""
"*Parameters:* The parameter list of each function, combined with the "
"interface pragmas define the hardware interface exposed by the generated "
"hardware module."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:116
msgid ""
"Parameters passed by value indicate a read-only hardware memory-mapped "
"register that the host can write to. This fetch function for instance has an"
" ``insn_count`` parameter which will be synthesized as a memory mapped "
"register for the host to write to, in order to set the length of a given VTA"
" instruction sequence."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:122
msgid ""
"Pointer parameters can mean one of two things depending on the interface "
"pragma being used."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:125
msgid ""
"When used with a ``m_axi`` interface pragma, an AXI master interface gets "
"generated to provide DMA access to DRAM."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:127
msgid ""
"When used with a ``bram`` interface pragma, a BRAM interface gets generated "
"to expose read and/or write ports to an FPGA block-RAM."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:129
msgid ""
"HLS streams being passed by reference combined with the ``axis`` interface "
"pragma produce FIFO interfaces to the module. Hardware FIFOs provide a "
"useful synchronization mechanism between modules."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:132
msgid ""
"*Pragmas*: Compiler pragmas are essential to define hardware implementation "
"of each module. We list several pragmas used in the VTA design to "
"communicate implementation requirements to the compiler."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:136
msgid ""
"``HLS INTERFACE``: specifies the interface of the synthesized hardware "
"module."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:138
msgid ""
"``HLS PIPELINE``: defines hardware pipeline performance target by setting an"
" initiation interval goal. When the ``II == 1`` target is set, it tells the "
"compiler that the synthesized hardware pipeline should be able to execute "
"one loop iteration per cycle."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:142
msgid ""
"``HLS DEPENDENCE``: instructs the compiler to ignore certain types of "
"dependence checks in a given loop. Consider a loop body that writes and "
"reads to the same BRAM structure, and needs to achieve an II of 1. The HLS "
"compiler has to assume worst-case scenario, whereby a read is issued to an "
"address that a past write updated the cycle prior: this cannot be achieved "
"given BRAM timing characteristics (it takes at least 2 cycles to see the "
"updated value). Therefore in order to achieve an II of 1, the dependence "
"checks have to be relaxed. Note that when turning this optimization on, it "
"falls onto the software stack to prevent writes followed by reads to the "
"same address."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:154
msgid ""
"This `reference guide "
"<https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_2/ug902"
"-vivado-high-level-synthesis.pdf>`_ provides a much more in-depth, and "
"complete specification of HLS for the Xilinx 2018.2 toolchains."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:158
msgid "Architectural Overview"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:161
msgid "Instruction Set Architecture"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:163
msgid ""
"VTA's instruction set architecture (ISA) is composed of 4 CISC instructions "
"that have a variable execution latency, two of which execute a micro-coded "
"instruction sequence to perform computation."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:165
msgid "The VTA instructions are listed below:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:167
msgid ""
"``LOAD`` instruction: loads a 2D tensor from DRAM into the input buffer, "
"weight buffer, or register file. It can also load a micro-kernel into the "
"micro-op cache. Supports dynamic padding when loading input and weight "
"tiles."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:168
msgid ""
"``GEMM`` instruction: performs a micro-op sequence of matrix-matrix "
"multiplications over an input tensor and a weight tensors, and adds the "
"result to a register-file tensor."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:169
msgid ""
"``ALU`` instruction: performs a micro-op sequence of matrix-matrix ALU "
"operations over register-file tensor data."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:170
msgid ""
"``STORE`` instruction: stores a 2D tensor from the output buffer to DRAM."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:172
msgid ""
"The ``LOAD`` instructions are executed by the load and compute modules "
"depending on the store memory buffer location target. The ``GEMM`` and "
"``ALU`` instructions are executed by the compute module's GEMM core and "
"tensor ALU. Finally, the ``STORE`` instructions are executed by the store "
"module exclusively. The fields of each instruction is described in the "
"figure below. The meaning of each field will be further explained in the "
":ref:`vta-uarch` section."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:183
msgid ""
"Note that the VTA ISA changes as VTA's architectural parameters are modified"
" (i.e. GEMM core shape, data type, memory size etc.), and as a result the "
"ISA does not guarantee compatibility across all variants of VTA. This is "
"acceptable however, since the VTA runtime adapts to parameter changes, and "
"produces binary code tailored for the version of the accelerator that gets "
"generated. This exemplifies the co-design philosophy adopted by the VTA "
"stack which embraces fluidity of the hardware-software interface."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:188
msgid "Dataflow Execution"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:190
msgid ""
"VTA relies on dependence FIFO queues between hardware modules to synchronize"
" the execution of concurrent tasks. The figure below shows how a given "
"hardware module can execute concurrently from its producer and consumer "
"modules in a dataflow fashion through the use of dependence FIFO queues, and"
" single-reader/single-writer SRAM buffers. Each module is connected to its "
"consumer and producer via read-after-write (RAW) and write-after-read (WAR) "
"dependence queues."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:198
msgid ""
"The pseudo-code above describes how a module executes a given instruction "
"predicated on dependences with other instructions. First, the dependence "
"flags within each instruction are decoded in hardware. If the instruction "
"has an incoming RAW dependences, execution is predicated upon receiving a "
"RAW dependence token from the producer module. Similarly, if the task has an"
" incoming WAR dependence, execution is predicated upon receiving a WAR "
"dependence token from the consumer module. Finally when the task is done, we"
" check for outgoing RAW and WAR dependences, and notify the consumer and "
"producer modules respectively."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:205
msgid ""
"Note that the dependence tokens in this scenario are information-less. This "
"is because the instructions executed by each module cannot be reordered by "
"design, as they arrive in FIFO order."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:209
msgid "Pipeline Expandability"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:211
msgid ""
"The default VTA design is composed of four modules that describe a 3-stage "
"``load-compute-store`` task pipeline. Following the dataflow hardware "
"organization principle, we can extend VTA the pipeline to include more "
"stages. For example, we can envision separating the tensor ALU from the GEMM"
" core in order to maximize the utilization of the GEMM core. This would "
"result in a ``load-gemm-activate-store`` task pipeline which closely "
"reflects the TPU design. Adding more stages has a cost however: it can add "
"storage and extra logic overhead, which is why we opted for a default "
"3-stage pipeline."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:220
msgid "Microarchitectural Overview"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:222
msgid ""
"We describe the modules that compose the VTA design. The module definitions "
"are contained in ``3rdparty/vta-hw/hardware/xilinx/sources/vta.cc``."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:226
msgid "Fetch Module"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:228
msgid ""
"VTA is programmed by a linear instruction stream. The fetch module is the "
"entry point of VTA to the CPU and is programmed via three memory mapped "
"registers:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:231
msgid ""
"The read-write ``control`` register starts the fetch module, and is read to "
"check for its completion."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:232
msgid ""
"The write-only ``insn_count`` register sets the number of instructions to "
"execute."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:233
msgid ""
"The write-only ``insns`` register sets the start address of the instruction "
"stream in DRAM."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:235
msgid ""
"The CPU prepares the instruction stream in DRAM in a physically-contiguous "
"buffer prepared by the VTA runtime. When the instruction stream is ready, "
"the CPU writes the start physical address into the ``insns`` register, the "
"length of the instruction stream into the ``insn_count`` register, and "
"asserts the start signal in the ``control`` register. This procedure starts "
"VTA, which reads in the instruction stream from DRAM via DMA."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:239
msgid ""
"Upon accessing the instruction stream, the fetch module partially decodes "
"instructions, and pushes those instructions into command queues that feed "
"into the load, compute, and store modules:"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:241
msgid ""
"``STORE`` instructions are pushed to the store command queue to be processed"
" by the store module."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:242
msgid ""
"``GEMM`` and ``ALU`` instructions are pushed to the compute command queue to"
" be processed by the compute module."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:243
msgid ""
"``LOAD`` instructions that describe a load operation of micro-op kernels or "
"register file data are pushed to the compute command queue to be processed "
"by the compute module."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:244
msgid ""
"``LOAD`` instructions that describe a load operation of input or weight data"
" are pushed to the load command queue to be processed by the load module."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:246
msgid ""
"When one of the command queues becomes full, the fetch module stalls until "
"the queue is not full. Consequently, the command queues are sized to be deep"
" enough to allow for a wide execution window, and allow multiple tasks to be"
" in flight concurrently across the ``load-compute-store`` pipeline."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:251
msgid "Compute Module"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:253
msgid ""
"VTA's compute module acts as a RISC processor that performs computation on "
"tensor registers rather than scalar registers. Two functional units mutate "
"the register file: the tensor ALU, and the GEMM core."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:256
msgid ""
"The compute module executes RISC micro-ops from the micro-op cache. There "
"are two types of compute micro-ops: ALU and GEMM operations. To minimize the"
" footprint of micro-op kernels, while avoiding the need for control-flow "
"instructions such as conditional jumps, the compute module executes micro-op"
" sequences inside a two-level nested loop that computes the location of each"
" tensor register location via an affine function. This compression approach "
"helps reduce the micro-kernel instruction footprint, and applies to both "
"matrix multiplication and 2D convolution, commonly found in neural network "
"operators."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:265
msgid ""
"The **GEMM core** evaluates GEMM instructions, by executing a micro-code "
"sequence in a 2-level nested loop described in the Figure above. The GEMM "
"core can perform one input-weight matrix multiplication per cycle. The "
"dimensions of the single-cycle matrix multiplication defines a hardware "
"*tensorization intrinsic* which the TVM compiler has to lower a computation "
"schedule onto. This tensorization intrinsic is defined by the dimensions of "
"the input, weight and accumulator tensors. Each data type can have a "
"different integer precision: typically both weight and input types are low-"
"precision (8-bits or less), while the accumulator tensor has a wider type to"
" prevent overflows (32-bits). In order to keep the GEMM core busy, each of "
"the input buffer, weight buffer, and register file have to expose sufficient"
" read/write bandwidth."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:276
msgid ""
"The **Tensor ALU** supports a set of standard operations to implement common"
" activation, normalization, and pooling operators. VTA being a modular "
"design, the range of operators that the Tensor ALU supports can be extended "
"for higher operator coverage, at the expense of higher resource utilization."
" The Tensor ALU can perform tensor-tensor operations, as well as tensor-"
"scalar operations on an immediate value. The opcode of the tensor ALU, and "
"the immediate value are specified by the high-level CISC instruction. The "
"micro-code in the context of tensor ALU computation only takes care of "
"specifying data access patterns."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:283
msgid ""
"In terms of computational throughput, the Tensor ALU does not execute at a "
"rate of one operation per cycle. The limitation comes from the lack of read-"
"ports: since one register file tensor can be read per cycle, the tensor ALU "
"has an initiation interval of at least 2 (i.e. performs at most 1 operation "
"every 2 cycles). In addition, performing a single tensor-tensor operation at"
" once can be expensive especially given that register file types are wide, "
"typically 32-bit integers. As a result, in order to balance the resource "
"utilization footprint of the Tensor ALU with the GEMM core, a tensor-tensor "
"operation is by default performed via vector-vector operations over multiple"
" cycles."
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:290
msgid "Load and Store Modules"
msgstr ""

#: ../../_staging/topic/vta/dev/hardware.rst:296
msgid ""
"The load and store modules perform 2D DMA loads with a strided access "
"pattern from DRAM to SRAM. In addition, the load module can insert 2D "
"padding on the fly, which is useful when blocking 2D convolution. This means"
" that VTA can tile 2D convolution inputs without paying the overhead of re-"
"laying data out in DRAM to insert spatial padding around input and weight "
"tiles."
msgstr ""
