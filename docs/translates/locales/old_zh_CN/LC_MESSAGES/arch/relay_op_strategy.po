# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:29+0000\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/arch/relay_op_strategy.rst:21
msgid "Relay Operator Strategy"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:23
msgid ""
"In order to lower Relay operators to the implementations defined in TOPI "
"library, a compute and schedule function need to be registered to each Relay"
" operator.  However, compute and schedule functions are usually specialized "
"for each target, and further, even for the same target, we may have multiple"
" algorithms and implementations available. To deal with the complexity, we "
"introduce operator strategy to allow developers to define a flexible "
"lowering strategy for each operator and target."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:33
msgid "Operator Strategy Design"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:35
msgid ""
"The basic element in operator strategy is an ``OpImplementation``. It "
"includes the a pair of compute and schedule function, the name of the "
"implementation, and a priority level (the use of priority level is explained"
" in `Select Implementation from Op Strategy`_)."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:40
msgid ""
"The ``OpStrategy`` includes a list of ``OpSpecialization``. Each "
"``OpSpecialization`` contains a list of ``OpImplementation`` associated with"
" a ``SpecializedCondition`` (see definition in "
"``include/tvm/te/schedule.h``).  The ``SpecializedCondition`` can be null, "
"indicating the implementations are generally applicable; otherwise, the "
"implementations are only considered when the specialized condition is "
"satisfied. ``SpecializedCondition`` consists of a list of clauses defined in"
" Tensor Expression in conjunctive normal form (CNF) and only supports "
"conditions on tensor shapes."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:49
msgid ""
"Last, a strategy function, or ``FTVMStrategy``, determines which pair(s) of "
"compute and schedule functions should be used given a workload, and needs to"
" be registered to each Relay operator.  ``FTVMStrategy`` is a generic "
"function (see ``include/tvm/target/generic_func.h``), that can be "
"overwritten for each target. The function signature is"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:59
msgid ""
"that the function returns an ``OpStrategy`` given the op attributes, input "
"tensors, output types, and target to compile to."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:64
msgid "Write A Strategy Function"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:66
msgid ""
"We recommend developers to write strategy function in Python as most TOPI "
"compute and schedule functions are written in Python. In python, we provide "
"``OpStrategy`` class in ``pyton/tvm/relay/op/op.py``. It only has one API, "
"which is to add an implementation to the strategy:"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:76
msgid ""
"We now take ``topk`` as an example to explain how to write the "
"``FTVMStrategy`` function:"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:101
msgid ""
"In this example, we use ``topi.cuda.topk`` and ``topi.cuda.schedule_topk`` "
"as the compute and schedule function for CUDA or GPU target, while use TOPI "
"generic compute and schedule for the rest of targets. Note that we use two "
"wrapper functions that wrap the topi compute and schedule to conform with "
"the required function signature ( see ``FTVMCompute`` and ``FTVMSchedule`` "
"in ``include/tvm/relay/op_attr_types.h``). Usually we need to write a "
"customized compute wrapper function for each operator to get different "
"fields from op attributes."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:110
msgid ""
"The example above shows a very basic strategy function that only adds one "
"implementation in the strategy. But for many complicated operators, we may "
"need to add multiple implementations that use different algorithms. For "
"example, we can use both direct and winograd algorithm to compute a conv2d "
"op. In order to achieve this, we can write the strategy function as follows:"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:132
msgid ""
"In this example, we add two implementations to the conv2d strategy where "
"winograd algorithm is only added when ``winograd_condition`` is true. The "
"implementation ``\"conv2d_nchw_winograd.cuda\"`` will be used to compile "
"conv2d when ``winograd_condition`` is true as it has higher priority level "
"(this could be changed if certain implementation is an AutoTVM template. See"
" `Select Implementation from Op Strategy`_ for more details). Otherwise, "
"``\"conv2d_nchw.cuda\"`` is used."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:140
msgid ""
"We can extend the example above to third party library implementation. For "
"example, we can add the implementation that invokes kernel in the cblas "
"library when cblas is included in the target."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:154
msgid ""
"Further, we can add implementation specialized for a certain range of "
"shapes. The code below shows an example of dense strategy that adds an "
"implementation that is specialized for ``m`` greater than 16. The main "
"difference between hardcode python condition like examples above and "
"specialized condition is that it allows TVM to generate multiple kernels "
"when the input tensors have symbolic shapes. The compile engine will "
"generate a dispatch function that invokes the specialized kernel when the "
"corresponding condition is met; otherwise, invoke the kernel that has no "
"associated specialized condition (``dense_common`` in this example). This "
"part is still work in progress. More details will be provided after it is "
"done."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:186
msgid "Register Strategy Function to An Operator"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:188
msgid ""
"After we define the strategy function for an operator, we can now register "
"the strategy function to this operator with"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:195
msgid ""
"However, it takes much effort to write a strategy function for an operator. "
"Therefore, we provide two other methods for simpler operators."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:198
msgid ""
"First, for operators that have injective, broadcast, or reduction pattern, "
"we can call ``register_injective_schedule``, "
"``register_broadcast_schedule``, and ``register_reduce_schedule`` "
"repsectively. The schedule function for these patterns are already "
"registered by each target and can be applied to these operators. We assume "
"the compute function should be the same across all targets, and "
"``FTVMCompute`` needs to be registered to the op before invoking register "
"schedule."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:210
msgid ""
"Second, for operators that doesn't have these common patterns mentioned "
"before, but also have the same compute function for all targets, we can use "
"``register_schedule`` API. It is easier to write ``FTVMSchedule`` function "
"as we only need to provide which schedule function to use. The following "
"code snippet shows ``FTVMSchedule`` function for pooling."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:229
msgid ""
"After we created the ``FTVMSchedule`` for an operator, we can register the "
"strategy using ``register_schedule``:"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:238
msgid "Register Strategies for A New Target"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:240
msgid ""
"There are two ways to register strategies for a new target. The more "
"straightforward one is adding a new target file in the directory "
"``python/tvm/relay/op/strategy``. You only need to customize the strategy "
"for ops that have been implemented for this new target and reuse the generic"
" strategies for the rest."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:246
msgid ""
"Alternatively, you can also register the strategy for the new target outside"
" the TVM python library. The following code snippet shows an example how to "
"do so. You can find more examples in ``vta/python/vta/top/op.py``."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:258
msgid "Select Implementation from Op Strategy"
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:260
msgid ""
"During the compilation, Relay compile engine needs to determine which "
"implementation to use for an operator when there are multiple. The selection"
" policy works as follows."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:264
msgid ""
"When the input tensors to an operator or a fused op all have constant "
"shapes, the compile engine first finds the best implementation based on "
"AutoTVM tuning logs. If there is no implementation that is an AutoTVM "
"template or all AutoTVM templates have fallback configs, the implementation "
"with highest priority level will then be chosen. Implementations with same "
"priority level in this case leads to an undefined behavior, and any of them "
"might be selected."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:271
msgid ""
"The selection policy for ops with symbolic input shapes is still work in "
"progess. Currently, if any input tensor has a symbolic shape, only the "
"implementation with highest priority level will be used for this operator. "
"This will be updated after the implemention finishes."
msgstr ""

#: ../../_staging/arch/relay_op_strategy.rst:276
msgid ""
"For debug purpose, you can add the following lines before you compile the "
"Relay model to learn which implementation is used for each operator."
msgstr ""
