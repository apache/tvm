# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/arch/index.rst:19
msgid "Design and Architecture"
msgstr ""

#: ../../_staging/arch/index.rst:21
msgid ""
"This document is intended for developers who want to understand the "
"architecture of TVM and/or actively develop on the project. This page is "
"organized as follows:"
msgstr ""

#: ../../_staging/arch/index.rst:25
msgid ""
"The `Example Compilation Flow`_ gives an overview of the steps that TVM "
"takes to turn a high level description of a model into a deployable "
"module. To get started, please read this section first."
msgstr ""

#: ../../_staging/arch/index.rst:28
msgid ""
"The `Logical Architecture Components`_ section describes the logical "
"components. The sections after are specific guides focused on each "
"logical component, organized by the component's name."
msgstr ""

#: ../../_staging/arch/index.rst:32
msgid ""
"The :ref:`Device/Target Interactions <tvm-target-specific-overview>` page"
" describes how TVM interacts with each supported physical device and "
"code-generation target."
msgstr ""

#: ../../_staging/arch/index.rst:36
msgid ""
"Feel free to also check out the :ref:`dev-how-to` for useful development "
"tips."
msgstr ""

#: ../../_staging/arch/index.rst:38
msgid ""
"This guide provides a few complementary views of the architecture. First,"
" we review a single end-to-end compilation flow and discuss the key data "
"structures and the transformations. This runtime-based view focuses on "
"the interactions of each components when running the compiler. Then we "
"will review the logical modules of the codebase and their relationship. "
"This part provides a static overarching view of the design."
msgstr ""

#: ../../_staging/arch/index.rst:45
msgid "Example Compilation Flow"
msgstr ""

#: ../../_staging/arch/index.rst:47
msgid ""
"In this guide, we will study an example compilation flow in the compiler."
" The figure below shows the flow. At a high-level, it contains several "
"steps:"
msgstr ""

#: ../../_staging/arch/index.rst:49
msgid ""
"Import: The frontend component ingests a model into an IRModule, which "
"contains a collection of functions that internally represent the model."
msgstr ""

#: ../../_staging/arch/index.rst:50
msgid ""
"Transformation: The compiler transforms an IRModule to another "
"functionally equivalent or approximately equivalent(e.g. in the case of "
"quantization) IRModule. Many of the transformations are target (backend) "
"independent. We also allow target to affect the configuration of the "
"transformation pipeline."
msgstr ""

#: ../../_staging/arch/index.rst:53
msgid ""
"Target Translation: The compiler translates(codegen) the IRModule to an "
"executable format specified by the target. The target translation result "
"is encapsulated as a `runtime.Module` that can be exported, loaded, and "
"executed on the target runtime environment."
msgstr ""

#: ../../_staging/arch/index.rst:55
msgid ""
"Runtime Execution: the user loads back a `runtime.Module` and runs the "
"compiled functions in the supported runtime environment."
msgstr ""

#: ../../_staging/arch/index.rst:64
msgid "Key data structures"
msgstr ""

#: ../../_staging/arch/index.rst:66
msgid ""
"One of the best ways to design and understand a complex system is to "
"identify the key data structures and APIs that manipulate (transform) "
"these data structures. Once we identified the key data structures, we can"
" then breakdown a system into logical components that either define a "
"collection of key data structures or transformations among the data "
"structures."
msgstr ""

#: ../../_staging/arch/index.rst:70
msgid ""
"**IRModule** is the primary data structure used across the entire stack. "
"An IRModule (intermediate representation module) contains a collection of"
" functions. Currently, we support two primary variants of functions."
msgstr ""

#: ../../_staging/arch/index.rst:73
msgid ""
"**relay::Function** is a high-level functional program representation. A "
"relay.Function usually corresponds to an end-to-end model. You can view a"
" relay.Function as a computational graph with additional support for "
"control-flow, recursion, and complex data structures."
msgstr ""

#: ../../_staging/arch/index.rst:75
msgid ""
"**tir::PrimFunc** is a low-level program representation that contains "
"elements including loop-nest choices, multi-dimensional load/store, "
"threading, and vector/tensor instructions. It is usually used to "
"represent an operator program that executes a (possibly-fused) layer in a"
" model."
msgstr ""

#: ../../_staging/arch/index.rst:78
msgid ""
"During the compilation, a relay function may be lowered to multiple "
"tir::PrimFunc functions and a top-level function that calls into those "
"tir::PrimFunc functions."
msgstr ""

#: ../../_staging/arch/index.rst:82
msgid "Transformations"
msgstr ""

#: ../../_staging/arch/index.rst:84
msgid ""
"Now that we have covered the key data structures, let us talk about the "
"transformations. Each transformation could serve one of the following "
"purposes:"
msgstr ""

#: ../../_staging/arch/index.rst:86
msgid ""
"optimization: transform a program to an equivalent, possibly more "
"optimized version."
msgstr ""

#: ../../_staging/arch/index.rst:87
msgid ""
"lowering: transform a program to a lower-level representation that is "
"closer to the target."
msgstr ""

#: ../../_staging/arch/index.rst:89
msgid ""
"**relay/transform** contains a collection of passes that optimize the "
"model. The optimizations include common program optimizations such as "
"constant folding and dead-code elimination, and tensor-computation "
"specific passes such as layout transformation and scaling factor folding."
msgstr ""

#: ../../_staging/arch/index.rst:93
msgid ""
"Near the end of the relay optimization pipeline, we will run a "
"pass(FuseOps) to break the end-to-end function(e.g. MobileNet) into sub-"
"function(e.g. conv2d-relu) segments. We call these segments of functions."
" This process helps us to divide the original problem into two sub-"
"problems:"
msgstr ""

#: ../../_staging/arch/index.rst:97
msgid "Compilation and optimization for each sub-function."
msgstr ""

#: ../../_staging/arch/index.rst:98
msgid ""
"Overall execution structure: we need to do a sequence of calls into the "
"generated sub-functions to execute the whole model."
msgstr ""

#: ../../_staging/arch/index.rst:100
msgid ""
"We use the low-level tir phase to compile and optimize each sub-"
"functions. For specific targets, we may also directly go to the target "
"translation phase and use external code generators."
msgstr ""

#: ../../_staging/arch/index.rst:103
msgid ""
"There are a few different ways(in relay/backend) to handle the calls into"
" the overall execution problem. For simple models with known shapes and "
"no control flow, we can lower to a graph executor that stores the "
"execution structure in a graph. We also support a virtual machine backend"
" for dynamic executions. Finally, we plan to support ahead of time "
"compilation that compiles the high-level execution structure into the "
"executable and generated primitive functions. All of these execution "
"modes are encapsulated by a unified **runtime.Module** interface, which "
"we will discuss in the latter part of the guide."
msgstr ""

#: ../../_staging/arch/index.rst:105
msgid ""
"**tir/transform** contains transformation passes for TIR level functions."
" Many tir passes serve the purpose of lowering. For example, there are "
"passes to flatten multi-dimensional access to one-dimensional pointer "
"access, to expand the intrinsics into target-specific ones, and to "
"decorate the function entry to meet the runtime calling convention. Of "
"course, there are also optimizations passes, such as access index "
"simplification and dead code elimination."
msgstr ""

#: ../../_staging/arch/index.rst:107
msgid ""
"Many low-level optimizations can be handled in the target phase by the "
"LLVM, CUDA C, and other target compilers. As a result, we leave low-level"
" optimizations such as register allocation to the downstream compilers "
"and only focus on optimizations that are not covered by them."
msgstr ""

#: ../../_staging/arch/index.rst:110
msgid "Search-space and Learning-based Transformations"
msgstr ""

#: ../../_staging/arch/index.rst:112
msgid ""
"The transformation passes we described so far are deterministic and rule-"
"based. One design goal of the TVM stack is to support high-performance "
"code optimizations for different hardware platforms. To do so, we will "
"need to investigate as many optimization choices as possible, including "
"but not limited to, multi-dimensional tensor access, loop tiling "
"behavior, special accelerator memory hierarchy, and threading."
msgstr ""

#: ../../_staging/arch/index.rst:114
msgid ""
"It is hard to define a heuristic to make all of the choices. Instead, we "
"will take a search and learning-based approach. We first define a "
"collection of actions we can take to transform a program. Example actions"
" include loop transformations, inlining, vectorization. We call these "
"actions **scheduling primitives**. The collection of scheduling "
"primitives defines a search space of possible optimizations we can make "
"to a program. The system then searches over different possible scheduling"
" sequence to pick the best scheduling combination. The search procedure "
"is usually guided by a machine learning algorithm."
msgstr ""

#: ../../_staging/arch/index.rst:121
msgid ""
"We can record the best schedule sequence for an (possibly-fused) operator"
" once the search is completed. The compiler can then just lookup the best"
" schedule sequence and apply it to the program. Notably, this schedule "
"application phase is **exactly like** the rule-based transformations, "
"enabling us to share the same interface convention with tradition passes."
msgstr ""

#: ../../_staging/arch/index.rst:125
msgid ""
"We use search based optimizations to handle the initial tir function "
"generation problem. This part of the module is called "
"AutoTVM(auto_scheduler). We expect to expand the learning-based "
"transformations to more areas as we continue to develop the TVM stack."
msgstr ""

#: ../../_staging/arch/index.rst:129
msgid "Target Translation"
msgstr ""

#: ../../_staging/arch/index.rst:131
msgid ""
"The target translation phase transforms an IRModule to the corresponding "
"target executable format. For backends such as x86 and ARM, we use the "
"LLVM IRBuilder to build in-memory LLVM IR. We can also generate source-"
"level languages such as CUDA C and OpenCL. Finally, we support direct "
"translations of a Relay function (sub-graph) to specific targets via "
"external code generators. It is important that the final code generation "
"phase is as lightweight as possible. Vast majority of transformations and"
" lowering should be performed before the target translation phase."
msgstr ""

#: ../../_staging/arch/index.rst:138
msgid ""
"We also provide a Target structure to specify the compilation target. The"
" transformations before the target translation phase can also be affected"
" by the target — for example, a target's vector length would change the "
"vectorization behavior."
msgstr ""

#: ../../_staging/arch/index.rst:144
msgid "Runtime Execution"
msgstr ""

#: ../../_staging/arch/index.rst:146
msgid ""
"The main goal of TVM's runtime is to provide a minimal API for loading "
"and executing the compiled artifact in a language of their choice, "
"including Python, C++, Rust, Go, Java, and JavaScript. The code snippet "
"below shows such an example in Python:"
msgstr ""

#: ../../_staging/arch/index.rst:159
msgid ""
":py:class:`tvm.runtime.Module` encapsulates the result of compilation. A "
"runtime.Module contains a GetFunction method to obtain PackedFuncs by "
"name."
msgstr ""

#: ../../_staging/arch/index.rst:161
msgid ""
":py:class:`tvm.runtime.PackedFunc` is a type-erased function interface "
"for both the generated functions. A runtime.PackedFunc can take arguments"
" and return values with the following types: POD types(int, float), "
"string, runtime.PackedFunc, runtime.Module, runtime.NDArray, and other "
"sub-classes of runtime.Object."
msgstr ""

#: ../../_staging/arch/index.rst:164
msgid ""
":py:class:`tvm.runtime.Module` and :py:class:`tvm.runtime.PackedFunc` are"
" powerful mechanisms to modularize the runtime. For example, to get the "
"above `addone` function on CUDA, we can use LLVM to generate the host-"
"side code to compute the launching parameters(e.g. size of the thread "
"groups) and then call into another PackedFunc from a CUDAModule that is "
"backed by the CUDA driver API. The same mechanism can be used for OpenCL "
"kernels."
msgstr ""

#: ../../_staging/arch/index.rst:166
msgid ""
"The above example only deals with a simple `addone` function. The code "
"snippet below gives an example of an end-to-end model execution using the"
" same interface:"
msgstr ""

#: ../../_staging/arch/index.rst:183
msgid ""
"The main take away is that runtime.Module and runtime.PackedFunc are "
"sufficient to encapsulate both operator level programs (such as addone), "
"as well as the end-to-end models."
msgstr ""

#: ../../_staging/arch/index.rst:186
msgid "Summary and Discussions"
msgstr ""

#: ../../_staging/arch/index.rst:188
msgid "In summary, the key data structures in the compilation flows are:"
msgstr ""

#: ../../_staging/arch/index.rst:190
msgid "IRModule: contains relay.Function and tir.PrimFunc"
msgstr ""

#: ../../_staging/arch/index.rst:191
msgid "runtime.Module: contains runtime.PackedFunc"
msgstr ""

#: ../../_staging/arch/index.rst:193
msgid ""
"Most parts of the compilation are transformations among the key data "
"structures."
msgstr ""

#: ../../_staging/arch/index.rst:195
msgid ""
"relay/transform and tir/transform are determinstic rule-based "
"transformations"
msgstr ""

#: ../../_staging/arch/index.rst:196
msgid "auto_scheduler and autotvm contains the search-based transformations"
msgstr ""

#: ../../_staging/arch/index.rst:198
msgid ""
"Finally, the compilation flow example is only a typical use-case of the "
"TVM stack. We expose these key data structures and transformations to "
"python and C++ APIs. As a result, you can use TVM just like the way you "
"use numpy, except that the data structure of interest changes from the "
"numpy.ndarray to tvm.IRModule. Here are some example use-cases:"
msgstr ""

#: ../../_staging/arch/index.rst:202
msgid "Directly construct IRModule using the python API."
msgstr ""

#: ../../_staging/arch/index.rst:203
msgid "Compose a custom set of transformations(e.g. customize quantization)."
msgstr ""

#: ../../_staging/arch/index.rst:204
msgid "Manipulate the IR directly using TVM's python API."
msgstr ""

#: ../../_staging/arch/index.rst:208
msgid "Logical Architecture Components"
msgstr ""

#: ../../_staging/arch/index.rst:214
msgid "TVM Architecture Diagram"
msgstr ""

#: ../../_staging/arch/index.rst:216
msgid ""
"The above figure shows the major logical components in the project. "
"Please read the following sections for information about the components "
"and their relations."
msgstr ""

#: ../../_staging/arch/index.rst:221
msgid "tvm/support"
msgstr ""

#: ../../_staging/arch/index.rst:222
msgid ""
"The support module contains the most common utilities for the "
"infrastructure, such as generic arena allocator, socket, and logging."
msgstr ""

#: ../../_staging/arch/index.rst:226
msgid "tvm/runtime"
msgstr ""

#: ../../_staging/arch/index.rst:228
msgid ""
"The runtime serves as the foundation of the TVM stack. It provides the "
"mechanism to load and execute compiled artifacts. The runtime defines a "
"stable standard set of C APIs to interface with frontend languages such "
"as Python and Rust."
msgstr ""

#: ../../_staging/arch/index.rst:231
msgid ""
"`runtime::Object` is one of the primary data structures in TVM runtime "
"besides the `runtime::PackedFunc`. It is a reference-counted base class "
"with a type index to support runtime type checking and downcasting. The "
"object system allows the developer to introduce new data structures to "
"the runtime, such as Array, Map, and new IR data structures."
msgstr ""

#: ../../_staging/arch/index.rst:235
msgid ""
"Besides deployment use-cases, the compiler itself also makes heavy use of"
" TVM's runtime mechanism. All of the IR data structures are subclasses of"
" `runtime::Object`, as a result, they can be directly accessed and "
"manipulated from the Python frontend. We use the PackedFunc mechanism to "
"expose various APIs to the frontend."
msgstr ""

#: ../../_staging/arch/index.rst:239
msgid ""
"Runtime support for different hardware backends are defined in "
"subdirectories of runtime(e.g. runtime/opencl). These hardware-specific "
"runtime modules define APIs for device memory allocation and device "
"function serialization."
msgstr ""

#: ../../_staging/arch/index.rst:242
msgid ""
"`runtime/rpc` implements an RPC support for PackedFunc. We can use the "
"RPC mechanism to send a cross-compiled library to a remote device and "
"benchmark the execution performance. The rpc infrastructure enables data "
"collection from a wide range of hardware backends for learning-based "
"optimizations."
msgstr ""

#: ../../_staging/arch/index.rst:264
msgid "tvm/node"
msgstr ""

#: ../../_staging/arch/index.rst:265
msgid ""
"The node module adds additional features on top of the `runtime::Object` "
"for IR data structures. The main features include reflection, "
"serialization, structural equivalence, and hashing."
msgstr ""

#: ../../_staging/arch/index.rst:268
msgid ""
"Thanks to the node module, we can directly access any field of the TVM's "
"IRNode by their name in Python."
msgstr ""

#: ../../_staging/arch/index.rst:279
msgid ""
"We can also serialize arbitrary IR node into a JSON format, and load them"
" back. The ability to save/store, and inspect an IR node provides a "
"foundation for making the compiler more accessible."
msgstr ""

#: ../../_staging/arch/index.rst:284
msgid "tvm/ir"
msgstr ""

#: ../../_staging/arch/index.rst:285
msgid ""
"The `tvm/ir` folder contains the unified data structure and interfaces "
"across for all IR function variants. The components in `tvm/ir` are "
"shared by `tvm/relay` and `tvm/tir`, notable ones include"
msgstr ""

#: ../../_staging/arch/index.rst:288
msgid "IRModule"
msgstr ""

#: ../../_staging/arch/index.rst:289
msgid "Type"
msgstr ""

#: ../../_staging/arch/index.rst:290
msgid "PassContext and Pass"
msgstr ""

#: ../../_staging/arch/index.rst:291
msgid "Op"
msgstr ""

#: ../../_staging/arch/index.rst:293
msgid ""
"Different variants of functions(e.g. relay.Function and tir.PrimFunc) can"
" co-exist in an IRModule. While these variants may not have the same "
"content representation, they use the same data structure to represent "
"types. As a consequence, we use the same data structure to represent "
"function (type) signatures of these variants. The unified type system "
"allows one function variant to call another function once we clearly "
"define the calling convention. This opens doors for future cross-"
"function-variant optimizations."
msgstr ""

#: ../../_staging/arch/index.rst:299
msgid ""
"We also provide a unified PassContext for configuring the pass behavior, "
"and common composite passes to execute a pass pipeline. The following "
"code snippet gives an example of PassContext configuration."
msgstr ""

#: ../../_staging/arch/index.rst:309
msgid ""
"Op is the common class to represent all system-defined primitive "
"operator/intrinsics. Developers can register new Ops as well as their "
"additional attributes(e.g. whether the Op is elementwise) to the system."
msgstr ""

#: ../../_staging/arch/index.rst:319
msgid "tvm/target"
msgstr ""

#: ../../_staging/arch/index.rst:320
msgid ""
"The target module contains all the code generators that translate an "
"IRModule to a target runtime.Module. It also provides a common `Target` "
"class that describes the target."
msgstr ""

#: ../../_staging/arch/index.rst:326
msgid ""
"The compilation pipeline can be customized according to the target by "
"querying the attribute information in the target and builtin information "
"registered to each target id(cuda, opencl)."
msgstr ""

#: ../../_staging/arch/index.rst:335
msgid "tvm/tir"
msgstr ""

#: ../../_staging/arch/index.rst:337
msgid ""
"TIR contains the definition of the low-level program representations. We "
"use `tir::PrimFunc` to represent functions that can be transformed by TIR"
" passes. Besides the IR data structures, the tir module also defines a "
"set of builtin intrinsics and their attributes via the common Op "
"registry, as well as transformation passes in `tir/transform`."
msgstr ""

#: ../../_staging/arch/index.rst:341
msgid "tvm/arith"
msgstr ""

#: ../../_staging/arch/index.rst:343
msgid ""
"This module is closely tied to the TIR. One of the key problems in the "
"low-level code generation is the analysis of the indices' arithmetic "
"properties — the positiveness, variable bound, and the integer set that "
"describes the iterator space. arith module provides a collection of tools"
" that do (primarily integer) analysis. A TIR pass can use these analyses "
"to simplify and optimize the code."
msgstr ""

#: ../../_staging/arch/index.rst:348
msgid "tvm/te"
msgstr ""

#: ../../_staging/arch/index.rst:350
msgid ""
"The name te stands for \"tensor expression\". This is a domain-specific "
"language module that allows us to construct `tir::PrimFunc` variants "
"quickly by writing tensor expressions. Importantly, a tensor expression "
"itself is not a self-contained function that can be stored into IRModule."
" Instead, it is a fragment of IR that we can stitch together to build an "
"IRModule."
msgstr ""

#: ../../_staging/arch/index.rst:353
msgid ""
"`te/schedule` provides a collection of scheduling primitives to control "
"the function being generated. In the future, we might bring some of these"
" scheduling components to the a `tir::PrimFunc` itself."
msgstr ""

#: ../../_staging/arch/index.rst:363
msgid "tvm/topi"
msgstr ""

#: ../../_staging/arch/index.rst:364
msgid ""
"While possible to construct operators directly via TIR or tensor "
"expressions (TE) for each use case it is tedious to do so. `topi` (Tensor"
" operator inventory) provides a set of pre-defined operators (in TE or "
"TIR) defined by numpy and found in common deep learning workloads. We "
"also provide a collection of common schedule templates to obtain "
"performant implementations across different target platforms."
msgstr ""

#: ../../_staging/arch/index.rst:370
msgid "tvm/relay"
msgstr ""

#: ../../_staging/arch/index.rst:371
msgid ""
"Relay is the high-level functional IR used to represent full models. "
"Various optimizations are defined in `relay.transform`. The Relay "
"compiler defines multiple dialects, and each dialect is designed to "
"support specific styles of optimization. Notable ones include QNN(for "
"importing pre-quantized models), VM(for lowering to dynamic virtual "
"machine), memory(for memory optimization)."
msgstr ""

#: ../../_staging/arch/index.rst:384
msgid "tvm/autotvm"
msgstr ""

#: ../../_staging/arch/index.rst:386
msgid ""
"AutoTVM and AutoScheduler are both components which automate search based"
" program optimization. This is rapidly evolving and primarily consists "
"of:"
msgstr ""

#: ../../_staging/arch/index.rst:388
msgid "Cost models and feature extraction."
msgstr ""

#: ../../_staging/arch/index.rst:389
msgid ""
"A record format for storing program benchmark results for cost model "
"construction."
msgstr ""

#: ../../_staging/arch/index.rst:390
msgid "A set of search policies over program transformations."
msgstr ""

#: ../../_staging/arch/index.rst:392
msgid ""
"Automated program optimization is still an active research field. As a "
"result, we have attempted to modularize the design so that researchers "
"may quickly modify a component or apply their own algorithms via the "
"Python bindings, and customize the search and plugin their algorithms "
"from the Python binding."
msgstr ""

#: ../../_staging/arch/index.rst:402
msgid "Frontends"
msgstr ""

#: ../../_staging/arch/index.rst:403
msgid ""
"Frontends ingest models from different frameworks into the TVM stack. "
":py:mod:`tvm.relay.frontend` is the namespace for model ingestion APIs."
msgstr ""

#: ../../_staging/arch/index.rst:413
msgid "Security"
msgstr ""

#: ../../_staging/arch/index.rst:421
msgid "microTVM"
msgstr ""

