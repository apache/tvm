# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/arch/convert_layout.rst:18
msgid "Convert Layout Pass"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:19
msgid "**Author**: `Animesh Jain <https://github.com/anijain2305>`_"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:23
msgid "1. Background"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:25
msgid ""
"Data layout format describes how the data is laid out in the memory. For "
"example, Tensorflow framework default data layout for convolution "
"operator is NHWC, i.e, the data is 4-dimensions and is laid out in row-"
"major format with N being the first dimension and C being the last "
"dimension. Data layout has a major role in model performance, "
"significantly affecting spatial and temporal locality. For example, Intel"
" x86 backend in TVM prefers layout as NCHWc where the C dimension is "
"tiled in 2 dimensions to exploit data locality efficiently. Similarly, "
"CUDA backend prefers the data layout to be in NCHW format."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:27
msgid ""
"Essentially, TVM has to deal with data layouts throughout the compiler "
"toolchain - Framework parsers, Relay layout transformations, and TOPI "
"schedules. As we move towards third-party codegen integration, which "
"might have their own data layout restrictions, handling layouts at all "
"levels in TVM toolchain is going to become even more challenging. "
"Therefore, we developed a new Relay pass - **ConvertLayout** -- to reduce"
" some of the complications that arise due to layout handling."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:29
msgid ""
"If you directly want to understand the usage of ConvertLayout Pass, "
"directly jump to Section 4 - Usage."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:33
msgid "2. Motivation and Overview"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:35
msgid ""
"Let's look at a simple scenario to understand the complications that "
"arise due to different layouts - Suppose we want to compile a Tensorflow "
"NHWC graph for an ARM edge device. But, suppose we currently support only"
" NCHW schedules in TOPI for ARM. So, there is a mismatch between "
"framework layout and TOPI-supported layout. One way to deal with this "
"mismatch is to insert layout transforms before each and after "
"convolution, such that resulting convolution has NCHW input data layout "
"and can use TOPI schedules. However, this can lead to performance "
"degradation because of the presence of too many layout transforms."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:37
msgid "We encountered similar problems in other use cases as well"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:39
msgid ""
"No way to run TFLite graphs on Nvidia GPUs. TOPI has NCHW-only schedules "
"for GPUs."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:40
msgid ""
"Ever-complicating logic in AlterOpLayout for convolution to support "
"different pairs of layout transformations."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:41
msgid "Sub-optimal performance for TF graphs due to extra layout transforms."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:42
msgid ""
"Complication in third-party codegen integrations like TensorRT that "
"prefers data layout to be in one format."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:44
msgid ""
"To solve these problems, we introduced *ConvertLayout* pass that sets up "
"the infrastructure to change the data layout of the whole graph with "
"minimal number of data layout transforms. In ideal cases, we will have "
"only 2 layout transforms for data, one at the start and one at the end. "
"An example to show the transformation is below"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:73
msgid "3. Design"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:75
msgid ""
"Before delving into ConvertLayout pass, let's categorize the operators "
"into 3 categories based on their sensitivity to data layouts. This "
"categorization will be useful later to understand Convertlayout pass "
"details."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:77
msgid ""
"**Layout agnostic** - Relu, pow etc. These operators are not affected, "
"neither functionality nor performance, by data layouts."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:78
msgid ""
"**Lightly-layout sensitive** - pad, concatenate, reduce ops like sum etc."
" These operators have some attributes that are functionally affected if "
"we do a layout transformation before them. However, performance-wise, the"
" difference is not significant. For these operators, it is beneficial to "
"just adapt to the previous operator output data layout."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:79
msgid ""
"**Heavily-layout sensitive** - Convolution, conv2d_transpose etc. These "
"operators are heavily affected, both functionally and performance-wise, "
"by data layouts. They also have data layout as the op attribute. "
"Typically, it is beneficial to modify the input data layouts for these "
"operators (if its not a performant data layout), while the rest of "
"*layout agnostic* and *lightly-layout sensitive* operators adapt to the "
"layout governed by the output of these *heavliy-layout sensitive* "
"operators."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:82
msgid ""
"Let us now look at two relevant Relay operator properties. Each relay "
"operator has properties, like InferType, that can be defined by a TVM "
"developer. Typically, a Relay pass traverses the graph operator-by-"
"operator and reads these operator properties. For example, InferType pass"
" looks at the InferType property of on operator, determines its output "
"shape and type, and then passes it to the next operator InferType "
"property. Similarly, in our context, we have 2 such properties - "
"*FTVMConvertLayout* and *FInferCorrectLayout*. ConvertLayout pass "
"traverses the graph and looks at these 2 properties along with an "
"automatic layout transform insertion module to handle data layouts. So, "
"the whole process can be broken down into 3 steps:"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:84
msgid ""
"Run FTVMConvertLayout property - This allows the developers to transform "
"the original Relay expr into a new Relay expr with new layouts, allowing "
"user-defined layout alteration. There is a python callback for "
"developer's ease. This is used only for heavily-layout sensitive "
"operators."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:85
msgid ""
"Run FTVMInferCorretLayout property - We can view this as layout "
"inference. It looks at the original input layout and the new input "
"layouts, which are either coming from previous operator or from the "
"FTVMConvertLayout modified expr (if it was used). This can be used by "
"lightly-layout sensitive operators to adapt its attributes to new data "
"layouts. Layout inference happens for each operator."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:86
msgid ""
"Automatic insertion of layout transforms - The previous step - layout "
"inference - sets the new layout for the input exprs. If these layouts are"
" different from the original layouts, then this component automatically "
"inserts a layout transform. Therefore, a developer does not need to do "
"anything for this component."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:88
msgid ""
"These steps happen for each operator in sequence, where ConvertLayout "
"pass keeps on passing the new layouts to the next operator properties, "
"finally resulting in modifying the whole graph operator-by-operator. Now,"
" let's look at a couple of examples of how to define the two properties."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:90
msgid ""
"**FTVMConvertLayout - Python callback for layout alteration** - This is "
"used for *heavily-layout sensitive* operators. For example, one can "
"return a new convolution operator with new data and kernel layout. The "
"other 2 components will infer layout and insert layout transforms if "
"needed. One example for convolution operator is as follows where we are "
"converting to NCHW layout."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:143
msgid ""
"**FInferCorrectLayout - Layout inference** - Currently, this attribute is"
" exposed only in C++. This function takes original input layouts and the "
"new input layouts (passed from the previous operator or from the python "
"callback for layout alteration), and infers the final data layouts. "
"Layout inference is called for each operator. The usage might vary for "
"different operator categories. For layout agnostic operators, we just "
"want to return the new data layouts in this function. For lightly-layout "
"and heavily-layout sensitive operators, we can change the operator "
"attributes (like axis for concatenate, pad_width for pad) so that we can "
"adapt to the new data layout, preventing insertion of layout transforms. "
"Let's look at a couple of examples to understand this better."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:145
msgid ""
"First example is for layout agnostic operators. These operators do not "
"have any operator attributes that are affected by data layouts, so we "
"just adapt to new layouts."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:175
msgid ""
"Second example is for a lightly-layout sensitive operator - batch "
"normalization. BatchNorm has an axis operator that has to change when we "
"go from NHWC to NCHW data layout. (Similar handling also needs to be for "
"heavily-layout sensitive operators)"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:229
msgid "4. Usage"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:232
msgid ""
"ConvertLayout pass is extremely easy to use. The pass is not a part of "
"default relay.build pipeline. The intended usage is to call it between "
"the framework-to-relay parser and relay.build module call."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:234
msgid ""
"In order to specify the layouts to convert to, we create a mapping of "
"heavily-layout sensitive operators to a list of the desired layouts for "
"that operator. The first example below specifies data layout, we allow "
"the kernel layout to be automatically converted to one that is supported "
"by TVM (for that particular data layout and operator). This is specified "
"by the use of the \"default\" keyword. The second example shows how we "
"could have also converted to a specific kernel layout of our choosing. "
"It's worth noting that the following examples will convert to the same "
"layouts i.e. `{'nn.conv2d': ['NCHW', 'default']} == {'nn.conv2d': "
"['NCHW', 'OIHW']}`"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:264
msgid ""
"The ordering of the layouts is defined by the implementation of "
"`register_convert_op_layout(\"OPNAME\")`, you can refer to the docstring "
"which should explicitly state the expected layout. In the examples above "
"it's [data_layout, kernel_layout]."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:266
msgid ""
"Current implementation has support for almost all the operators commonly "
"used in image classification models. However, if one encounters too many "
"data layout transforms in the graph, it is highly likely that there is an"
" operator whose layouts need special handling as described in Section 3. "
"Some pull requests that can help in such a situation are"
msgstr ""

#: ../../_staging/arch/convert_layout.rst:268
msgid ""
"Layout inference for `Batch Norm "
"<https://github.com/apache/tvm/pull/4600>`_ - Batch normalization falls "
"into the category of lightly-sensitive operator. The PR shows how to "
"handle the layout inference for batch norm."
msgstr ""

#: ../../_staging/arch/convert_layout.rst:269
msgid ""
"Python Callback for `Convolution "
"<https://github.com/apache/tvm/pull/4335>`_- For highly-sensitive "
"operators, one might have to do python callback as well. The PR shows how"
" to define a python callback function for Convolution operator."
msgstr ""

