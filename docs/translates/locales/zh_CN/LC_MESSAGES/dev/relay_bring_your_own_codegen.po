# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1713+gbe5f05f3f\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-18 01:16+0000\n"
"PO-Revision-Date: 2021-09-18 05:25+0000\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124870/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:22
msgid "Bring Your Own Codegen To TVM"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:24
msgid ""
"As the number of hardware devices targeted by deep learning workloads keeps "
"increasing, the required knowledge for users to achieve high performance on "
"various devices keeps increasing as well. To free data scientists from "
"worrying about the performance when developing a new model, hardware backend"
" providers either provide libraries such as MKLDNN or cuDNN with many "
"commonly used deep learning operators, or provide frameworks such as "
"TensorRT to let users describe their models in a certain way to achieve high"
" performance. However, users have to learn a new programming interface when "
"they attempt to work on a new library or device. As a result, the demand for"
" a unified programming interface becomes more and more important to 1) let "
"all users and hardware backend providers stand on the same page, and 2) "
"provide a feasible solution to allow specialized hardware or library to only"
" support widely used operators with extremely high performance, but fallback"
" unsupported operators to general devices like CPU/GPU."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:26
msgid ""
"In this developer guide, we demonstrate how you, as a hardware backend "
"provider, can easily implement your own codegen and register it as a Relay "
"backend compiler to support your hardware device/library. This guide covers "
"two types of codegen based on different graph representations you need:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:28
msgid "**1. You want to generate C code.**"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:30
msgid ""
"If your hardware already has a well-optimized C/C++ library, such as Intel "
"CBLAS/MKL to CPU and NVIDIA CUBLAS to GPU, then this is what you are looking"
" for. Fortunately, C source code module is fully compatible with TVM runtime"
" module, which means the generated code could be compiled by any C/C++ "
"compiler with proper compilation flags, so the only task you have is to "
"implement a codegen that generates C code for subgraphs and a C source "
"module to integrate into TVM runtime module. We will demonstrate how to "
"implement a C code generator for your hardware in the following section."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:32
msgid "**2. You want to generate any other graph representations.**"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:34
msgid ""
"Your hardware may require other forms of graph representation, such as JSON."
" In this case, you need to implement not only a codegen but also a "
"customized TVM runtime module to let TVM runtime know how this graph "
"representation should be executed. If you already have a complete graph "
"execution engine for your hardware, such as TensorRT for GPU, then this is a"
" solution you can consider."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:36
msgid ""
"After you finish the codegen and runtime, you can then let your customers "
"annotate their models with your customized tag to make use of them. The "
"tutorial for end-users to annotate and launch a specific codegen is **here "
"(TBA)**."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:40
msgid "Implement a C Codegen"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:42
msgid ""
"In this part, we demonstrate how to implement a codegen that generates C "
"code with pre-implemented operator functions. To simplify, our example "
"codegen does not depend on third-party libraries. Instead, we manually "
"implement two macros in C:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:63
msgid ""
"With the two macros, we can generate binary operators for 1-D and 2-D "
"tensors. For example, given a subgraph as follows. Assuming all inputs are "
"2-D tensors with shape (10, 10)."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:77
msgid ""
"Our goal is to generate the following compilable code to execute the "
"subgraph:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:132
msgid "Here we highlight the notes marked in the above code:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:134
msgid ""
"**Note 1** is the function implementation for the three nodes in the "
"subgraph."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:136
msgid ""
"**Note 2** is a function to execute the subgraph by allocating intermediate "
"buffers and invoking corresponding functions."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:138
msgid ""
"**Note 3** is a TVM runtime compatible wrapper function. It accepts a list "
"of input tensors and one output tensor (the last argument), casts them to "
"the right data type, and invokes the subgraph function described in Note 2. "
"In addition, ``TVM_DLL_EXPORT_TYPED_FUNC`` is a TVM macro that generates "
"another function ``gcc_0`` with unified the function arguments by packing "
"all tensors to ``TVMArgs``. As a result, the TVM runtime can directly invoke"
" ``gcc_0`` to execute the subgraph without additional efforts. With the "
"above code generated, TVM is able to compile it along with the rest parts of"
" the graph and export a single library for deployment."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:140
msgid ""
"In the rest of this section, we will implement a codegen step-by-step to "
"generate the above code. Your own codegen has to be located at "
"``src/relay/backend/contrib/<your-codegen-name>/``. In our example, we name "
"our codegen \"codegen_c\" and put it under "
"`/src/relay/backend/contrib/codegen_c/ "
"<https://github.com/apache/tvm/blob/main/src/relay/backend/contrib/codegen_c/codegen.cc>`_."
" Feel free to check this file for a complete implementation."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:142
msgid ""
"Specifically, we are going to implement two classes in this file and here is"
" their relationship:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:153
msgid ""
"When TVM backend finds a function (subgraph) in a Relay graph is annotated "
"with the registered compiler tag (``ccompiler`` in this example), TVM "
"backend invokes ``CSourceCodegen`` and passes the subgraph. "
"``CSourceCodegen``'s member function ``CreateCSourceModule`` will 1) "
"generate C code for the subgraph, and 2) wrap the generated C code to a C "
"source runtime module for TVM backend to compile and deploy. In particular, "
"the C code generation is transparent to the ``CodegenC`` class because it "
"provides many useful utilities to ease the code generation implementation. "
"The following sections will implement these two classes in the bottom-up "
"order."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:156
msgid "Implement CodegenC"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:158
msgid ""
"In ``src/relay/backend/contrib/codegen_c/codegen.cc``, we first create a "
"codegen class skeleton under the namespace of ``tvm.relay.contrib``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:204
msgid ""
"The ``CodegenC`` class inherits two classes: ``ExprVisitor`` provides "
"abilities to traverse subgraphs and collects the required information and "
"generate subgraph functions such as ``gcc_0_``; ``CodegenCBase`` provides "
"abilities and utilities to generate wrapper functions such as ``gcc_0`` in "
"the above example. As can be seen, we only need to implement three functions"
" in this codegen class to make it work."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:207
msgid "Code Generation for Operators"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:209
msgid ""
"We first implement ``VisitExpr_(const CallNode* call)``. This function "
"visits all call nodes when traversing the subgraph. Each call node contains "
"an operator that we want to offload to your hardware. As a result, we need "
"to generate the corresponding C code with correct operators in topological "
"order. We implement this function step-by-step as follows."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:211
msgid "**1. Generate the function declaration**"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:213
msgid "Example Result: ``GCC_BINARY_OP_2D(gcc_0_0, *, 10, 10);``"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:215
msgid ""
"To generate the function declaration, as shown above, we need 1) a function "
"name (e.g., ``gcc_0_0``), 2) the type of operator (e.g., ``*``), and 3) the "
"input tensor shape (e.g., ``(10, 10)``). Fortunately, this information can "
"be obtained easily from ``CallNode``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:248
msgid ""
"As can be seen, we push the generated code to class member variables "
"``func_decl_``. It means after we finish traversing the entire subgraph, we "
"have collected all required function declarations and the only thing we need"
" to do is having them compiled by GCC. The rest implementation of "
"``VisitExpr_(const CallNode* call)`` also follow this concept."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:250
msgid "**2. Generate the function call**"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:252
msgid "Example Result: ``gcc_0_0(buf_1, gcc_input3, out);``"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:254
msgid ""
"After generating the function declaration, we need to generate a function "
"call with proper inputs and outputs. To know which inputs or buffers we "
"should put when calling this function, we have to visit its arguments:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:272
msgid "Again, we want to highlight the notes in the above code:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:274
msgid ""
"**Note 1**: ``VisitExpr(call->args[i])`` is a recursive call to visit "
"arguments of the current function. An argument could be an output of another"
" node or an input tensor. In our example implementation, we make sure every "
"node updates a class variable ``out_`` before leaving the visitor. Here is "
"an illustration:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:285
msgid ""
"We can see in the above figure, class variable ``out_`` is empty before "
"visiting the argument node, and it was filled with the output buffer name "
"and size of ``arg_node``. As a result, when we finished visiting the "
"argument node, we know the proper input buffer we should put by looking at "
"``out_``. You will find out how we update ``out_`` at the end of this "
"section as well as the next section."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:287
msgid ""
"**Note 2**: You may notice that we did not close the function call string in"
" this step. The current function call string looks like: ``gcc_0_0(buf_1, "
"gcc_input3``. This is because we have not put the last argument (i.e., the "
"output) to this call. The output of a function call could be either an "
"allocated temporary buffer or the subgraph output tensor. For simplify, in "
"this example, we allocate an output buffer for every call node (next step) "
"and copy the result in the very last buffer to the output tensor."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:289
msgid "**3. Generate the output buffer**"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:291
msgid "Example Result: ``float* buf_0 = (float*)malloc(4 * 100);``"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:293
msgid ""
"As mentioned in the previous step, in addition to the subgraph input and "
"output tensors, we may also need buffers to keep the intermediate results. "
"To generate the buffer, we extract the shape information to determine the "
"buffer type and size:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:316
msgid ""
"After we have allocated the output buffer, we can now close the function "
"call string and push the generated function call to a class variable "
"``ext_func_body``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:323
msgid "**4. Update output buffer**"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:325
msgid ""
"To let the next node, which accepts the output of the current call node as "
"its input, know which buffer it should take, we need to update the class "
"variable ``out_`` before leaving this visit function:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:332
msgid ""
"Congratulations! we have finished the most difficult function in this class."
" In the next two sections, we just need to make up some minor missing parts "
"in this function."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:335
msgid "Code Generation for Input Variables"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:337
msgid ""
"Recall that we collected the input buffer information by visiting the "
"arguments of a call node (2nd step in the previous section), and handled the"
" case when its argument is another call node (4th step). In this section, we"
" demonstrate how to handle other nodes by taking ``VarNode`` as an example."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:339
msgid ""
"``VarNode`` represents input tensors in a model. The only but important "
"information it has is a name hint (e.g., ``data``, ``weight``, etc). When "
"visiting a ``VarNode``, we simply update class variable ``out_`` to pass the"
" name hint so that the descendant call nodes can generate the correct "
"function call."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:349
msgid ""
"Note that in this example we assume the subgraph we are offloading has only "
"call nodes and variable nodes. If your subgraphs contain other types of "
"nodes, such as ``TupleNode``, then you also need to visit them and bypass "
"the output buffer information."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:352
msgid "Code Emitting"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:354
msgid ""
"The final part in this codegen class is a ``JIT`` function that emits a C "
"function for the subgraph and uses the C code we just generated as the "
"function body. Remember, in addition to the subgraph function we generated "
"in the previous sections, we also need a wrapper function with a unified "
"argument for TVM runtime to invoke and pass data. Fortunately, the base "
"class we inherited already provides an implementation, ``JitImpl``, to "
"generate the function. For example, we can invoke ``JitImpl`` as follows:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:364
msgid ""
"The above call will generate three functions (one from the TVM wrapper "
"macro):"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:366
msgid ""
"The subgraph function ``gcc_0_`` (with one more underline at the end of the "
"function name) with all C code we generated to execute a subgraph."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:368
msgid ""
"The wrapper function ``gcc_0__wrapper_`` with a list of ``DLTensor`` "
"arguments that casts data to the right type and invokes ``gcc_0_``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:370
msgid ""
"The TVM runtime compatible function ``gcc_0`` with TVM unified function "
"arguments that unpacks TVM packed tensors and invokes ``gcc_0__wrapper_``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:372
msgid ""
"Accordingly, the only thing we need in ``JIT`` implementation is passing all"
" subgraph function code we generated to ``JitImpl``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:384
msgid ""
"All variables (``ext_func_id``, etc) we passed are class variables and were "
"filled when we traversed the subgraph."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:387
msgid "Implement CSourceCodegen"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:389
msgid ""
"Again, let's create a class skeleton and implement the required functions. "
"Note that it inherits ``CSourceModuleCodegenBase``"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:406
msgid "Implement GenCFunc"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:408
msgid ""
"``GenCFunc`` simply uses the ``CodegenC`` we just implemented to traverse a "
"Relay function (subgraph) and obtains the generated C code. The builtin "
"function ``GetExtSymbol`` retrieves a unique symbol name (e.g., ``gcc_0``) "
"in the Relay function and we **must** use it as the C function name, because"
" this symbol is going to be used for DSO runtime lookup."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:424
msgid "Implement CreateCSourceModule"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:426
msgid ""
"This function creates a runtime module for the external library. In this "
"example, we create a CSourceModule that can be directly compiled and linked "
"together with a TVM generated DSOModule. After you have implemented "
"``CodegenC``, implementing this function is relatively straightforward:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:482
msgid "Register Your Codegen"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:484
msgid ""
"The last step is registering your codegen to TVM backend. We first implement"
" a simple function to invoke our codegen and generate a runtime module."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:493
msgid "Finally, we register this function to TVM backend:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:499
msgid ""
"where ``ccompiler`` is a customized tag to let TVM know this is the codegen "
"it should use to generate and offload subgraphs when the subgraph is "
"annotated with ``ccompiler``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:501
msgid ""
"Finally, a good practice is to set up a CMake configuration flag to include "
"your compiler only for your customers. We first create a cmake file: "
"``cmake/modules/contrib/CODEGENC.cmake``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:510
msgid ""
"So that users can configure whether to include your compiler when "
"configuring TVM using ``config.cmake``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:518
msgid "Implement a Codegen for Your Representation"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:520
msgid ""
"Although we have demonstrated how to implement a C codegen, your hardware "
"may require other forms of graph representation, such as JSON. In this case,"
" you could modify ``CodegenC`` class we have implemented to generate your "
"own graph representation and implement a customized runtime module to let "
"TVM runtime know how this graph representation should be executed."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:522
msgid ""
"To simplify, we define a graph representation named \"ExampleJSON\" in this "
"guide. ExampleJSON does not mean the real JSON but just a simple "
"representation for graphs without a control flow. For example, assuming we "
"have the following subgraph named ``subgraph_0``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:536
msgid "Then the ExampleJON of this subgraph looks like:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:549
msgid ""
"The ``input`` keyword declares an input tensor with its ID and shape; while "
"the other statements describes computations in ``<op> <output ID> inputs: "
"[input ID] shape: [shape]`` syntax."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:551
msgid ""
"In this section, our goal is to implement the following customized TVM "
"runtime module to execute ExampleJSON graphs."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:564
msgid ""
"**Note 1**: We will implement a customized codegen later to generate a "
"ExampleJSON code string by taking a subgraph."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:566
msgid ""
"**Note 2**: This line obtains a pointer to a function for creating the "
"customized runtime module. You can see that it takes subgraph code in "
"ExampleJSON format we just generated and initializes a runtime module."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:568
msgid ""
"In the following sections, we are going to introduce 1) how to implement "
"``ExampleJsonCodeGen`` and 2) how to implement and register "
"``examplejson_module_create``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:571
msgid "Implement ExampleJsonCodeGen"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:573
msgid ""
"Similar to the C codegen, we also derive ``ExampleJsonCodeGen`` from "
"``ExprVisitor`` to make use of visitor patterns for subgraph traversing. On "
"the other hand, we do not have to inherit ``CodegenCBase`` because we do not"
" need TVM C++ wrappers. The codegen class is implemented as follows:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:619
msgid ""
"**Note 1**: We again implement corresponding visitor functions to generate "
"ExampleJSON code and store it to a class variable ``code`` (we skip the "
"visitor function implementation in this example as their concepts are "
"basically the same as C codegen). After finished the graph visiting, we "
"should have an ExampleJSON graph in ``code``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:621
msgid ""
"**Note 2**: We define an internal API ``gen`` to take a subgraph and "
"generate a ExampleJSON code. This API can be in an arbitrary name you "
"prefer."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:623
msgid ""
"The next step is to implement a customized runtime to make use of the output"
" of ``ExampleJsonCodeGen``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:626
msgid "Implement a Customized Runtime"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:628
msgid ""
"In this section, we will implement a customized TVM runtime step-by-step and"
" register it to TVM runtime modules. The customized runtime should be "
"located at ``src/runtime/contrib/<your-runtime-name>/``. In our example, we "
"name our runtime \"example_ext_runtime\"."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:630
msgid ""
"Again, we first define a customized runtime class as follows. The class has "
"to be derived from TVM ``ModuleNode`` in order to be compatible with other "
"TVM runtime modules."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:686
msgid ""
"In particular, there are some functions derived from ``ModuleNode`` that we "
"must implement in ``ExampleJsonModule``:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:688
msgid ""
"Constructor: The constructor of this class should accept a subgraph (in your"
" representation), process and store it in any format you like. The saved "
"subgraph could be used by the following two functions."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:690
msgid ""
"``GetFunction``: This is the most important function in this class. When TVM"
" runtime wants to execute a subgraph with your compiler tag, TVM runtime "
"invokes this function from your customized runtime module. It provides the "
"function name as well as runtime arguments, and ``GetFunction`` should "
"return a packed function implementation for TVM runtime to execute."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:692
msgid ""
"``SaveToBinary`` and ``LoadFromBinary``: ``SaveToBinary`` serialize the "
"runtime module to a binary format for later deployment. This function will "
"be called by TVM when users use ``export_library`` API. On the other hand, "
"since we are now using our own graph representation, we have to make sure "
"that ``LoadFromBinary`` is able to construct the same runtime module by "
"taking the serialized binary generated by ``SaveToBinary``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:694
msgid ""
"``GetSource`` (optional): If you would like to see the generated ExampleJSON"
" code, you can implement this function to dump it; otherwise you can skip "
"the implementation."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:696
msgid ""
"Other functions and class variables will be introduced along with the "
"implementation of above must-have functions."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:699
msgid "Implement Constructor"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:708
msgid ""
"Then, we implement ``ParseJson`` to parse a subgraph in ExampleJSON format "
"and construct a graph in memory for later usage. Since we do not support "
"subgraph with branches in this example, we simply use an array to store "
"every nodes in a subgraph in order."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:767
msgid ""
"**Note 1**: We use a class variable ``op_id_`` to map from subgraph node ID "
"to the operator name (e.g., ``add``) so that we can invoke the corresponding"
" operator function in runtime."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:769
msgid ""
"**Note 2**: We use a class variable ``graph_`` to map from subgraph name to "
"an array of nodes. ``GetFunction`` will query graph nodes by a subgraph ID "
"in runtime."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:771
msgid ""
"**Note 3**: We use a class variable `data_entry_` to map from a subgraph "
"node ID to a tensor data placeholder. We will put inputs and outputs to the "
"corresponding data entry in runtime."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:774
msgid "Implement GetFunction"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:776
msgid ""
"After the construction, we should have the above class variables ready. We "
"then implement ``GetFunction`` to provide executable subgraph functions to "
"TVM runtime:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:822
msgid ""
"As can be seen, ``GetFunction`` is composed of three major parts. The first "
"part copies data from TVM runtime arguments to the corresponding data "
"entries we assigned in the constructor. The second part executes the "
"subgraph with ``Run`` function (will implement later) and saves the results "
"to another data entry. The third part copies the results from the output "
"data entry back to the corresponding TVM runtime argument for output."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:825
msgid "Implement Run"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:827
msgid ""
"Now let's implement ``Run`` function. This function accepts 1) a subgraph "
"ID, 2) a list of input data entry indexs, and 3) an output data entry index."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:862
msgid ""
"``Run`` function mainly has two parts. The first part allocates a list of "
"``TVMValue``, and maps corresponding data entry blocks. This will become the"
" arguments of our operator functions. The second part than invokes our "
"operator functions. Although we use the same C functions as the previous "
"example, you can replace ``Add``, ``Sub``, and ``Mul`` with your own engine."
" You only need to make sure your engine stores the results to the last "
"argument so that they can be transferred back to TVM runtime."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:864
msgid ""
"With above functions implemented, our customized codegen and runtime can now"
" execute subgraphs. The last step is registering an API "
"(``examplejson_module_create``) to create this module:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:875
msgid "Implement SaveToBinary and LoadFromBinary"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:877
msgid ""
"So far we have implemented the main features of a customized runtime so that"
" it can be used as other TVM runtimes. However, when users want to save the "
"built runtime to a disk for deployment, TVM has no idea about how to save "
"it. This is the reason we want to implement ``SaveToBinary`` and "
"``LoadFromBinary``, which tell TVM how should this customized runtime be "
"persist and restored."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:879
msgid ""
"We first implement ``SaveToBinary`` function to allow users to save this "
"module in disk."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:887
msgid ""
"We can find that this function is pretty simple. Recall that the only "
"argument we took in constructor is a subgraph representation, meaning that "
"we only need a subgraph representation to construct/recover this customized "
"runtime module. As a result, ``SaveToBinary`` simply writes the subgraph to "
"an output DMLC stream. That is, when users use ``export_library`` API to "
"export the module, the customized module will be an ExampleJSON stream of a "
"subgraph."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:889
msgid ""
"Similarity, ``LoadFromBinary`` reads the subgraph stream and re-constructs "
"the customized runtime module:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:901
msgid ""
"We also need to register this function to enable the corresponding Python "
"API:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:908
msgid ""
"The above registration means when users call "
"``tvm.runtime.load_module(lib_path)`` API and the exported library has an "
"ExampleJSON stream, our ``LoadFromBinary`` will be invoked to create the "
"same customized runtime module."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:910
msgid ""
"In addition, if you want to support module creation directly from an "
"ExampleJSON file, you can also implement a simple function and register a "
"Python API as follows:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:933
msgid ""
"It means users can manually write/modify an ExampleJSON file, and use Python"
" API ``tvm.runtime.load_module(\"mysubgraph.examplejson\", "
"\"examplejson\")`` to construct a customized module."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:937
msgid "Summary"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:939
msgid "In summary, here is a checklist for you to refer:"
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:941
msgid ""
"A codegen class derived from ``ExprVisitor`` and ``CodegenCBase`` (only for "
"C codegen) with following functions."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:943
msgid "``VisitExpr_(const CallNode* call)`` to collect call node information."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:944
msgid "Other visitor functions you needed to collect subgraph information."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:945
msgid "``JIT`` to generate subgraph code."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:946
msgid "Register codegen."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:948
msgid "A function to create ``CSourceModule`` (for C codegen)."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:950
msgid ""
"A runtime module class derived from ``ModuleNode`` with following functions "
"(for your graph representation)."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:952
msgid "Constructor."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:953
msgid "``GetFunction`` to generate a TVM runtime compatible ``PackedFunc``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:954
msgid "``Run`` to execute a subgraph."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:955
msgid "Register a runtime creation API."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:956
msgid ""
"``SaveToBinary`` and ``LoadFromBinary`` to serialize/deserialize customized "
"runtime module."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:957
msgid ""
"Register ``LoadFromBinary`` API to support "
"``tvm.runtime.load_module(your_module_lib_path)``."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:958
msgid ""
"(optional) ``Create`` to support customized runtime module construction from"
" subgraph file in your representation."
msgstr ""

#: ../../_staging/dev/relay_bring_your_own_codegen.rst:960
msgid ""
"An annotator to annotate a user Relay program to make use of your compiler "
"and runtime (TBA)."
msgstr ""
