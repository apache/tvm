# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# a_flying_fish <a_flying_fish@outlook.com>, 2021
# HLearning, 2021
# JiaKui Hu, 2021
# juzi, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:38+0000\n"
"Last-Translator: juzi, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/reference/api/python/relay/nn.rst:19
msgid "tvm.relay.nn"
msgstr ""

#: ../../_staging/docstring of tvm.relay.nn:1
msgid "Neural network related operators."
msgstr ""

#: ../../_staging/docstring of tvm.relay.nn:1
msgid "**Classes:**"
msgstr "**类：**"

#: ../../_staging/docstring of tvm.relay.nn:1:<autosummary>:1
msgid ":obj:`Constant <tvm.relay.nn.Constant>`\\ \\(data\\)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.nn:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of tvm.relay.expr.Constant:1
msgid "A constant expression in Relay."
msgstr ""

#: ../../_staging/docstring of tvm.relay.nn:1:<autosummary>:1
msgid ":obj:`Expr <tvm.relay.nn.Expr>`\\"
msgstr ""

#: ../../_staging/docstring of tvm.relay.nn:1:<autosummary>:1
msgid "alias of :class:`tvm.ir.expr.RelayExpr`"
msgstr ""

#: ../../_staging/docstring of tvm.relay.nn:1
msgid "**Functions:**"
msgstr "**函数：**"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`adaptive_avg_pool1d <tvm.relay.nn.adaptive_avg_pool1d>`\\ "
"\\(data\\[\\, output\\_size\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "1D adaptive average pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`adaptive_avg_pool2d <tvm.relay.nn.adaptive_avg_pool2d>`\\ "
"\\(data\\[\\, output\\_size\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "2D adaptive average pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`adaptive_avg_pool3d <tvm.relay.nn.adaptive_avg_pool3d>`\\ "
"\\(data\\[\\, output\\_size\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "3D adaptive avg pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`adaptive_max_pool1d <tvm.relay.nn.adaptive_max_pool1d>`\\ "
"\\(data\\[\\, output\\_size\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "1D adaptive max pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`adaptive_max_pool2d <tvm.relay.nn.adaptive_max_pool2d>`\\ "
"\\(data\\[\\, output\\_size\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "2D adaptive max pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`adaptive_max_pool3d <tvm.relay.nn.adaptive_max_pool3d>`\\ "
"\\(data\\[\\, output\\_size\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "3D adaptive max pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`avg_pool1d <tvm.relay.nn.avg_pool1d>`\\ \\(data\\[\\, pool\\_size\\, "
"strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.avg_pool1d:1
msgid "1D average pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`avg_pool2d <tvm.relay.nn.avg_pool2d>`\\ \\(data\\[\\, pool\\_size\\, "
"strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.avg_pool2d:1
msgid "2D average pooling operator."
msgstr "2D 平均池化算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`avg_pool2d_grad <tvm.relay.nn.avg_pool2d_grad>`\\ \\(out\\_grad\\, "
"data\\[\\, pool\\_size\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.avg_pool2d_grad:1
msgid "Gradient of 2D average pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`avg_pool3d <tvm.relay.nn.avg_pool3d>`\\ \\(data\\[\\, pool\\_size\\, "
"strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.avg_pool3d:1
msgid "3D average pooling operator."
msgstr "3D 平均池化算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`batch_flatten <tvm.relay.nn.batch_flatten>`\\ \\(data\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.batch_flatten:1
msgid "BatchFlatten."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`batch_matmul <tvm.relay.nn.batch_matmul>`\\ \\(tensor\\_a\\, "
"tensor\\_b\\[\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.batch_matmul:1
msgid "Compute batch matrix multiplication of `tensor_a` and `tensor_b`."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`batch_norm <tvm.relay.nn.batch_norm>`\\ \\(data\\, gamma\\, beta\\, "
"moving\\_mean\\, ...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
msgstr "批量处理规范化层 （Ioffe and Szegedy, 2014）。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`batch_to_space_nd <tvm.relay.nn.batch_to_space_nd>`\\ \\(data\\, "
"block\\_shape\\, crops\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.batch_to_space_nd:1
msgid "Reshape the batch dimension into spatial dimensions."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`bias_add <tvm.relay.nn.bias_add>`\\ \\(data\\, bias\\[\\, axis\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.bias_add:1
msgid "add_bias operator."
msgstr "add_bias 算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`bitpack <tvm.relay.nn.bitpack>`\\ \\(data\\[\\, bits\\, pack\\_axis\\,"
" bit\\_axis\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.bitpack:1
msgid "Tensor packing for bitserial operations."
msgstr "对于位序列算子的张量包装。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`bitserial_conv2d <tvm.relay.nn.bitserial_conv2d>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.bitserial_conv2d:1
msgid "2D convolution using bitserial computation."
msgstr "使用位序列计算的 2D 卷积。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`bitserial_dense <tvm.relay.nn.bitserial_dense>`\\ \\(data\\, "
"weight\\[\\, units\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Bitserial Dense operator."
msgstr "位序列稠密算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`const <tvm.relay.nn.const>`\\ \\(value\\[\\, dtype\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.expr.const:1
msgid "Create a constant value."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv2d_gemm_weight_transform "
"<tvm.relay.nn.contrib_conv2d_gemm_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:1
msgid "Weight Transformation part for 2D convolution with gemm algorithm."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv2d_gemm_without_weight_transform "
"<tvm.relay.nn.contrib_conv2d_gemm_without_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:1
msgid "2D convolution with gemm algorithm."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv2d_nchwc <tvm.relay.nn.contrib_conv2d_nchwc>`\\ \\(data\\,"
" kernel\\[\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:1
msgid "Variant of 2D convolution."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv2d_winograd_nnpack_weight_transform "
"<tvm.relay.nn.contrib_conv2d_winograd_nnpack_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:1
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:1
msgid "Weight Transformation part for 2D convolution with winograd algorithm."
msgstr "基于 Winograd 算法对 2D 卷积的转换部分进行加权。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv2d_winograd_weight_transform "
"<tvm.relay.nn.contrib_conv2d_winograd_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv2d_winograd_without_weight_transform "
"<tvm.relay.nn.contrib_conv2d_winograd_without_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:1
msgid "2D convolution with winograd algorithm."
msgstr "基于 Winograd 算法的 2D 卷积。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv3d_winograd_weight_transform "
"<tvm.relay.nn.contrib_conv3d_winograd_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:1
msgid "Weight Transformation part for 3D convolution with winograd algorithm."
msgstr "基于 Winograd 算法对 3D 卷积的转换部分进行加权。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_conv3d_winograd_without_weight_transform "
"<tvm.relay.nn.contrib_conv3d_winograd_without_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:1
msgid "3D convolution with winograd algorithm."
msgstr "基于 Winograd 算法的 3D 卷积。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_dense_pack <tvm.relay.nn.contrib_dense_pack>`\\ \\(data\\, "
"weight\\[\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Dense operator."
msgstr "稠密算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`contrib_depthwise_conv2d_nchwc "
"<tvm.relay.nn.contrib_depthwise_conv2d_nchwc>`\\ \\(data\\, kernel\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:1
msgid "Variant of 2D depthwise convolution."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`conv1d <tvm.relay.nn.conv1d>`\\ \\(data\\, weight\\[\\, strides\\, "
"padding\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.conv1d:1
msgid "1D convolution."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`conv1d_transpose <tvm.relay.nn.conv1d_transpose>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.conv1d_transpose:1
msgid "One dimensional transposed convolution operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`conv2d <tvm.relay.nn.conv2d>`\\ \\(data\\, weight\\[\\, strides\\, "
"padding\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.conv2d:1
msgid "2D convolution."
msgstr "2D 卷积。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`conv2d_transpose <tvm.relay.nn.conv2d_transpose>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.conv2d_transpose:1
msgid "Two dimensional transposed convolution operator."
msgstr "二维转置卷积算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`conv3d <tvm.relay.nn.conv3d>`\\ \\(data\\, weight\\[\\, strides\\, "
"padding\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.conv3d:1
msgid "3D convolution."
msgstr "3D 卷积。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`conv3d_transpose <tvm.relay.nn.conv3d_transpose>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.conv3d_transpose:1
msgid "3D transpose convolution."
msgstr "3D 转置卷积。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`correlation <tvm.relay.nn.correlation>`\\ \\(data1\\, data2\\, "
"kernel\\_size\\, ...\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.correlation:1
msgid "Applies correlation to inputs."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`cross_entropy <tvm.relay.nn.cross_entropy>`\\ \\(predictions\\, "
"targets\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.cross_entropy:1
msgid "CrossEntropy without logits."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`cross_entropy_with_logits <tvm.relay.nn.cross_entropy_with_logits>`\\ "
"\\(predictions\\, targets\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:1
msgid "CrossEntropy with logits."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`deformable_conv2d <tvm.relay.nn.deformable_conv2d>`\\ \\(data\\, "
"offset\\, weight\\[\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.deformable_conv2d:1
msgid "Deformable 2d convolution."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`dense <tvm.relay.nn.dense>`\\ \\(data\\, weight\\[\\, units\\, out\\_dtype\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`depth_to_space <tvm.relay.nn.depth_to_space>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\, mode\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.depth_to_space:1
msgid "Convert channels into spatial blocks."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`dilate <tvm.relay.nn.dilate>`\\ \\(data\\, strides\\[\\, dilation\\_value\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.dilate:1
msgid "Dilate data with given dilation value (0 by default)."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`dropout <tvm.relay.nn.dropout>`\\ \\(data\\[\\, rate\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.dropout:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.dropout_raw:1
msgid "Applies the dropout operation to the input array."
msgstr "对输入数列使用 dropout 算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`dropout_raw <tvm.relay.nn.dropout_raw>`\\ \\(data\\[\\, rate\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`fast_softmax <tvm.relay.nn.fast_softmax>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.softmax:1
msgid "Computes softmax."
msgstr "计算 softmax。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`fifo_buffer <tvm.relay.nn.fifo_buffer>`\\ \\(data\\, buffer\\, axis\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.fifo_buffer:1
msgid ""
"FIFO buffer to enable computation reuse in CNNs with sliding indow input"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`get_pad_tuple1d <tvm.relay.nn.get_pad_tuple1d>`\\ \\(padding\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.utils.get_pad_tuple1d:1
msgid ""
"Common code to get the 1 dimensional pad option :param padding: Padding size"
" :type padding: Union[int, Tuple[int, ...]]"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`get_pad_tuple2d <tvm.relay.nn.get_pad_tuple2d>`\\ \\(padding\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.utils.get_pad_tuple2d:1 ../../_staging/docstring of
#: tvm.relay.op.nn.utils.get_pad_tuple3d:1
msgid ""
"Common code to get the pad option :param padding: Padding size :type "
"padding: Union[int, Tuple[int, ...]]"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`get_pad_tuple3d <tvm.relay.nn.get_pad_tuple3d>`\\ \\(padding\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`global_avg_pool1d <tvm.relay.nn.global_avg_pool1d>`\\ \\(data\\[\\, "
"layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.global_avg_pool1d:1
msgid "1D global average pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`global_avg_pool2d <tvm.relay.nn.global_avg_pool2d>`\\ \\(data\\[\\, "
"layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.global_avg_pool2d:1
msgid "2D global average pooling operator."
msgstr "2D 全局平均池化算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`global_avg_pool3d <tvm.relay.nn.global_avg_pool3d>`\\ \\(data\\[\\, "
"layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.global_avg_pool3d:1
msgid "3D global average pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`global_max_pool1d <tvm.relay.nn.global_max_pool1d>`\\ \\(data\\[\\, "
"layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.global_max_pool1d:1
msgid "1D global maximum pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`global_max_pool2d <tvm.relay.nn.global_max_pool2d>`\\ \\(data\\[\\, "
"layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.global_max_pool2d:1
msgid "2D global maximum pooling operator."
msgstr "2D 全局最大池化算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`global_max_pool3d <tvm.relay.nn.global_max_pool3d>`\\ \\(data\\[\\, "
"layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.global_max_pool3d:1
msgid "3D global maximum pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`group_norm <tvm.relay.nn.group_norm>`\\ \\(data\\, gamma\\, beta\\, "
"num\\_groups\\[\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
"Group normalization normalizes over group of channels for each training "
"examples."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`instance_norm <tvm.relay.nn.instance_norm>`\\ \\(data\\, gamma\\, "
"beta\\[\\, axis\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.instance_norm:1
msgid ""
"Instance Normalization (Ulyanov and et al., 2016) Applies instance "
"normalization to the n-dimensional input array."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`l2_normalize <tvm.relay.nn.l2_normalize>`\\ \\(data\\, eps\\[\\, axis\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.l2_normalize:1
msgid "Perform L2 normalization on the input data"
msgstr "对输入数据执行L2标准化"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`layer_norm <tvm.relay.nn.layer_norm>`\\ \\(data\\, gamma\\, beta\\[\\,"
" axis\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Layer normalization (Lei Ba and et al., 2016)."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`leaky_relu <tvm.relay.nn.leaky_relu>`\\ \\(data\\[\\, alpha\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.leaky_relu:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.prelu:1
msgid ""
"This operator takes data as input and does Leaky version of a Rectified "
"Linear Unit."
msgstr "该算子将数据作为输入，并代入带泄露的修正线性单元。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`log_softmax <tvm.relay.nn.log_softmax>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.log_softmax:1
msgid "Computes log softmax."
msgstr "计算 log_softmax。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`lrn <tvm.relay.nn.lrn>`\\ \\(data\\[\\, size\\, axis\\, bias\\, alpha\\, beta\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.lrn:1
msgid ""
"This operator takes data as input and does local response normalization."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`matmul <tvm.relay.nn.matmul>`\\ \\(tensor\\_a\\, tensor\\_b\\[\\, units\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Matmul operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`max_pool1d <tvm.relay.nn.max_pool1d>`\\ \\(data\\[\\, pool\\_size\\, "
"strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.max_pool1d:1
msgid "1D maximum pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`max_pool2d <tvm.relay.nn.max_pool2d>`\\ \\(data\\[\\, pool\\_size\\, "
"strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.max_pool2d:1
msgid "2D maximum pooling operator."
msgstr "2D 最大池化算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`max_pool2d_grad <tvm.relay.nn.max_pool2d_grad>`\\ \\(out\\_grad\\, "
"data\\[\\, pool\\_size\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.max_pool2d_grad:1
msgid "Gradient of 2D maximum pooling operator."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`max_pool3d <tvm.relay.nn.max_pool3d>`\\ \\(data\\[\\, pool\\_size\\, "
"strides\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.max_pool3d:1
msgid "3D maximum pooling operator."
msgstr "3D 最大池化算子。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`mirror_pad <tvm.relay.nn.mirror_pad>`\\ \\(data\\, pad\\_width\\[\\, mode\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.mirror_pad:1
msgid "MirrorPadding"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`nll_loss <tvm.relay.nn.nll_loss>`\\ \\(predictions\\, targets\\, "
"weights\\[\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.nll_loss:1
msgid "Negative log likelihood loss."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`pad <tvm.relay.nn.pad>`\\ \\(data\\, pad\\_width\\[\\, pad\\_value\\, pad\\_mode\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.pad:1
msgid "Padding"
msgstr "填充"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`prelu <tvm.relay.nn.prelu>`\\ \\(data\\, alpha\\[\\, axis\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`relu <tvm.relay.nn.relu>`\\ \\(data\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.relu:1
msgid "Rectified linear unit."
msgstr "修正线性单元。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`softmax <tvm.relay.nn.softmax>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`space_to_batch_nd <tvm.relay.nn.space_to_batch_nd>`\\ \\(data\\, "
"block\\_shape\\, paddings\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.space_to_batch_nd:1
msgid ""
"Divide spatial dimensions of the data into a grid of blocks and interleave "
"them into batch dim."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`space_to_depth <tvm.relay.nn.space_to_depth>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.space_to_depth:1
msgid "Convert spatial blocks into channels."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`sparse_add <tvm.relay.nn.sparse_add>`\\ \\(dense\\_mat\\, sparse\\_mat\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.sparse_add:1
msgid ""
"Computes the matrix addition of `dense_mat` and `sparse_mat`, where "
"`dense_mat` is a dense matrix and `sparse_mat` is a sparse (CSR) namedtuple "
"with fields `data`, `indices`, and `indptr`."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`sparse_dense <tvm.relay.nn.sparse_dense>`\\ \\(dense\\_mat\\, "
"sparse\\_mat\\[\\, sparse\\_lhs\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.sparse_dense:1
msgid ""
"Computes the matrix multiplication of `dense_mat` and `sparse_mat`, where "
"`dense_mat` is a dense matrix and `sparse_mat` is a sparse (either BSR or "
"CSR) namedtuple with fields `data`, `indices`, and `indptr`."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":obj:`sparse_transpose <tvm.relay.nn.sparse_transpose>`\\ \\(x\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.sparse_transpose:1
msgid ""
"Computes the fast matrix transpose of x, where x is a sparse tensor in CSR "
"format (represented as a namedtuple with fields `data`, `indices`, and "
"`indptr`)."
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`upsampling <tvm.relay.nn.upsampling>`\\ \\(data\\[\\, scale\\_h\\, "
"scale\\_w\\, layout\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.upsampling:1
msgid "Upsampling."
msgstr "上采样。"

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":obj:`upsampling3d <tvm.relay.nn.upsampling3d>`\\ \\(data\\[\\, scale\\_d\\,"
" scale\\_h\\, ...\\]\\)"
msgstr ""

#: ../../../python/tvm/relay/expr.pydocstring of
#: tvm.relay.expr.Constant:1:<autosummary>:1 ../../_staging/docstring of
#: tvm.relay.op.nn.nn.upsampling3d:1
msgid "3D Upsampling."
msgstr "3D 上采样。"

#: ../../../python/tvm/relay/expr.pydocstring of tvm.relay.expr.Constant:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:0
#: ../../_staging/docstring of tvm.relay.expr.const:0 ../../_staging/docstring
#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy_with_logits:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.relu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:0
msgid "Parameters"
msgstr "参数"

#: ../../../python/tvm/relay/expr.pydocstring of tvm.relay.expr.Constant:3
msgid "The data content of the constant expression."
msgstr ""

#: ../../../python/tvm/ir/expr.pydocstring of
#: tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":obj:`checked_type <tvm.relay.nn.Expr.checked_type>`\\"
msgstr ""

#: ../../../python/tvm/ir/expr.pydocstring of
#: tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the checked type of tvm.relay.Expr."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:1
msgid "1D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"across each window represented by W."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:7
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with shape"
" `(batch_size, in_channels, width)`, to produce an output Tensor with shape "
"(batch_size, in_channels, output_width)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:12
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:12
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:12
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:12
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:11
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:22
msgid "For output_size:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output width."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x C x"
" output_size) for any input (NCW)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:14
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:9
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:7
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:6
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:6
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:14
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:11
msgid "The input data to the operator."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:26
msgid "Output height and width."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:33
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:19
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:23
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:24
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:24
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:42
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:42
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:42
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:19
msgid "Layout of the input."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy_with_logits:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:0
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.relu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:0
msgid "Returns"
msgstr "返回"

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:31
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:31
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:31
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:40
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:26
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:13
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:31
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:32
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:11
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:11
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:33
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:11
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:33
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:21
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:51
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:51
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:51
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy_with_logits:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:34
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:37
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:14
#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.relu:9
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:33
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:29
msgid "**result** -- The computed result."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:0
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy_with_logits:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.relu:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:0
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:0
msgid "Return type"
msgstr "返回的数据类型"

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:1
msgid "2D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"across each window represented by WxH."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with shape (batch_size, in_channels, output_height, output_width)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output height and width."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x C x"
" output_size x output_size) for any input (NCHW)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:22
msgid ""
"If a tuple of integers (height, width) are provided for output_size, the "
"output size is (N x C x height x width) for any input (NCHW)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:1
msgid "3D adaptive avg pooling operator. This operator is experimental."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D avg value calculation across "
"each window represented by DxWxH."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:6
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with shape (batch_size, in_channels, output_depth, "
"output_height, output_width)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:15
msgid ""
"If this argument is not provided, input depth, height and width will be used"
" as output depth, height and width."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:18
msgid ""
"If a single integer is provided for output_size, the output size is (N x C x"
" output_size x output_size x output_size) for any input (NCDHW)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:21
msgid ""
"If a tuple of integers (depth, height, width) are provided for output_size, "
"the output size is (N x C x depth x height x width) for any input (NCDHW)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:1
msgid "1D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation across "
"each window represented by W."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:1
msgid "2D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation across "
"each window represented by WxH."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:1
msgid "3D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation across "
"each window represented by DxWxH."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation with"
" in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with shape"
" `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:11
msgid ""
"The ceil_mode is used to take ceil or floor while computing out shape. "
"count_include_pad indicates including or excluding padded input values in "
"computation. This operator accepts data layout specification."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:9
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:9
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:17
msgid "The size of window for pooling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:19
msgid "The strides of pooling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:21
msgid "The dilation of pooling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:31
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:22
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:23
msgid "The padding for pooling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:35
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:34
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:27
msgid "To enable or disable ceil while pooling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool1d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:37
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:29
msgid "To include padding to compute the average."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation with"
" in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:11
msgid "with data of shape (b, c, h, w), pool_size (kh, kw)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:13
msgid ""
"\\mbox{out}(b, c, y, x)  = \\frac{1}{kh * kw} \\sum_{m=0}^{kh-1} \\sum_{n=0}^{kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x + n)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d:18
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to take"
" ceil or floor while computing out shape. count_include_pad indicates "
"including or excluding padded input values in computation. This operator "
"accepts data layout specification."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:3
msgid ""
"This operator takes out_grad and data as input and calculates gradient of "
"avg_pool2d."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:5
msgid "The output gradient"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation with"
" in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.avg_pool3d:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, channels, depth, height, width)`, to produce an output "
"Tensor."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:3
msgid ""
"This operator flattens all the dimensions except for the batch dimension. "
"which results a 2D output."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:6
msgid ""
"For data with shape ``(d1, d2, ..., dk)`` batch_flatten(data) returns "
"reshaped output of shape ``(d1, d2*...*dk)``."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_flatten:13
msgid "**result** -- The Flattened result."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:3
msgid ""
"Both `tensor_a` and `tensor_b` can be transposed. For legacy reason, we use "
"NT format (transpose_a=False, transpose_b=True) by default."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:6
msgid ""
"\\mbox{batch_matmul}(A, B)[i, :, :] = \\mbox{matmul}(A[i, :, :], B[i, :, :])"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:10
msgid "The first input."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:12
msgid "The second input."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:14
msgid "Specifies the output data type for mixed precision batch matmul."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:16
msgid "Whether the first tensor is in transposed format."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_matmul:18
msgid "Whether the second tensor is in transposed format."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:1
msgid ""
"Batch normalization layer (Ioffe and Szegedy, 2014). Normalizes the input at"
" each batch, i.e. applies a transformation that maintains the mean "
"activation close to 0 and the activation standard deviation close to 1."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:6
msgid ""
"data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
"data\\_var[i] = var(data[:,i,:,...])"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:11
msgid ""
"Then compute the normalized output, which has the same shape as input, as "
"following:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:13
msgid ""
"out[:,i,:,...] = \\frac{data[:,i,:,...] - data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
"    * gamma[i] + beta[i]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:18
msgid ""
"Both *mean* and *var* returns a scalar by treating the input as a vector."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:14
msgid ""
"Assume the input has size *k* on axis 1, then both ``gamma`` and ``beta`` "
"have shape *(k,)*."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:23
msgid ""
"Besides the inputs and the outputs, this operator accepts two auxiliary "
"states, ``moving_mean`` and ``moving_var``, which are *k*-length vectors. "
"They are global statistics for the whole dataset, which are updated by"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:32
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes the "
"'channel' (separately normalized groups).  The default is 1. Specifying -1 "
"sets the channel axis to be the last item in the input shape."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:38
#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:6
msgid "This operator can be optimized away for inference."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:40
msgid "Input to which batch_norm will be applied."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:42
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:21
msgid "The gamma scale factor."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:44
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:23
msgid "The beta offset factor."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:46
msgid "Running mean of input,"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:48
msgid "Running variance of input."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:50
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:31
msgid "Specify along which shape axis the channel is specified."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:52
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:36
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:33
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:27
msgid "Small float added to variance to avoid dividing by zero."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:54
#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:38
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:35
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:29
msgid ""
"If True, add offset of beta to normalized tensor, If False, beta is ignored."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:57
msgid ""
"If true, multiply by gamma. If False, gamma is not used. When the next layer"
" is piecewise linear (also e.g. nn.relu), this can be disabled since the "
"scaling will be done by the next layer."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_norm:62
msgid ""
"**result** -- Tuple of normed data (same shape as input), new running mean "
"(k-length vector), and new running variance (k-length vector)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:4
msgid "N-D with shape [batch, spatial_shape, remaining_shape]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:6
msgid ""
"1-D of size [M] where M is number of spatial dims, specifies block size for "
"each spatial dimension."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:8
msgid ""
"2-D of shape [M, 2] where M is number of spatial dims, specifies [begin, "
"end] crop size for each spatial dimension."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.batch_to_space_nd:12
msgid ""
"**result** -- N-D Tensor with shape [batch / prod(block_shape), in_shape[1] "
"* block_shape[0] - crops[0,0] - crops[0,1], ..., in_shape[M] * "
"block_shape[M-1] - crops[M-1, 0] - crops[M-1, 1], remaining_shape]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:3
msgid ""
"Add 1D bias to the axis of data. This function is a special case of add "
"which allows inference of shape of the bias from data."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:9
msgid "The bias to be added."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:11
msgid "The axis to add the bias."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bias_add:14
msgid "**result** -- The final result."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:3
msgid ""
"The values along the input tensor's pack_axis are quantized and packed "
"together into the specified pack_type in a new bit axis."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:6
msgid ""
"For example, consider bitpacking with data to be a tensor with shape `[1, "
"64, 128, 128]`, pack_axis=1, bit_axis=4, pack_type=uint8, and bits=2. The "
"output in this case will be of shape `[1, 8, 128, 128, 2]`. The dimension of"
" axis 1 has been reduced by a factor of 8 since each value is packed into an"
" 8-bit uint8. Axis 4 is now two bitplanes representing the quantized value "
"of the incoming data. The output tensor is now ready to be used in a "
"bitserial operation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:13
msgid "The incoming tensor to be packed."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:15
msgid "Number of bits that should be packed."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:17
msgid "Axis that should be decomposed and packed."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:19
msgid "New axis containing bitplane."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:23
msgid "Datatype to pack bits into."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:23
msgid "Name of the operation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitpack:26
msgid "**result** -- The packed tensor."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:11
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:6
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:8
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:6
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:6
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:8
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:6
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:9
msgid "The weight expressions."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:7
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:11
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:12
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:12
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:30
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:11
msgid "The strides of convolution."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:9
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:12
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:13
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:14
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:14
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:32
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:9
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:13
msgid "The padding of convolution on both sides of inputs before convolution."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:11
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:18
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:19
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:20
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:20
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:19
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:38
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:38
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:38
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:21
msgid "Number of output channels of this convolution."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:13
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:20
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:21
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:22
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:22
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:40
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:40
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:23
msgid "The spatial of the convolution kernel."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:15
msgid "Number of bits to pack for activations."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:17
msgid "Number of bits to pack for weights."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:21
msgid "Layout of the kernel"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_conv2d:25
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:28
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:29
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:30
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:30
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:29
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:48
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:48
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:48
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:31
msgid "Specifies the output data type for mixed precision conv2d."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:1
msgid ""
"Bitserial Dense operator. Applies matrix multiplication of two quantized "
"matrices using a fast bitserial algorithm."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:7
msgid "`Y = X * W`"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:14
msgid "Number of hidden units of the dense transformation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:15
msgid "Number of bits incoming tensor should be packed with."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:17
msgid "Number of bits weight tensor should be packed with."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:19
msgid "Datatype to pack individual bits into before computation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:18
msgid "Specifies the output data type for mixed precision dense."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.bitserial_dense:23
msgid "Whether to use unipolar or bipolar quantization for inputs."
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:3
msgid "The constant value."
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:5
msgid "The data type of the resulting constant."
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:10
msgid "When dtype is None, we use the following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:12
msgid "int maps to \"int32\""
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:13
msgid "float maps to \"float32\""
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:14
msgid "bool maps to \"bool\""
msgstr ""

#: ../../_staging/docstring of tvm.relay.expr.const:15
msgid "other using the same default rule as numpy."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv2d_gemm_without_weight_transform"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:8
msgid "Tile rows of the weight transformation for ConvGemm."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:10
msgid "Tile columns of the weight transformation for ConvGemm."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv2d. It assumes "
"the weight is pre-transformed by nn.contrib_conv2d_gemm_weight_transform"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:14
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:15
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:16
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:16
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:34
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:34
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:34
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:15
msgid "Specifies the dilation rate to be used for dilated convolution."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:16
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:17
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:18
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:18
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:36
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:36
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:19
msgid "Number of groups for grouped convolution."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:24
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:25
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:26
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:26
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:44
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:44
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:44
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:21
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:27
msgid "Layout of the weight."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:26
#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:27
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:28
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:28
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:27
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:46
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:46
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:46
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:23
#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:29
msgid ""
"Layout of the output, by default, out_layout is the same as data_layout"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it "
"with data to produce an output, following a specialized NCHWc data layout."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:9
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:9
msgid "The kernel expressions."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:3
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv2d_winograd_without_weight_transform"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:8
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:8
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:10
msgid ""
"The Tile size of winograd. E.g. 2 for F(2x2, 3x3) and 4 for F(4x4, 3x3)"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv2d. It assumes "
"the weight is pre-transformed by nn.contrib_conv2d_winograd_weight_transform"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv3d_winograd_without_weight_transform"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:8
#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:10
msgid ""
"The Tile size of winograd. E.g. 2 for F(2x2x2, 3x3x3) and 4 for F(4x4x4, "
"3x3x3)"
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv3d. It assumes "
"the weight is pre-transformed by nn.contrib_conv3d_winograd_weight_transform"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:1
msgid "Dense operator. Applies a linear transformation with packed weight"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:6
msgid "`Y = X * W^T`"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:8
msgid "The input data to the operator, of shape `(batch, units_in)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:11
msgid ""
"The transformed weight expressions, 3-D matrix, of shape `(units // "
"pack_weight_tile, units_in, pack_weight_tile)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.contrib_dense_pack:14
msgid "The layout of weight, such as \"NC\" or \"NC8n\"."
msgstr ""

#: ../../_staging/docstring of
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:3
msgid ""
"This operator takes the weight as the depthwise convolution kernel and "
"depthwise convolves it with data to produce an output, following a "
"specialized NCHWc data layout."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it "
"with data to produce an output."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:7
msgid ""
"In the default case, where the data_layout is `NCW` and kernel_layout is "
"`OIW`, conv1d takes in a data Tensor with shape `(batch_size, in_channels, "
"width)`, and a weight Tensor with shape `(channels, in_channels, "
"kernel_size)` to produce an output Tensor with the following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:13
msgid ""
"\\mbox{out}[b, c, w] = \\sum_{dw, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * w + dw] *\n"
"   \\mbox{weight}[c, k, dw]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before the "
"computation. This operator accepts data layout specification. Semantically, "
"the operator will convert the layout to the canonical layout (`NCW` for data"
" and `OIW` for weight), perform the computation, then convert to the "
"out_layout."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:32
msgid ""
"The padding of convolution on both sides of the input before convolution."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:36
msgid "Currently unused for 1D convolution."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d:40
msgid "The spatial dimension of the convolution kernel."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:9
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:9
msgid "The padding of convolution on both sides of inputs."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv1d_transpose:25
#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d_transpose:25
msgid "Used to disambiguate the output shape."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` and kernel_layout is "
"`OIHW`, conv2d takes in a data Tensor with shape `(batch_size, in_channels, "
"height, width)`, and a weight Tensor with shape `(channels, in_channels, "
"kernel_size[0], kernel_size[1])` to produce an output Tensor with the "
"following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:13
msgid ""
"\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * y  + dy, \\mbox{strides}[1] * x + dx] *\n"
"   \\mbox{weight}[c, k, dy, dx]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv2d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before the "
"computation. This operator accepts data layout specification. Semantically, "
"the operator will convert the layout to the canonical layout (`NCHW` for "
"data and `OIHW` for weight), perform the computation, then convert to the "
"out_layout."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` and kernel_layout is "
"`OIDHW`, conv3d takes in a data Tensor with shape `(batch_size, in_channels,"
" depth, height, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_size[0], kernel_size[1], kernel_size[2])` to produce an "
"output Tensor with the following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:13
msgid ""
"\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * z  + dz, \\mbox{strides}[1] * y  + dy,\n"
"   \\mbox{strides}[2] * x + dx] * \\mbox{weight}[c, k, dz, dy, dx]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before the "
"computation. This operator accepts data layout specification. Semantically, "
"the operator will convert the layout to the canonical layout (`NCDHW` for "
"data and `OIDHW` for weight), perform the computation, then convert to the "
"out_layout."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.conv3d_transpose:25
msgid "Specifies the output data type for mixed precision conv3d."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:3
msgid ""
"The correlation layer performs multiplicative patch comparisons between two "
"feature maps. Given two multi-channel feature maps :math:`f_{1}, f_{2}`, "
"with :math:`w`, :math:`h`, and :math:`c` being their width, height, and "
"number of channels, the correlation layer lets the network compare each "
"patch from :math:`f_{1}` with each patch from :math:`f_{2}`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:8
msgid ""
"For now we consider only a single comparison of two patches. The "
"'correlation' of two patches centered at :math:`x_{1}` in the first map and "
":math:`x_{2}` in the second map is then defined as:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:12
msgid ""
"c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]} <f_{1}(x_{1} + o), "
"f_{2}(x_{2} + o)>"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:16
msgid "for a square patch of size :math:`K:=2k+1`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:18
msgid ""
"Note that the equation above is identical to one step of a convolution in "
"neural networks, but instead of convolving data with a filter, it convolves "
"data with other    data. For this reason, it has no training weights."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:22
msgid ""
"Computing :math:`c(x_{1}, x_{2})` involves :math:`c * K^{2}` "
"multiplications. Comparing all patch combinations involves "
":math:`w^{2}*h^{2}` such computations."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:25
msgid ""
"Given a maximum displacement :math:`d`, for each location :math:`x_{1}` it "
"computes correlations :math:`c(x_{1}, x_{2})` only in a neighborhood of size"
" :math:`D:=2d+1`, by limiting the range of :math:`x_{2}`. We use strides "
":math:`s_{1}, s_{2}`, to quantize :math:`x_{1}` globally and to quantize "
":math:`x_{2}` within the neighborhood centered around :math:`x_{1}`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:31
msgid "The final output is defined by the following expression:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:33
msgid "out[n, q, i, j] = c(x_{i, j}, x_{q})"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:37
msgid ""
"where :math:`i` and :math:`j` enumerate spatial locations in :math:`f_{1}`, "
"and :math:`q` denotes the :math:`q^{th}` neighborhood of :math:`x_{i,j}`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:40
#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:42
msgid "4-D with shape [batch, channel, height, width]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:44
msgid "Kernel size for correlation, must be an odd number"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:46
msgid "Max displacement of Correlation"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:48
msgid "Stride for data1"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:50
msgid "Stride for data2 within the neightborhood centered around data1"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:52
msgid ""
"Padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, pad_left, "
"pad_bottom, pad_right] for 4 ints"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:56
msgid "operation type is either multiplication or substraction"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:58
msgid "layout of data1, data2 and the output"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.correlation:61
msgid ""
"**Output** -- 4-D with shape [batch, out_channel, out_height, out_width]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy_with_logits:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:10
msgid "The predictions."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy:5
#: ../../_staging/docstring of tvm.relay.op.nn.nn.cross_entropy_with_logits:5
msgid "The targets."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:3
msgid ""
"The deformable convolution operation is described in "
"https://arxiv.org/abs/1703.06211"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:7
msgid "The offset expressions."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.deformable_conv2d:17
msgid "Number of deformable groups."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:1
msgid "Dense operator. Applies a linear transformation"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:8
msgid ""
"The input data to the operator, of shape `(d_1, d_2, ..., d_n, units_in)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:11
msgid "The weight expressions, 2-D matrix, of shape `(units, units_in)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dense:16
msgid ""
"Specifies the output data type for mixed precision dense, of shape `(d_1, "
"d_2, ..., d_n, units)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:3
msgid "Input data with channels divisible by block_size**2"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:5
msgid "Size of blocks to convert channels into."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:7
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:7
msgid "One of NCHW or NHWC, indicates channel axis."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:9
msgid "One of DCR or CDR, indicates which order channels are accessed in."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:13
msgid ""
"**result** --  Tensor with shape [in_batch, in_channel / block_size * "
"block_size,                    in_height * block_size, in_width * "
"block_size]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:10
msgid "**result** --"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:15
msgid "Tensor with shape [in_batch, in_channel / block_size * block_size,"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.depth_to_space:16
msgid "in_height * block_size, in_width * block_size]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:3
msgid "n-D, can be any layout."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:5
msgid "Dilation stride on each dimension, 1 means no dilation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:7
msgid "Value used to dilate the input."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dilate:10
msgid "**Output** -- The computed result"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:3
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:3
msgid ""
"During training, each element of the input is set to zero with probability "
"``p``. The whole array is rescaled by ``1/(1-p)`` to keep the expected sum "
"of the input unchanged."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:9
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:9
msgid "The probability for an element to be reset to 0."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout:12
#: ../../_staging/docstring of tvm.relay.op.nn.nn.dropout_raw:12
msgid "**result** -- The result of dropout"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:1
msgid ""
"Computes softmax. Use approximation to compute exponent for faster speed."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:4
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:3
msgid ""
"\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n"
"\n"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fast_softmax:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.softmax:10
msgid "The axis to sum over when computing softmax"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:3
msgid "Compute equivalent of"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:12
msgid "Useful for"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:14
msgid ""
"Encoding explicit re-use of computation in convolution ops operated on a "
"sliding window input"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:15
msgid ""
"Implementing a FIFO queue to cache intermediate results, e.g. as in Fast "
"WaveNet."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:17
#: ../../_staging/docstring of tvm.relay.op.nn.nn.relu:6
msgid "The input data"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:19
msgid "Previous value of the FIFO buffer"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:21
msgid "Specify which axis should be used for buffering"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.fifo_buffer:24
msgid "**result** -- Updated value for the buffer"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:5
msgid ""
"* **pad_left** (*int*) -- Padding size on left * **pad_right** (*int*) -- "
"Padding size on right."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:5
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:6
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:7
msgid "**pad_left** (*int*) -- Padding size on left"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:6
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:8
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:10
msgid "**pad_right** (*int*) -- Padding size on right."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:5
msgid ""
"* **pad_top** (*int*) -- Padding size on top * **pad_left** (*int*) -- "
"Padding size on left * **pad_down** (*int*) -- Padding size on down. * "
"**pad_right** (*int*) -- Padding size on right."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:5
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:6
msgid "**pad_top** (*int*) -- Padding size on top"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:7
#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:9
msgid "**pad_down** (*int*) -- Padding size on down."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:5
msgid ""
"* **pad_front** (*int*) -- Padding size on front * **pad_top** (*int*) -- "
"Padding size on top * **pad_left** (*int*) -- Padding size on left * "
"**pad_back** (*int*) -- Padding size on back * **pad_down** (*int*) -- "
"Padding size on down. * **pad_right** (*int*) -- Padding size on right."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:5
msgid "**pad_front** (*int*) -- Padding size on front"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:8
msgid "**pad_back** (*int*) -- Padding size on back"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with shape"
" `(batch_size, in_channels, width)`, to produce an output Tensor with the "
"following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:10
msgid "with data of shape (b, c, w)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool1d:12
msgid "\\mbox{out}(b, c, 1)  = \\frac{1}{w} \\sum_{n=0}^{w-1} \\mbox{data}(b, c, n)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:11
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:11
msgid "with data of shape (b, c, h, w)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool2d:13
msgid ""
"\\mbox{out}(b, c, 1, 1)  = \\frac{1}{h * w} \\sum_{m=0}^{h-1} \\sum_{n=0}^{w-1}\n"
"     \\mbox{data}(b, c, m, n)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"across each window represented by DxWxH."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:6
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with the following rule:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:10
msgid "with data of shape (b, c, d, h, w)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_avg_pool3d:12
msgid ""
"\\mbox{out}(b, c, 1, 1, 1)  = \\frac{1}{d * h * w} \\sum_{l=0}^{d-1}  \\sum_{m=0}^{h-1}\n"
"     \\sum_{n=0}^{w-1} \\mbox{data}(b, c, l, m, n)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool1d:10
msgid "with data of shape (b, c, w) .. math::"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool2d:13
msgid ""
"\\mbox{out}(b, c, 1, 1)  = \\max_{m=0, \\ldots, h} \\max_{n=0, \\ldots, w}\n"
"     \\mbox{data}(b, c, m, n)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.global_max_pool3d:10
msgid "with data of shape (b, c, d, h, w) .. math::"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:1
msgid ""
"Group normalization normalizes over group of channels for each training "
"examples. We can say that, Group Norm is in between Instance Norm and Layer "
"Norm. When we put all the channels into a single group, group normalization "
"becomes Layer normalization. And, when we put each channel into different "
"groups it becomes Instance normalization"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:6
msgid "https://arxiv.org/pdf/1803.08494.pdf"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:8
msgid ""
"Applies group normalization to the n-dimensional input array by seperating "
"the input channels into 'num_groups' groups, each containing 'num_channels /"
" num_groups' channels. The mean and standard-deviation are calculated "
"separately over the each group. gamma and beta are learnable per-channel "
"affine transform parameter vectors of size num_channels."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:6
msgid ""
"out = \\frac{data - mean(data, axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:18
msgid ""
"Unlike batch normalization, the mean and var are computed along a group of "
"channels."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:20
msgid ""
"If the input has size k on axis 1, then both gamma and beta have shape (k,)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:26
msgid "Input to which group_norm will be applied."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:32
msgid "The number of groups to separate the channels into."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:34
msgid "The axis of the channels."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:41
#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:38
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:32
msgid "If True, multiply by gamma. If False, gamma is not used."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.group_norm:44
#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:35
msgid "**result** -- The normalized data."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:4
msgid ""
"out = \\frac{data - mean(data)}{\\sqrt{var(data)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:9
msgid ""
"The instance normalization is similar to batch normalization, but unlike "
"batch normalization, the mean and var are calculated per-dimension "
"separately for each object(instance) in a mini-batch, not over a batch. And "
"the same normalization is applied both at test and train time."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:17
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes the "
"'channel'.  The default is 1. Specifying -1 sets the channel axis to be the "
"last item in the input shape."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:25
msgid "Input to which instance_norm will be applied."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:41
msgid ""
"* **result** (*tvm.relay.Expr*) -- The normalized data. * **.. _`Instance "
"Normalization** (The Missing Ingredient for Fast Stylization`:) -- "
"https://arxiv.org/abs/1607.08022"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:41
msgid "**result** (*tvm.relay.Expr*) -- The normalized data."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.instance_norm:42
msgid ""
"**.. _`Instance Normalization** (The Missing Ingredient for Fast "
"Stylization`:) -- https://arxiv.org/abs/1607.08022"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:3
msgid ""
"y(i, j) = x(i, j) / sqrt(max(sum(x^2), eps))\n"
"\n"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:8
msgid "epsilon value"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.l2_normalize:10
msgid "axis over the normalization applied"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:1
msgid ""
"Layer normalization (Lei Ba and et al., 2016). Applies layer normalization "
"to the n-dimensional input array. This operator takes an n-dimensional input"
" array and normalizes the input using the given axis:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:11
msgid ""
"Unlike batch normalization, the mean and var are computed along the channel "
"dimension."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:13
msgid ""
"Assume the input has size k on axis 1, then both gamma and beta have shape "
"(k,)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:19
msgid "Input to which layer_norm will be applied."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.layer_norm:25
msgid ""
"The axis that should be normalized, typically the axis of the channels."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:4
msgid "`y = x > 0 ? x : alpha * x`"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.leaky_relu:10
#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:10
msgid "Slope coefficient for the negative half axis."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:3
msgid "\\text{log_softmax}(x)_i = \\log \\frac{exp(x_i)}{\\sum_j exp(x_j)}"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.log_softmax:12
msgid "The axis to sum over when computing log softmax"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:3
msgid ""
"Normalize the input in a local region across or within feature maps. Each "
"input value is divided by (data / (bias + (alpha * sum_data ^2 /size))^beta)"
" where n is the size of each local region, and the sum is taken over the "
"region centered at that value (zero padding is added where necessary)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:8
msgid ""
"(data / (bias + (alpha * sum_data ^2 /size))^beta)\n"
"\n"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:13
msgid "The size of the local region to be considered for normalization."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:15
msgid "Input data layout channel axis. Default value is 1 for NCHW format"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:17
msgid "The offset parameter to avoid dividing by 0."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:19
msgid "The scaling parameter."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.lrn:21
msgid "The exponent parameter."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:1
msgid ""
"Matmul operator. Applies a linear transformation. The A & B can be "
"transposed."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:4
msgid "`C = A * B`"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:8
msgid ""
"The first input of the operator, of shape `(d_1, d_2, ..., d_n, units_in)` "
"or `(d_1, d_2, ..., units_in, d_n)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:11
msgid ""
"The second input expressions, 2-D matrix, of shape `(units_in, units)` or "
"`(units, units_in)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:14
msgid "Number of hidden units of the matmul transformation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:16
msgid ""
"Specifies the output data type for mixed precision matmul, of shape `(d_1, "
"d_2, ..., d_n, units)`."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:19
msgid "Whether the data tensor is in transposed format."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.matmul:21
msgid "Whether the weight tensor is in transposed format."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation with in "
"pool_size sized window by striding defined by stride."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation with in "
"pool_size sized window by striding defined by stride"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:11
msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:13
msgid ""
"\\mbox{out}(b, c, y, x)  = \\max_{m=0, \\ldots, kh-1} \\max_{n=0, \\ldots, kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x + n)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d:18
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to take"
" ceil or floor while computing out shape. This operator accepts data layout "
"specification."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool2d_grad:3
msgid ""
"This operator takes out_grad and data as input and calculates gradient of "
"max_pool2d."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation with in "
"pool_size sized window by striding defined by stride."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified widths "
"using mirroring of the border pixels."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:6
#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:6
msgid "The input data to the operator"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:8
#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:8
msgid ""
"Number of values padded to the edges of each axis, in the format of "
"((before_1, after_1), ..., (before_N, after_N))"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.mirror_pad:11
msgid "What type of mirroring to use, must be SYMMETRIC or REFLECT."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:6
msgid "output{n, i_1, i_2, ..., i_k} = -p * w"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:6
msgid "where t = target{n, i_1, i_2, ..., i_k}"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:5
msgid ""
"p = predictions{n, t, i_1, i_2, i_k} w = weights{n, i_1, i_2, ..., i_k} if t"
" != ignore_index else 0"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:8
msgid "result = reduction(output)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:12
msgid "The target value of each prediction."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:14
msgid "The weight of each target value."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:16
msgid ""
"The reduction method to apply to the output. Possible values are \"mean\", "
"\"sum\" and \"none\"."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.nll_loss:19
msgid "The target value to ignore."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified widths "
"using the specified value."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:11
msgid "The value used for padding"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.pad:13
msgid ""
"'constant' pads with constant_value pad_value 'edge' pads using the edge "
"values of the input array 'reflect' pads by reflecting values with respect "
"to the edge"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:4
msgid "y = x > 0 ? x : alpha * x"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.prelu:12
msgid "Specify which shape axis the channel is specified."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.relu:3
msgid ""
"out = max(x, 0)\n"
"\n"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:9
msgid ""
"2-D of shape [M, 2] where M is number of spatial dims, specifies [before, "
"after] paddings for each spatial dimension."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:12
msgid "The value used for padding."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_batch_nd:15
msgid ""
"**result** -- N-D Tensor with shape [in_batch * prod(block_shape), "
"padded_data[1] / block_shape[0], ..., padded_data[M] / block_shape[M-1], "
"remaining_shape]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:3
msgid "Input data with spatial dimensions divisible by block_size"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:5
msgid "Size of blocks to decompose into channels."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:10
msgid ""
"**result** --  Tensor with shape [in_batch, in_channel * block_size * "
"block_size,                    in_height / block_size, in_width / "
"block_size]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:12
msgid "Tensor with shape [in_batch, in_channel * block_size * block_size,"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.space_to_depth:13
msgid "in_height / block_size, in_width / block_size]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:5
msgid ""
"\\mbox{sparse_add}(dense_mat, sparse_mat)[m, n] = "
"\\mbox{add}(\\mbox{as_dense}(S), (D))[m, n]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:9
msgid ""
"where `as_dense` returns dense equivalent of the given S(sparse matrix) "
"while performing addition with given D(dense matrix)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:12
msgid "The input dense matrix for the matrix addition"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:14
msgid "The input sparse matrix(CSR) for the matrix addition."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_add:21
msgid "Examples"
msgstr "样例"

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:9
msgid "\\if sparse_lhs=False:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:6
msgid ""
"\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
"= \\mbox{matmul}(D, \\mbox{as_dense}(S)^T)[m, n]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:15
msgid "\\if sparse_lhs=True:"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:12
msgid ""
"\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
"= \\mbox{matmul}(\\mbox{as_dense}(S), (D)^T)[m, n]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:17
msgid ""
"where `as_dense` returns dense equivalent of the given S(sparse matrix) "
"while performing matmul with given D(dense matrix)."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:20
msgid ""
"See "
"https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
" and "
"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.bsr_matrix.html"
" for more detail on the sparse matrix representation."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:26
msgid "The input dense matrix for the matrix multiplication"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:28
msgid "The input sparse matrix for the matrix multiplication."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_dense:30
msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:5
msgid "** Currently only support Square Matrices **"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:7
msgid "\\mbox{sparse_transpose}(x)[n, n] = (x^T)[n, n]"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:11
msgid ""
"Please refer to "
"https://github.com/scipy/scipy/blob/v1.3.0/scipy/sparse/csr.py for the "
"algorithm implemented in this operator."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:14
msgid "The sparse weight matrix for the fast matrix transpose."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.sparse_transpose:17
msgid ""
"**result** -- Tuple of output sparse tensor (same shape and format as "
"input), i.e. if CSR then output is in ([data, indices, indptr]) form"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:3
msgid ""
"This operator takes data as input and does 2D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCHW` with data of "
"shape (n, c, h, w) out will have a shape (n, c, h*scale_h, w*scale_w)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:8
msgid ""
"method indicates the algorithm to be used while calculating the out value "
"and method can be one of (\"bilinear\", \"nearest_neighbor\", \"bicubic\")"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:13
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:15
msgid "The scale factor for height upsampling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:15
#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:17
msgid "The scale factor for width upsampling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:19
msgid "Scale method to used [nearest_neighbor, bilinear, bicubic]."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling:21
msgid "Whether to keep corners in proper place."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:3
msgid ""
"This operator takes data as input and does 3D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCDHW` with data of "
"shape (n, c, d, h, w) out will have a shape (n, c, d*scale_d, h*scale_h, "
"w*scale_w)"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:8
msgid ""
"method indicates the algorithm to be used while calculating the out value "
"and method can be one of (\"trilinear\", \"nearest_neighbor\")"
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:13
msgid "The scale factor for depth upsampling."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:21
msgid "Scale method to used [nearest_neighbor, trilinear]."
msgstr ""

#: ../../_staging/docstring of tvm.relay.op.nn.nn.upsampling3d:23
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details. Available options are \"half_pixel\", "
"\"align_corners\" and \"asymmetric\"."
msgstr ""
