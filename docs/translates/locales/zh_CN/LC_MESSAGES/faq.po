# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1713+gbe5f05f3f\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-18 01:16+0000\n"
"PO-Revision-Date: 2021-09-18 05:20+0000\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124870/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/faq.rst:20
msgid "Frequently Asked Questions"
msgstr ""

#: ../../_staging/faq.rst:24
msgid "How to Install"
msgstr ""

#: ../../_staging/faq.rst:25
msgid "See :ref:`installation`."
msgstr ""

#: ../../_staging/faq.rst:29
msgid "How to add a new Hardware Backend"
msgstr ""

#: ../../_staging/faq.rst:31
msgid ""
"If the hardware backend has LLVM support, then we can directly generate the "
"code by setting the correct target triple as in :py:mod:`~tvm.target`."
msgstr ""

#: ../../_staging/faq.rst:33
msgid ""
"If the target hardware is a GPU, try to use the cuda, opencl or vulkan "
"backend."
msgstr ""

#: ../../_staging/faq.rst:34
msgid ""
"If the target hardware is a special accelerator, checkout :ref:`vta-index` "
"and :ref:`relay-bring-your-own-codegen`."
msgstr ""

#: ../../_staging/faq.rst:36
msgid ""
"For all of the above cases, You may want to add target specific optimization"
" templates using AutoTVM, see :ref:`tutorials-autotvm-sec`."
msgstr ""

#: ../../_staging/faq.rst:38
msgid ""
"Besides using LLVM's vectorization, we can also embed micro-kernels to "
"leverage hardware intrinsics, see :ref:`tutorials-tensorize`."
msgstr ""

#: ../../_staging/faq.rst:43
msgid "TVM's relation to Other IR/DSL Projects"
msgstr ""

#: ../../_staging/faq.rst:44
msgid ""
"There are usually two levels of abstractions of IR in the deep learning "
"systems. TensorFlow's XLA and Intel's ngraph both use a computation graph "
"representation. This representation is high level, and can be helpful to "
"perform generic optimizations such as memory reuse, layout transformation "
"and automatic differentiation."
msgstr ""

#: ../../_staging/faq.rst:49
msgid ""
"TVM adopts a low-level representation, that explicitly express the choice of"
" memory layout, parallelization pattern, locality and hardware primitives "
"etc. This level of IR is closer to directly target hardwares. The low-level "
"IR adopts ideas from existing image processing languages like Halide, "
"darkroom and loop transformation tools like loopy and polyhedra-based "
"analysis. We specifically focus on expressing deep learning workloads (e.g. "
"recurrence), optimization for different hardware backends and embedding with"
" frameworks to provide end-to-end compilation stack."
msgstr ""

#: ../../_staging/faq.rst:60
msgid "TVM's relation to libDNN, cuDNN"
msgstr ""

#: ../../_staging/faq.rst:61
msgid ""
"TVM can incorporate these libraries as external calls. One goal of TVM is to"
" be able to generate high-performing kernels. We will evolve TVM an "
"incremental manner as we learn from the techniques of manual kernel crafting"
" and add these as primitives in DSL. See also top for recipes of operators "
"in TVM."
msgstr ""

#: ../../_staging/faq.rst:68
msgid "Security"
msgstr ""

#: ../../_staging/faq.rst:69
msgid "See :ref:`dev-security`"
msgstr ""
