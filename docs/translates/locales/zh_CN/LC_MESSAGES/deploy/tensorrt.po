# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1713+gbe5f05f3f\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-18 01:16+0000\n"
"PO-Revision-Date: 2021-09-18 07:42+0000\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/deploy/tensorrt.rst:19
msgid "Relay TensorRT Integration"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:20
msgid "**Author**: `Trevor Morris <https://github.com/trevor-m>`_"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:23
msgid "Introduction"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:25
msgid ""
"NVIDIA TensorRT is a library for optimized deep learning inference. This "
"integration will offload as many operators as possible from Relay to "
"TensorRT, providing a performance boost on NVIDIA GPUs without the need to "
"tune schedules."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:29
msgid ""
"This guide will demonstrate how to install TensorRT and build TVM with "
"TensorRT BYOC and runtime enabled. It will also provide example code to "
"compile and run a ResNet-18 model using TensorRT and how to configure the "
"compilation and runtime settings. Finally, we document the supported "
"operators and how to extend the integration to support other operators."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:35
msgid "Installing TensorRT"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:37
msgid ""
"In order to download TensorRT, you will need to create an NVIDIA Developer "
"program account. Please see NVIDIA's documentation for more info: "
"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html. If "
"you have a Jetson device such as a TX1, TX2, Xavier, or Nano, TensorRT will "
"already be installed on the device via the JetPack SDK."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:43
msgid "There are two methods to install TensorRT:"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:45
msgid "System install via deb or rpm package."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:46
msgid "Tar file installation."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:48
msgid ""
"With the tar file installation method, you must provide the path of the "
"extracted tar archive to USE_TENSORRT_RUNTIME=/path/to/TensorRT. With the "
"system install method, USE_TENSORRT_RUNTIME=ON will automatically locate "
"your installation."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:53
msgid "Building TVM with TensorRT support"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:55
msgid ""
"There are two separate build flags for TensorRT integration in TVM. These "
"flags also enable cross-compilation: USE_TENSORRT_CODEGEN=ON will also you "
"to build a module with TensorRT support on a host machine, while "
"USE_TENSORRT_RUNTIME=ON will enable the TVM runtime on an edge device to "
"execute the TensorRT module. You should enable both if you want to compile "
"and also execute models with the same TVM build."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:61
msgid ""
"USE_TENSORRT_CODEGEN=ON/OFF - This flag will enable compiling a TensorRT "
"module, which does not require any TensorRT library."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:63
msgid ""
"USE_TENSORRT_RUNTIME=ON/OFF/path-to-TensorRT - This flag will enable the "
"TensorRT runtime module. This will build TVM against the installed TensorRT "
"library."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:66
msgid "Example setting in config.cmake file:"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:75
msgid "Build and Deploy ResNet-18 with TensorRT"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:77
msgid "Create a Relay graph from a MXNet ResNet-18 model."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:92
msgid ""
"Annotate and partition the graph for TensorRT. All ops which are supported "
"by the TensorRT integration will be marked and offloaded to TensorRT. The "
"rest of the ops will go through the regular TVM CUDA compilation and code "
"generation."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:102
msgid ""
"Build the Relay graph, using the new module and config returned by "
"partition_for_tensorrt. The target must always be a cuda target. "
"``partition_for_tensorrt`` will automatically fill out the required values "
"in the config, so there is no need to modify it - just pass it along to the "
"PassContext so the values can be read during compilation."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:114
msgid "Export the module."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:121
msgid ""
"Load module and run inference on the target machine, which must be built "
"with ``USE_TENSORRT_RUNTIME`` enabled. The first run will take longer "
"because the TensorRT engine will have to be built."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:135
msgid "Partitioning and Compilation Settings"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:137
msgid ""
"There are some options which can be configured in "
"``partition_for_tensorrt``."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:139
msgid ""
"``version`` - TensorRT version to target as tuple of (major, minor, patch). "
"If TVM is compiled with USE_TENSORRT_RUNTIME=ON, the linked TensorRT version"
" will be used instead. The version will affect which ops can be partitioned "
"to TensorRT."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:142
msgid ""
"``use_implicit_batch`` - Use TensorRT implicit batch mode (default true). "
"Setting to false will enable explicit batch mode which will widen supported "
"operators to include those which modify the batch dimension, but may reduce "
"performance for some models."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:145
msgid ""
"``remove_no_mac_subgraphs`` - A heuristic to improve performance. Removes "
"subgraphs which have been partitioned for TensorRT if they do not have any "
"multiply-accumulate operations. The removed subgraphs will go through TVM's "
"standard compilation instead."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:148
msgid ""
"``max_workspace_size`` - How many bytes of workspace size to allow each "
"subgraph to use for TensorRT engine creation. See TensorRT documentation for"
" more info. Can be overriden at runtime."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:153
msgid "Runtime Settings"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:155
msgid ""
"There are some additional options which can be configured at runtime using "
"environment variables."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:157
msgid ""
"Automatic FP16 Conversion - Environment variable ``TVM_TENSORRT_USE_FP16=1``"
" can be set to automatically convert the TensorRT components of your model "
"to 16-bit floating point precision. This can greatly increase performance, "
"but may cause some slight loss in the model accuracy."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:160
msgid ""
"Caching TensorRT Engines - During the first inference, the runtime will "
"invoke the TensorRT API to build an engine. This can be time consuming, so "
"you can set ``TVM_TENSORRT_CACHE_DIR`` to point to a directory to save these"
" built engines to on the disk. The next time you load the model and give it "
"the same directory, the runtime will load the already built engines to avoid"
" the long warmup time. A unique directory is required for each model."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:165
msgid ""
"TensorRT has a paramter to configure the maximum amount of scratch space "
"that each layer in the model can use. It is generally best to use the "
"highest value which does not cause you to run out of memory. You can use "
"``TVM_TENSORRT_MAX_WORKSPACE_SIZE`` to override this by specifying the "
"workspace size in bytes you would like to use."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:169
msgid ""
"For models which contain a dynamic batch dimension, the varaible "
"``TVM_TENSORRT_MULTI_ENGINE`` can be used to determine how TensorRT engines "
"will be created at runtime. The default mode, "
"``TVM_TENSORRT_MULTI_ENGINE=0``, will maintain only one engine in memory at "
"a time. If an input is encountered with a higher batch size, the engine will"
" be rebuilt with the new max_batch_size setting. That engine will be "
"compatible with all batch sizes from 1 to max_batch_size. This mode reduces "
"the amount of memory used at runtime. The second mode, "
"``TVM_TENSORRT_MULTI_ENGINE=1`` will build a unique TensorRT engine which is"
" optimized for each batch size that is encountered. This will give greater "
"performance, but will consume more memory."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:180
msgid "Operator support"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:182
msgid "Relay Node"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:182
msgid "Remarks"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:184
msgid "nn.relu"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:186
msgid "sigmoid"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:188
msgid "tanh"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:190
msgid "nn.batch_norm"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:192
msgid "nn.layer_norm"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:194
msgid "nn.softmax"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:196
msgid "nn.conv2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:198
msgid "nn.dense"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:200
msgid "nn.bias_add"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:202
msgid "add"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:204
msgid "subtract"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:206
msgid "multiply"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:208
msgid "divide"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:210
msgid "power"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:212
msgid "maximum"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:214
msgid "minimum"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:216
msgid "nn.max_pool2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:218
msgid "nn.avg_pool2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:220
msgid "nn.global_max_pool2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:222
msgid "nn.global_avg_pool2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:224
msgid "exp"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:226
msgid "log"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:228
msgid "sqrt"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:230
msgid "abs"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:232
msgid "negative"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:234
msgid "nn.batch_flatten"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:236
msgid "expand_dims"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:238
msgid "squeeze"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:240
msgid "concatenate"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:242
msgid "nn.conv2d_transpose"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:244
msgid "transpose"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:246
msgid "layout_transform"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:248
msgid "reshape"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:250
msgid "nn.pad"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:252
msgid "sum"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:254
msgid "prod"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:256
msgid "max"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:258
msgid "min"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:260
msgid "mean"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:262
msgid "nn.adaptive_max_pool2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:264
msgid "nn.adaptive_avg_pool2d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:266
msgid "nn.batch_matmul"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:268
msgid "clip"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:268
#: ../../_staging/deploy/tensorrt.rst:270
#: ../../_staging/deploy/tensorrt.rst:272
#: ../../_staging/deploy/tensorrt.rst:274
#: ../../_staging/deploy/tensorrt.rst:276
#: ../../_staging/deploy/tensorrt.rst:278
#: ../../_staging/deploy/tensorrt.rst:280
#: ../../_staging/deploy/tensorrt.rst:282
msgid "Requires TensorRT 5.1.5 or greater"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:270
msgid "nn.leaky_relu"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:272
msgid "sin"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:274
msgid "cos"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:276
msgid "atan"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:278
msgid "ceil"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:280
msgid "floor"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:282
msgid "strided_slice"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:284
msgid "nn.conv3d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:284
#: ../../_staging/deploy/tensorrt.rst:286
#: ../../_staging/deploy/tensorrt.rst:288
#: ../../_staging/deploy/tensorrt.rst:290
msgid "Requires TensorRT 6.0.1 or greater"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:286
msgid "nn.max_pool3d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:288
msgid "nn.avg_pool3d"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:290
msgid "nn.conv3d_transpose"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:292
msgid "erf"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:292
msgid "Requires TensorRT 7.0.0 or greater"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:297
msgid "Adding a new operator"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:298
msgid ""
"To add support for a new operator, there are a series of files we need to "
"make changes to:"
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:300
msgid ""
"`src/runtime/contrib/tensorrt/tensorrt_ops.cc` Create a new op converter "
"class which implements the ``TensorRTOpConverter`` interface. You must "
"implement the constructor to specify how many inputs there are and whether "
"they are tensors or weights. You must also implement the ``Convert`` method "
"to perform the conversion. This is done by using the inputs, attributes, and"
" network from params to add the new TensorRT layers and push the layer "
"outputs. You can use the existing converters as an example. Finally, "
"register your new op conventer in the ``GetOpConverters()`` map."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:307
msgid ""
"`python/relay/op/contrib/tensorrt.py` This file contains the annotation "
"rules for TensorRT. These determine which operators and their attributes "
"that are supported. You must register an annotation function for the relay "
"operator and specify which attributes are supported by your converter, by "
"checking the attributes are returning true or false."
msgstr ""

#: ../../_staging/deploy/tensorrt.rst:311
msgid ""
"`tests/python/contrib/test_tensorrt.py` Add unit tests for the given "
"operator."
msgstr ""
